{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re, random\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_content(dict_replace, target):\n",
    "    \"\"\"Based on dict, replaces key with the value on the target.\"\"\"\n",
    "\n",
    "    for check, replacer in list(dict_replace.items()):\n",
    "        target = target.replace(check, replacer)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "redict = {\n",
    "\"are n't\" : \"aren't\",\n",
    "\"ca n't\" : \"can't\",\n",
    "\"could n't\" : \"couldn't\",\n",
    "\"did n't\" : \"didn't\",\n",
    "\"does n't\" : \"doesn't\",\n",
    "\"do n't\" : \"don't\",\n",
    "\"had n't\" : \"hadn't\",\n",
    "\"has n't\" : \"hasn't\",\n",
    "\"have n't\" : \"haven't\",\n",
    "\"h e'd\" : \"he'd\",\n",
    "\"h e'll\" : \"he'll\",\n",
    "\"h e's\" : \"he's\",\n",
    "\"i 'd\" : \"i'd\",\n",
    "\"i 'll\" : \"i'll\",    \n",
    "\"i 'm\" : \"i'm\",\n",
    "\"i 've\" : \"i've\",\n",
    "\"is n't\" : \"isn't\",\n",
    "\"i t's\" : \"it's\",\n",
    "\"le t's\" : \"let's\",\n",
    "\"must n't\" : \"mustn't\",\n",
    "\"sha n't\" : \"shan't\",\n",
    "\"sh e'd\" : \"she'd\",\n",
    "\"sh e'll\" : \"she'll\",\n",
    "\"sh e's\" : \"she's\",\n",
    "\"should n't\" : \"shouldn't\",\n",
    "\"tha t's\" : \"that's\",\n",
    "\"ther e's\" : \"there's\",\n",
    "\"the y'll\" : \"they'll\",\n",
    "\"the y're\" : \"they're\",\n",
    "\"the y've\" : \"they've\",\n",
    "\"w e'd\" : \"we'd\",\n",
    "\"w e're\" : \"we're\",\n",
    "\"w e've\" : \"we've\",\n",
    "\"were n't\" : \"weren't\",\n",
    "\"wha t'll\" : \"what'll\",\n",
    "\"wha t're\" : \"what're\",\n",
    "\"wha t's\" : \"what's\",\n",
    "\"wha t've\" : \"what've\",\n",
    "\"wher e's\" : \"where's\",\n",
    "\"wh o'd\" : \"who'd\",\n",
    "\"wh o'll\" : \"who'll\",\n",
    "\"wh o're\" : \"who're\",\n",
    "\"wh o's\" : \"who's\",\n",
    "\"wh o've\" : \"who've\",\n",
    "\"wo n't\" : \"won't\",\n",
    "\"would n't\" : \"wouldn't\",\n",
    "\"yo u'd\" : \"you'd\",\n",
    "\"yo u'll\" : \"you'll\",\n",
    "\"yo u're\" : \"you're\",\n",
    "\"yo u've\" : \"you've\",\n",
    "# \" 's\" : \"'s\",\n",
    "# \" 're\": \"'re\",\n",
    "\"new zealand\" : \"\",\n",
    "\"<p>\" : \"\",\n",
    "\"<h>\" : \"\",\n",
    "\" @ \" : \"\",\n",
    "\"@\" : \"\",\n",
    "\"\\n\" : \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_au = pd.read_csv('AU_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_au.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "\n",
    "df_au.to_pickle('df_au.pkl')\n",
    "\n",
    "df_bd = pd.read_csv('BD_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_bd.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_bd.to_pickle('df_bd.pkl')\n",
    "\n",
    "df_ca = pd.read_csv('CA_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_ca.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_ca.to_pickle('df_ca.pkl')\n",
    "\n",
    "df_gb = pd.read_csv('GB_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_gb.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_gb.to_pickle('df_gb.pkl')\n",
    "\n",
    "df_gh = pd.read_csv('GH_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_gh.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_gh.to_pickle('df_gh.pkl')\n",
    "\n",
    "df_hk = pd.read_csv('HK_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_hk.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_hk.to_pickle('df_hk.pkl')\n",
    "\n",
    "df_ie = pd.read_csv('IE_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_ie.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_ie.to_pickle('df_ie.pkl')\n",
    "\n",
    "df_in = pd.read_csv('IN_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_in.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_in.to_pickle('df_in.pkl')\n",
    "\n",
    "df_jm = pd.read_csv('JM_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_jm.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_jm.to_pickle('df_jm.pkl')\n",
    "\n",
    "df_ke = pd.read_csv('KE_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_ke.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_ke.to_pickle('df_ke.pkl')\n",
    "\n",
    "df_lk = pd.read_csv('LK_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_lk.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_lk.to_pickle('df_lk.pkl')\n",
    "\n",
    "df_my = pd.read_csv('MY_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_my.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_my.to_pickle('df_my.pkl')\n",
    "\n",
    "df_ng = pd.read_csv('NG_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_ng.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_ng.to_pickle('df_ng.pkl')\n",
    "\n",
    "df_nz = pd.read_csv('NZ_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_nz.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_nz.to_pickle('df_nz.pkl')\n",
    "\n",
    "df_ph = pd.read_csv('PH_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_ph.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_ph.to_pickle('df_ph.pkl')\n",
    "\n",
    "### df_pk = pd.read_csv('PK_domestic_Ngram_stopword_lematize.csv', usecols=['article_text_Ngram_stopword_lemmatize'])\n",
    "\n",
    "df_sg = pd.read_csv('SG_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_sg.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_sg.to_pickle('df_sg.pkl')\n",
    "\n",
    "df_tz = pd.read_csv('TZ_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_tz.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_tz.to_pickle('df_tz.pkl')\n",
    "\n",
    "df_us = pd.read_csv('US_domestic_Ngram_stopword_lematize.csv', \\\n",
    "                    usecols=['article_title', 'year', 'article_text_Ngram_stopword_lemmatize'])\n",
    "df_us.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_us.to_pickle('df_us.pkl')\n",
    "\n",
    "### df_za = pd.read_csv('ZA_domestic_Ngram_stopword_lematize.csv', usecols=['article_text_Ngram_stopword_lemmatize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_au.sort_values(by=['year'], inplace=True, ignore_index=True)\n",
    "df_au_count = df_au.groupby('year').count().reset_index()\\\n",
    ".rename(columns={\"article_text_Ngram_stopword_lemmatize\":\"num_articles\"})\n",
    "\n",
    "df_au_articles = df_au.groupby('year').agg(articles_text = ('article_text_Ngram_stopword_lemmatize', 'sum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_au_new = pd.merge(df_au_articles, df_au_count, on='year', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_au_new['articles_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAMED ENTITIES = counts_au1, wordfreq1\n",
    "## NON-NAMED ENTITIES = counts_au2, wordfreq2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 47 Minutes : 01 Seconds'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_au = pd.read_pickle('df_au.pkl')\n",
    "\n",
    "counts_au1 = {}\n",
    "counts_au2 = {}\n",
    "\n",
    "wordfreq_au1 = []\n",
    "wordfreq_au2 = []\n",
    "\n",
    "for index, row in df_au.iterrows():\n",
    "    text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "    new_text = replace_content(redict, text_data)\n",
    "    document = nlp(new_text)\n",
    "\n",
    "    [wordfreq_au1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "    [wordfreq_au2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_au1 = Counter(word for word in wordfreq_au1)\n",
    "counts_au2 = Counter(word for word in wordfreq_au2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_au1:\n",
    "        del counts_au1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_au2:\n",
    "        del counts_au2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_au1:\n",
    "        del counts_au1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_au2:\n",
    "        del counts_au2[i]\n",
    "        \n",
    "df_counts_au1 = pd.DataFrame(Counter(counts_au1).most_common(500))\n",
    "df_counts_au2 = pd.DataFrame(Counter(counts_au2).most_common(500))\n",
    "\n",
    "df_counts_au1.to_pickle('df_counts_au1.pkl')\n",
    "df_counts_au2.to_pickle('df_counts_au2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# au, bd, ca, gb, gh, hk, ie, 'in', jm, ke, lk, my, ng, nz, ph, sg, tz, us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_au = pd.DataFrame(counts_au2.most_common(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \"'\", \"''\", \"'m\", \"/\", \"'ll\", \"*\", \"'d\", \"'ve\", \"m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd = pd.read_pickle('df_bd.pkl')\n",
    "\n",
    "counts_bd1 = {}\n",
    "counts_bd2 = {}\n",
    "\n",
    "wordfreq_bd1 = []\n",
    "wordfreq_bd2 = []\n",
    "\n",
    "for index, row in df_bd.iterrows():\n",
    "    text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "    new_text = replace_content(redict, text_data)\n",
    "    document = nlp(new_text)\n",
    "\n",
    "    [wordfreq_bd1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "    [wordfreq_bd2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_bd1 = Counter(word for word in wordfreq_bd1)\n",
    "counts_bd2 = Counter(word for word in wordfreq_bd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\", \\\n",
    "               'tk', 'bnp', 'bangabandhu', 'bangladesh', \\\n",
    "               'rahman', 'hossain', 'hasina', 'upazila', \\\n",
    "               'bangabandhu', 'bangla', 'indian', 'myanmar', \\\n",
    "               'chittagong', 'khan', 'rohingya', 'chowdhury'\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_bd1:\n",
    "        del counts_bd1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_bd2:\n",
    "        del counts_bd2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_bd1:\n",
    "        del counts_bd1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_bd2:\n",
    "        del counts_bd2[i]\n",
    "        \n",
    "df_counts_bd1 = pd.DataFrame(Counter(counts_bd1).most_common(500))\n",
    "df_counts_bd2 = pd.DataFrame(Counter(counts_bd2).most_common(500))\n",
    "\n",
    "df_counts_bd1.to_pickle('df_counts_bd1.pkl')\n",
    "df_counts_bd2.to_pickle('df_counts_bd2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts_bd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('country', 22510))\n",
      "(1, ('government', 20966))\n",
      "(2, ('people', 19966))\n",
      "(3, ('minister', 12601))\n",
      "(4, ('time', 12261))\n",
      "(5, ('work', 11527))\n",
      "(6, ('police', 10164))\n",
      "(7, ('come', 9810))\n",
      "(8, ('high', 9434))\n",
      "(9, ('use', 8819))\n",
      "(10, ('new', 8614))\n",
      "(11, ('include', 8222))\n",
      "(12, ('case', 8117))\n",
      "(13, ('add', 8097))\n",
      "(14, ('area', 7970))\n",
      "(15, ('world', 7732))\n",
      "(16, ('need', 7700))\n",
      "(17, ('bank', 7421))\n",
      "(18, ('official', 7324))\n",
      "(19, ('report', 7164))\n",
      "(20, ('day', 7131))\n",
      "(21, ('like', 7089))\n",
      "(22, ('student', 7014))\n",
      "(23, ('party', 6932))\n",
      "(24, ('state', 6917))\n",
      "(25, ('law', 6701))\n",
      "(26, ('project', 6697))\n",
      "(27, ('member', 6590))\n",
      "(28, ('development', 6557))\n",
      "(29, ('told', 6551))\n",
      "(30, ('international', 6515))\n",
      "(31, ('leader', 6462))\n",
      "(32, ('issue', 6457))\n",
      "(33, ('national', 6452))\n",
      "(34, ('number', 6256))\n",
      "(35, ('city', 6221))\n",
      "(36, ('general', 5950))\n",
      "(37, ('power', 5937))\n",
      "(38, ('woman', 5865))\n",
      "(39, ('local', 5832))\n",
      "(40, ('start', 5831))\n",
      "(41, ('court', 5822))\n",
      "(42, ('prime', 5749))\n",
      "(43, ('market', 5736))\n",
      "(44, ('child', 5697))\n",
      "(45, ('life', 5680))\n",
      "(46, ('company', 5640))\n",
      "(47, ('sector', 5576))\n",
      "(48, ('right', 5575))\n",
      "(49, ('service', 5541))\n",
      "(50, ('water', 5507))\n",
      "(51, ('good', 5485))\n",
      "(52, ('place', 5472))\n",
      "(53, ('political', 5308))\n",
      "(54, ('increase', 5303))\n",
      "(55, ('election', 5273))\n",
      "(56, ('president', 5125))\n",
      "(57, ('land', 5105))\n",
      "(58, ('director', 5096))\n",
      "(59, ('know', 5079))\n",
      "(60, ('home', 5075))\n",
      "(61, ('public', 5063))\n",
      "(62, ('different', 5053))\n",
      "(63, ('group', 5040))\n",
      "(64, ('set', 5035))\n",
      "(65, ('help', 5007))\n",
      "(66, ('lead', 4998))\n",
      "(67, ('family', 4970))\n",
      "(68, ('road', 4951))\n",
      "(69, ('provide', 4943))\n",
      "(70, ('secretary', 4934))\n",
      "(71, ('change', 4895))\n",
      "(72, ('team', 4866))\n",
      "(73, ('health', 4846))\n",
      "(74, ('demand', 4832))\n",
      "(75, ('sheikh', 4790))\n",
      "(76, ('way', 4789))\n",
      "(77, ('business', 4729))\n",
      "(78, ('foreign', 4719))\n",
      "(79, ('want', 4716))\n",
      "(80, ('school', 4556))\n",
      "(81, ('district', 4477))\n",
      "(82, ('meeting', 4428))\n",
      "(83, ('authority', 4399))\n",
      "(84, ('long', 4376))\n",
      "(85, ('photo', 4336))\n",
      "(86, ('war', 4316))\n",
      "(87, ('order', 4282))\n",
      "(88, ('run', 4274))\n",
      "(89, ('follow', 4267))\n",
      "(90, ('worker', 4264))\n",
      "(91, ('programme', 4219))\n",
      "(92, ('price', 4214))\n",
      "(93, ('support', 4101))\n",
      "(94, ('share', 4097))\n",
      "(95, ('chief', 4081))\n",
      "(96, ('kill', 4070))\n",
      "(97, ('found', 4053))\n",
      "(98, ('system', 4046))\n",
      "(99, ('level', 3992))\n",
      "(100, ('play', 3983))\n",
      "(101, ('visit', 3960))\n",
      "(102, ('university', 3960))\n",
      "(103, ('ask', 3935))\n",
      "(104, ('face', 3910))\n",
      "(105, ('ministry', 3846))\n",
      "(106, ('capital', 3791))\n",
      "(107, ('try', 3788))\n",
      "(108, ('office', 3778))\n",
      "(109, ('base', 3762))\n",
      "(110, ('year', 3755))\n",
      "(111, ('education', 3732))\n",
      "(112, ('force', 3730))\n",
      "(113, ('present', 3711))\n",
      "(114, ('act', 3705))\n",
      "(115, ('economic', 3702))\n",
      "(116, ('end', 3685))\n",
      "(117, ('plan', 3677))\n",
      "(118, ('death', 3676))\n",
      "(119, ('medium', 3671))\n",
      "(120, ('held', 3670))\n",
      "(121, ('form', 3660))\n",
      "(122, ('human', 3592))\n",
      "(123, ('policy', 3511))\n",
      "(124, ('building', 3502))\n",
      "(125, ('industry', 3489))\n",
      "(126, ('ensure', 3488))\n",
      "(127, ('hand', 3485))\n",
      "(128, ('nation', 3472))\n",
      "(129, ('large', 3453))\n",
      "(130, ('officer', 3439))\n",
      "(131, ('point', 3437))\n",
      "(132, ('test', 3415))\n",
      "(133, ('attack', 3414))\n",
      "(134, ('committee', 3376))\n",
      "(135, ('body', 3375))\n",
      "(136, ('social', 3365))\n",
      "(137, ('cost', 3363))\n",
      "(138, ('think', 3359))\n",
      "(139, ('money', 3357))\n",
      "(140, ('hospital', 3341))\n",
      "(141, ('charge', 3336))\n",
      "(142, ('export', 3333))\n",
      "(143, ('according', 3324))\n",
      "(144, ('river', 3308))\n",
      "(145, ('total', 3296))\n",
      "(146, ('accord', 3283))\n",
      "(147, ('create', 3225))\n",
      "(148, ('look', 3217))\n",
      "(149, ('food', 3211))\n",
      "(150, ('security', 3210))\n",
      "(151, ('growth', 3208))\n",
      "(152, ('problem', 3198))\n",
      "(153, ('rate', 3184))\n",
      "(154, ('situation', 3162))\n",
      "(155, ('match', 3147))\n",
      "(156, ('crore', 3145))\n",
      "(157, ('low', 3137))\n",
      "(158, ('begin', 3133))\n",
      "(159, ('house', 3127))\n",
      "(160, ('global', 3118))\n",
      "(161, ('arrest', 3103))\n",
      "(162, ('receive', 3058))\n",
      "(163, ('private', 3052))\n",
      "(164, ('great', 3050))\n",
      "(165, ('product', 3037))\n",
      "(166, ('term', 2996))\n",
      "(167, ('remain', 2988))\n",
      "(168, ('organisation', 2985))\n",
      "(169, ('big', 2967))\n",
      "(170, ('centre', 2964))\n",
      "(171, ('information', 2961))\n",
      "(172, ('source', 2946))\n",
      "(173, ('action', 2890))\n",
      "(174, ('department', 2890))\n",
      "(175, ('station', 2884))\n",
      "(176, ('cause', 2858))\n",
      "(177, ('return', 2828))\n",
      "(178, ('press', 2824))\n",
      "(179, ('commission', 2815))\n",
      "(180, ('trade', 2811))\n",
      "(181, ('fire', 2796))\n",
      "(182, ('study', 2775))\n",
      "(183, ('community', 2761))\n",
      "(184, ('process', 2758))\n",
      "(185, ('role', 2743))\n",
      "(186, ('reach', 2741))\n",
      "(187, ('open', 2716))\n",
      "(188, ('decision', 2712))\n",
      "(189, ('star', 2702))\n",
      "(190, ('pay', 2702))\n",
      "(191, ('activity', 2701))\n",
      "(192, ('interest', 2691))\n",
      "(193, ('rise', 2674))\n",
      "(194, ('file', 2662))\n",
      "(195, ('left', 2650))\n",
      "(196, ('continue', 2648))\n",
      "(197, ('later', 2644))\n",
      "(198, ('investment', 2632))\n",
      "(199, ('result', 2610))\n",
      "(200, ('film', 2610))\n",
      "(201, ('major', 2597))\n",
      "(202, ('chairman', 2586))\n",
      "(203, ('free', 2570))\n",
      "(204, ('lose', 2566))\n",
      "(205, ('job', 2527))\n",
      "(206, ('gas', 2516))\n",
      "(207, ('college', 2496))\n",
      "(208, ('question', 2476))\n",
      "(209, ('stop', 2459))\n",
      "(210, ('post', 2455))\n",
      "(211, ('carry', 2449))\n",
      "(212, ('young', 2447))\n",
      "(213, ('important', 2440))\n",
      "(214, ('society', 2439))\n",
      "(215, ('rule', 2426))\n",
      "(216, ('plant', 2394))\n",
      "(217, ('talk', 2393))\n",
      "(218, ('step', 2388))\n",
      "(219, ('future', 2379))\n",
      "(220, ('army', 2379))\n",
      "(221, ('write', 2372))\n",
      "(222, ('release', 2369))\n",
      "(223, ('far', 2365))\n",
      "(224, ('find', 2363))\n",
      "(225, ('head', 2358))\n",
      "(226, ('production', 2355))\n",
      "(227, ('incident', 2351))\n",
      "(228, ('statement', 2349))\n",
      "(229, ('best', 2344))\n",
      "(230, ('involve', 2334))\n",
      "(231, ('import', 2333))\n",
      "(232, ('unit', 2329))\n",
      "(233, ('technology', 2325))\n",
      "(234, ('event', 2324))\n",
      "(235, ('factory', 2318))\n",
      "(236, ('bring', 2317))\n",
      "(237, ('fund', 2312))\n",
      "(238, ('book', 2311))\n",
      "(239, ('special', 2302))\n",
      "(240, ('able', 2298))\n",
      "(241, ('person', 2298))\n",
      "(242, ('men', 2284))\n",
      "(243, ('division', 2282))\n",
      "(244, ('matter', 2270))\n",
      "(245, ('economy', 2269))\n",
      "(246, ('hold', 2265))\n",
      "(247, ('cricket', 2261))\n",
      "(248, ('justice', 2260))\n",
      "(249, ('allow', 2236))\n",
      "(250, ('bus', 2223))\n",
      "(251, ('agency', 2212))\n",
      "(252, ('village', 2211))\n",
      "(253, ('opportunity', 2204))\n",
      "(254, ('teacher', 2202))\n",
      "(255, ('supply', 2198))\n",
      "(256, ('border', 2195))\n",
      "(257, ('poor', 2190))\n",
      "(258, ('regard', 2190))\n",
      "(259, ('condition', 2186))\n",
      "(260, ('produce', 2186))\n",
      "(261, ('consider', 2182))\n",
      "(262, ('father', 2172))\n",
      "(263, ('senior', 2168))\n",
      "(264, ('launch', 2165))\n",
      "(265, ('league', 2163))\n",
      "(266, ('sell', 2155))\n",
      "(267, ('line', 2151))\n",
      "(268, ('traffic', 2143))\n",
      "(269, ('movement', 2139))\n",
      "(270, ('non', 2135))\n",
      "(271, ('expect', 2128))\n",
      "(272, ('crisis', 2127))\n",
      "(273, ('stand', 2124))\n",
      "(274, ('turn', 2113))\n",
      "(275, ('population', 2112))\n",
      "(276, ('research', 2112))\n",
      "(277, ('crime', 2109))\n",
      "(278, ('central', 2099))\n",
      "(279, ('region', 2097))\n",
      "(280, ('believe', 2095))\n",
      "(281, ('lack', 2094))\n",
      "(282, ('initiative', 2090))\n",
      "(283, ('control', 2089))\n",
      "(284, ('award', 2085))\n",
      "(285, ('coronavirus', 2084))\n",
      "(286, ('tax', 2083))\n",
      "(287, ('financial', 2075))\n",
      "(288, ('raise', 2074))\n",
      "(289, ('management', 2070))\n",
      "(290, ('quality', 2067))\n",
      "(291, ('effort', 2067))\n",
      "(292, ('conduct', 2065))\n",
      "(293, ('thing', 2057))\n",
      "(294, ('recently', 2045))\n",
      "(295, ('join', 2043))\n",
      "(296, ('grow', 2041))\n",
      "(297, ('conference', 2034))\n",
      "(298, ('medical', 2033))\n",
      "(299, ('address', 2033))\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(Counter(counts_bd2).most_common(300)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_bd1 = pd.DataFrame(Counter(counts_bd1).most_common(300))\n",
    "df_counts_bd2 = pd.DataFrame(Counter(counts_bd2).most_common(300))\n",
    "df_counts_bd1.to_pickle('df_counts_bd1.pkl')\n",
    "df_counts_bd2.to_pickle('df_counts_bd2.pkl')\n",
    "df_counts_bd2.to_pickle('df_counts_bd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in remove_dict:\n",
    "    if i in counts_bd1:\n",
    "        del counts_bd1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_bd2:\n",
    "        del counts_bd2[i]\n",
    "        \n",
    "        \n",
    "df_counts_bd1 = pd.DataFrame(counts_bd1.most_common(400))\n",
    "df_counts_bd2 = pd.DataFrame(counts_bd2.most_common(400))\n",
    "\n",
    "df_counts_bd1.to_pickle('df_counts_bd1.pkl')\n",
    "df_counts_bd2.to_pickle('df_counts_bd2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = pd.read_pickle('df_ca.pkl')\n",
    "\n",
    "counts_ca1 = {}\n",
    "counts_ca2 = {}\n",
    "\n",
    "wordfreq_ca1 = []\n",
    "wordfreq_ca2 = []\n",
    "\n",
    "for index, row in df_ca.iterrows():\n",
    "    text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "    new_text = replace_content(redict, text_data)\n",
    "    document = nlp(new_text)\n",
    "\n",
    "    [wordfreq_ca1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "    [wordfreq_ca2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_ca1 = Counter(word for word in wordfreq_ca1)\n",
    "counts_ca2 = Counter(word for word in wordfreq_ca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_ca1:\n",
    "        del counts_ca1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_ca2:\n",
    "        del counts_ca2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_ca1:\n",
    "        del counts_ca1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_ca2:\n",
    "        del counts_ca2[i]\n",
    "        \n",
    "df_counts_ca1 = pd.DataFrame(Counter(counts_ca1).most_common(500))\n",
    "df_counts_ca2 = pd.DataFrame(Counter(counts_ca2).most_common(500))\n",
    "\n",
    "df_counts_ca1.to_pickle('df_counts_ca1.pkl')\n",
    "df_counts_ca2.to_pickle('df_counts_ca2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts_ca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('time', 101831))\n",
      "(1, ('people', 96695))\n",
      "(2, ('use', 81684))\n",
      "(3, ('new', 80842))\n",
      "(4, ('like', 78735))\n",
      "(5, ('work', 78459))\n",
      "(6, ('come', 69959))\n",
      "(7, ('company', 62410))\n",
      "(8, ('include', 60010))\n",
      "(9, ('game', 57024))\n",
      "(10, ('look', 55700))\n",
      "(11, ('know', 54572))\n",
      "(12, ('want', 54483))\n",
      "(13, ('right', 52872))\n",
      "(14, ('information', 52481))\n",
      "(15, ('comment', 52001))\n",
      "(16, ('need', 51774))\n",
      "(17, ('way', 50782))\n",
      "(18, ('city', 50557))\n",
      "(19, ('team', 49435))\n",
      "(20, ('government', 49154))\n",
      "(21, ('think', 46044))\n",
      "(22, ('home', 44341))\n",
      "(23, ('help', 43253))\n",
      "(24, ('high', 42619))\n",
      "(25, ('community', 42170))\n",
      "(26, ('good', 40758))\n",
      "(27, ('change', 40473))\n",
      "(28, ('service', 40302))\n",
      "(29, ('business', 39974))\n",
      "(30, ('play', 39818))\n",
      "(31, ('world', 39759))\n",
      "(32, ('life', 39724))\n",
      "(33, ('family', 38974))\n",
      "(34, ('share', 38609))\n",
      "(35, ('start', 38515))\n",
      "(36, ('day', 38415))\n",
      "(37, ('thing', 37975))\n",
      "(38, ('report', 37770))\n",
      "(39, ('police', 36581))\n",
      "(40, ('point', 35368))\n",
      "(41, ('place', 35257))\n",
      "(42, ('school', 34009))\n",
      "(43, ('country', 33729))\n",
      "(44, ('public', 33514))\n",
      "(45, ('end', 33433))\n",
      "(46, ('long', 33171))\n",
      "(47, ('market', 33152))\n",
      "(48, ('lead', 32825))\n",
      "(49, ('big', 32736))\n",
      "(50, ('year', 32363))\n",
      "(51, ('lot', 31606))\n",
      "(52, ('woman', 31536))\n",
      "(53, ('group', 30936))\n",
      "(54, ('support', 30663))\n",
      "(55, ('best', 30567))\n",
      "(56, ('provide', 29887))\n",
      "(57, ('ask', 29445))\n",
      "(58, ('child', 29344))\n",
      "(59, ('health', 29142))\n",
      "(60, ('issue', 28976))\n",
      "(61, ('plan', 28945))\n",
      "(62, ('great', 28936))\n",
      "(63, ('try', 28493))\n",
      "(64, ('base', 28440))\n",
      "(65, ('add', 27849))\n",
      "(66, ('run', 27609))\n",
      "(67, ('program', 27158))\n",
      "(68, ('case', 26813))\n",
      "(69, ('statement', 26541))\n",
      "(70, ('story', 26412))\n",
      "(71, ('student', 26388))\n",
      "(72, ('create', 26362))\n",
      "(73, ('number', 26324))\n",
      "(74, ('member', 26121))\n",
      "(75, ('project', 26058))\n",
      "(76, ('player', 25436))\n",
      "(77, ('found', 25417))\n",
      "(78, ('area', 25245))\n",
      "(79, ('post', 24596))\n",
      "(80, ('find', 24557))\n",
      "(81, ('forward', 24510))\n",
      "(82, ('set', 24473))\n",
      "(83, ('write', 24449))\n",
      "(84, ('told', 24427))\n",
      "(85, ('event', 24366))\n",
      "(86, ('result', 24359))\n",
      "(87, ('continue', 24263))\n",
      "(88, ('open', 24211))\n",
      "(89, ('cost', 23724))\n",
      "(90, ('follow', 23472))\n",
      "(91, ('win', 23329))\n",
      "(92, ('large', 23176))\n",
      "(93, ('system', 23148))\n",
      "(94, ('account', 22814))\n",
      "(95, ('news', 22770))\n",
      "(96, ('release', 22709))\n",
      "(97, ('expect', 22616))\n",
      "(98, ('experience', 22411))\n",
      "(99, ('centre', 22400))\n",
      "(100, ('star', 22309))\n",
      "(101, ('man', 22074))\n",
      "(102, ('young', 22009))\n",
      "(103, ('goal', 21905))\n",
      "(104, ('job', 21871))\n",
      "(105, ('little', 21836))\n",
      "(106, ('feel', 21760))\n",
      "(107, ('president', 21729))\n",
      "(108, ('term', 21597))\n",
      "(109, ('local', 21519))\n",
      "(110, ('begin', 21235))\n",
      "(111, ('state', 21218))\n",
      "(112, ('free', 21158))\n",
      "(113, ('increase', 21028))\n",
      "(114, ('deal', 21025))\n",
      "(115, ('believe', 21020))\n",
      "(116, ('line', 20969))\n",
      "(117, ('left', 20698))\n",
      "(118, ('face', 20547))\n",
      "(119, ('court', 20413))\n",
      "(120, ('note', 20367))\n",
      "(121, ('vancouver', 20279))\n",
      "(122, ('minister', 20238))\n",
      "(123, ('party', 20158))\n",
      "(124, ('head', 20145))\n",
      "(125, ('policy', 20062))\n",
      "(126, ('site', 20016))\n",
      "(127, ('building', 19972))\n",
      "(128, ('care', 19871))\n",
      "(129, ('offer', 19816))\n",
      "(130, ('university', 19756))\n",
      "(131, ('money', 19714))\n",
      "(132, ('price', 19686))\n",
      "(133, ('content', 19337))\n",
      "(134, ('turn', 19309))\n",
      "(135, ('view', 19285))\n",
      "(136, ('street', 19046))\n",
      "(137, ('small', 19016))\n",
      "(138, ('love', 18986))\n",
      "(139, ('order', 18950))\n",
      "(140, ('close', 18838))\n",
      "(141, ('national', 18760))\n",
      "(142, ('talk', 18622))\n",
      "(143, ('different', 18547))\n",
      "(144, ('law', 18544))\n",
      "(145, ('mean', 18442))\n",
      "(146, ('season', 18436))\n",
      "(147, ('able', 18235))\n",
      "(148, ('future', 18182))\n",
      "(149, ('level', 18174))\n",
      "(150, ('question', 18172))\n",
      "(151, ('receive', 18145))\n",
      "(152, ('visit', 18079))\n",
      "(153, ('industry', 18003))\n",
      "(154, ('amp', 17894))\n",
      "(155, ('risk', 17804))\n",
      "(156, ('learn', 17797))\n",
      "(157, ('house', 17667))\n",
      "(158, ('charge', 17655))\n",
      "(159, ('low', 17647))\n",
      "(160, ('pay', 17615))\n",
      "(161, ('interest', 17603))\n",
      "(162, ('grow', 17589))\n",
      "(163, ('return', 17578))\n",
      "(164, ('product', 17476))\n",
      "(165, ('away', 17404))\n",
      "(166, ('development', 17389))\n",
      "(167, ('water', 17326))\n",
      "(168, ('personal', 17321))\n",
      "(169, ('officer', 17320))\n",
      "(170, ('lose', 17259))\n",
      "(171, ('read', 17202))\n",
      "(172, ('hand', 17043))\n",
      "(173, ('live', 17033))\n",
      "(174, ('sell', 16962))\n",
      "(175, ('record', 16907))\n",
      "(176, ('happen', 16879))\n",
      "(177, ('decision', 16854))\n",
      "(178, ('food', 16780))\n",
      "(179, ('book', 16756))\n",
      "(180, ('power', 16701))\n",
      "(181, ('board', 16670))\n",
      "(182, ('old', 16610))\n",
      "(183, ('press', 16602))\n",
      "(184, ('federal', 16564))\n",
      "(185, ('province', 16534))\n",
      "(186, ('problem', 16530))\n",
      "(187, ('kind', 16431))\n",
      "(188, ('study', 16282))\n",
      "(189, ('important', 16255))\n",
      "(190, ('director', 16222))\n",
      "(191, ('allow', 16213))\n",
      "(192, ('hit', 16194))\n",
      "(193, ('friend', 16193))\n",
      "(194, ('agree', 16082))\n",
      "(195, ('ottawa', 16064))\n",
      "(196, ('social', 15982))\n",
      "(197, ('article', 15855))\n",
      "(198, ('sign', 15781))\n",
      "(199, ('real', 15743))\n",
      "(200, ('fact', 15717))\n",
      "(201, ('act', 15645))\n",
      "(202, ('email', 15583))\n",
      "(203, ('leader', 15511))\n",
      "(204, ('hard', 15491))\n",
      "(205, ('medium', 15489))\n",
      "(206, ('far', 15472))\n",
      "(207, ('research', 15458))\n",
      "(208, ('attack', 15409))\n",
      "(209, ('international', 15318))\n",
      "(210, ('rate', 15208))\n",
      "(211, ('tax', 15194))\n",
      "(212, ('financial', 15145))\n",
      "(213, ('staff', 15139))\n",
      "(214, ('bad', 15106))\n",
      "(215, ('cause', 15093))\n",
      "(216, ('opportunity', 14918))\n",
      "(217, ('process', 14892))\n",
      "(218, ('force', 14882))\n",
      "(219, ('office', 14849))\n",
      "(220, ('action', 14779))\n",
      "(221, ('fire', 14774))\n",
      "(222, ('person', 14667))\n",
      "(223, ('car', 14613))\n",
      "(224, ('stop', 14599))\n",
      "(225, ('road', 14382))\n",
      "(226, ('death', 14335))\n",
      "(227, ('score', 14310))\n",
      "(228, ('tell', 14305))\n",
      "(229, ('drive', 14293))\n",
      "(230, ('sale', 14281))\n",
      "(231, ('photo', 14273))\n",
      "(232, ('hope', 14238))\n",
      "(233, ('history', 14117))\n",
      "(234, ('accord', 14065))\n",
      "(235, ('coach', 14017))\n",
      "(236, ('non', 14008))\n",
      "(237, ('security', 13996))\n",
      "(238, ('focus', 13960))\n",
      "(239, ('organization', 13873))\n",
      "(240, ('park', 13845))\n",
      "(241, ('official', 13806))\n",
      "(242, ('understand', 13798))\n",
      "(243, ('raise', 13789))\n",
      "(244, ('oil', 13759))\n",
      "(245, ('period', 13743))\n",
      "(246, ('men', 13701))\n",
      "(247, ('click', 13697))\n",
      "(248, ('kid', 13643))\n",
      "(249, ('consider', 13623))\n",
      "(250, ('past', 13598))\n",
      "(251, ('major', 13585))\n",
      "(252, ('online', 13570))\n",
      "(253, ('thought', 13542))\n",
      "(254, ('global', 13424))\n",
      "(255, ('value', 13378))\n",
      "(256, ('require', 13355))\n",
      "(257, ('customer', 13331))\n",
      "(258, ('bring', 13321))\n",
      "(259, ('guy', 13301))\n",
      "(260, ('let', 13290))\n",
      "(261, ('hold', 13236))\n",
      "(262, ('hockey', 13222))\n",
      "(263, ('fund', 13164))\n",
      "(264, ('sure', 13160))\n",
      "(265, ('available', 13138))\n",
      "(266, ('announce', 13124))\n",
      "(267, ('postmedia', 13086))\n",
      "(268, ('property', 13053))\n",
      "(269, ('store', 13040))\n",
      "(270, ('room', 13028))\n",
      "(271, ('watch', 13006))\n",
      "(272, ('idea', 12956))\n",
      "(273, ('final', 12919))\n",
      "(274, ('chief', 12888))\n",
      "(275, ('technology', 12883))\n",
      "(276, ('space', 12798))\n",
      "(277, ('appear', 12796))\n",
      "(278, ('trade', 12785))\n",
      "(279, ('series', 12774))\n",
      "(280, ('meeting', 12742))\n",
      "(281, ('general', 12725))\n",
      "(282, ('serve', 12710))\n",
      "(283, ('involve', 12703))\n",
      "(284, ('shot', 12681))\n",
      "(285, ('reason', 12643))\n",
      "(286, ('control', 12616))\n",
      "(287, ('related', 12598))\n",
      "(288, ('course', 12530))\n",
      "(289, ('land', 12522))\n",
      "(290, ('bank', 12466))\n",
      "(291, ('strong', 12449))\n",
      "(292, ('management', 12399))\n",
      "(293, ('data', 12377))\n",
      "(294, ('energy', 12353))\n",
      "(295, ('access', 12342))\n",
      "(296, ('list', 12297))\n",
      "(297, ('growth', 12263))\n",
      "(298, ('encourage', 12262))\n",
      "(299, ('film', 12255))\n",
      "(300, ('role', 12249))\n"
     ]
    }
   ],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_ca2 = pd.DataFrame(Counter(counts_ca2).most_common(300))\n",
    "df_counts_ca2.to_pickle('df_counts_ca.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb = pd.read_pickle('df_gb.pkl')\n",
    "\n",
    "counts_gb1 = {}\n",
    "counts_gb2 = {}\n",
    "\n",
    "wordfreq_gb1 = []\n",
    "wordfreq_gb2 = []\n",
    "\n",
    "for index, row in df_gb.iterrows():\n",
    "    text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "    new_text = replace_content(redict, text_data)\n",
    "    document = nlp(new_text)\n",
    "\n",
    "    [wordfreq_gb1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "    [wordfreq_gb2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_gb1 = Counter(word for word in wordfreq_gb1)\n",
    "counts_gb2 = Counter(word for word in wordfreq_gb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_gb1:\n",
    "        del counts_gb1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_gb2:\n",
    "        del counts_gb2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_gb1:\n",
    "        del counts_gb1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_gb2:\n",
    "        del counts_gb2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_gb1 = pd.DataFrame(Counter(counts_gb1).most_common(500))\n",
    "df_counts_gb2 = pd.DataFrame(Counter(counts_gb2).most_common(500))\n",
    "\n",
    "df_counts_gb1.to_pickle('df_counts_gb1.pkl')\n",
    "df_counts_gb2.to_pickle('df_counts_gb2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gh = pd.read_pickle('df_gh.pkl')\n",
    "\n",
    "counts_gh1 = {}\n",
    "counts_gh2 = {}\n",
    "\n",
    "wordfreq_gh1 = []\n",
    "wordfreq_gh2 = []\n",
    "\n",
    "for index, row in df_gh.iterrows():\n",
    "    text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "    new_text = replace_content(redict, text_data)\n",
    "    document = nlp(new_text)\n",
    "\n",
    "    [wordfreq_gh1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "    [wordfreq_gh2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_gh1 = Counter(word for word in wordfreq_gh1)\n",
    "counts_gh2 = Counter(word for word in wordfreq_gh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_gh1:\n",
    "        del counts_gh1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_gh2:\n",
    "        del counts_gh2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_gh1:\n",
    "        del counts_gh1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_gh2:\n",
    "        del counts_gh2[i]\n",
    "        \n",
    "df_counts_gh1 = pd.DataFrame(Counter(counts_gh1).most_common(500))\n",
    "df_counts_gh2 = pd.DataFrame(Counter(counts_gh2).most_common(500))\n",
    "\n",
    "df_counts_gh1.to_pickle('df_counts_gh1.pkl')\n",
    "df_counts_gh2.to_pickle('df_counts_gh2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 01 Minutes : 58 Seconds'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_hk = pd.read_pickle('df_hk.pkl')\n",
    "\n",
    "counts_hk1 = {}\n",
    "counts_hk2 = {}\n",
    "\n",
    "wordfreq_hk1 = []\n",
    "wordfreq_hk2 = []\n",
    "\n",
    "for index, row in df_hk.iterrows():\n",
    "    text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "    new_text = replace_content(redict, text_data)\n",
    "    document = nlp(new_text)\n",
    "\n",
    "    [wordfreq_hk1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "    [wordfreq_hk2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_hk1 = Counter(word for word in wordfreq_hk1)\n",
    "counts_hk2 = Counter(word for word in wordfreq_hk2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_hk1:\n",
    "        del counts_hk1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_hk2:\n",
    "        del counts_hk2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_hk1:\n",
    "        del counts_hk1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_hk2:\n",
    "        del counts_hk2[i]\n",
    "        \n",
    "df_counts_hk1 = pd.DataFrame(Counter(counts_hk1).most_common(500))\n",
    "df_counts_hk2 = pd.DataFrame(Counter(counts_hk2).most_common(500))\n",
    "\n",
    "df_counts_hk1.to_pickle('df_counts_hk1.pkl')\n",
    "df_counts_hk2.to_pickle('df_counts_hk2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 34 Minutes : 31 Seconds'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_ie = pd.read_pickle('df_ie.pkl')\n",
    "\n",
    "counts_ie1 = {}\n",
    "counts_ie2 = {}\n",
    "\n",
    "wordfreq_ie1 = []\n",
    "wordfreq_ie2 = []\n",
    "\n",
    "for index, row in df_ie.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_ie1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_ie2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_ie1 = Counter(word for word in wordfreq_ie1)\n",
    "counts_ie2 = Counter(word for word in wordfreq_ie2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_ie1:\n",
    "        del counts_ie1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_ie2:\n",
    "        del counts_ie2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_ie1:\n",
    "        del counts_ie1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_ie2:\n",
    "        del counts_ie2[i]\n",
    "        \n",
    "df_counts_ie1 = pd.DataFrame(Counter(counts_ie1).most_common(500))\n",
    "df_counts_ie2 = pd.DataFrame(Counter(counts_ie2).most_common(500))\n",
    "\n",
    "df_counts_ie1.to_pickle('df_counts_ie1.pkl')\n",
    "df_counts_ie2.to_pickle('df_counts_ie2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 44 Minutes : 32 Seconds'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_in = pd.read_pickle('df_in.pkl')\n",
    "\n",
    "counts_in1 = {}\n",
    "counts_in2 = {}\n",
    "\n",
    "wordfreq_in1 = []\n",
    "wordfreq_in2 = []\n",
    "\n",
    "for index, row in df_in.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_in1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_in2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_in1 = Counter(word for word in wordfreq_in1)\n",
    "counts_in2 = Counter(word for word in wordfreq_in2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\", \\\n",
    "               'tk', 'bnp', 'bangabandhu', 'bangladesh', \\\n",
    "               'rahman', 'hossain', 'hasina', 'upazila', \\\n",
    "               'bangabandhu', 'bangla', 'indian', 'myanmar', \\\n",
    "               'chittagong', 'khan', 'rohingya', 'chowdhury'\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_in1:\n",
    "        del counts_in1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_in2:\n",
    "        del counts_in2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_in1:\n",
    "        del counts_in1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_in2:\n",
    "        del counts_in2[i]\n",
    "        \n",
    "df_counts_in1 = pd.DataFrame(Counter(counts_in1).most_common(500))\n",
    "df_counts_in2 = pd.DataFrame(Counter(counts_in2).most_common(500))\n",
    "\n",
    "df_counts_in1.to_pickle('df_counts_in1.pkl')\n",
    "df_counts_in2.to_pickle('df_counts_in2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 19 Minutes : 29 Seconds'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_jm = pd.read_pickle('df_jm.pkl')\n",
    "\n",
    "counts_jm1 = {}\n",
    "counts_jm2 = {}\n",
    "\n",
    "wordfreq_jm1 = []\n",
    "wordfreq_jm2 = []\n",
    "\n",
    "for index, row in df_jm.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_jm1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_jm2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_jm1 = Counter(word for word in wordfreq_jm1)\n",
    "counts_jm2 = Counter(word for word in wordfreq_jm2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_jm1:\n",
    "        del counts_jm1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_jm2:\n",
    "        del counts_jm2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_jm1:\n",
    "        del counts_jm1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_jm2:\n",
    "        del counts_jm2[i]\n",
    "        \n",
    "df_counts_jm1 = pd.DataFrame(Counter(counts_jm1).most_common(500))\n",
    "df_counts_jm2 = pd.DataFrame(Counter(counts_jm2).most_common(500))\n",
    "\n",
    "df_counts_jm1.to_pickle('df_counts_jm1.pkl')\n",
    "df_counts_jm2.to_pickle('df_counts_jm2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 15 Minutes : 07 Seconds'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_ke = pd.read_pickle('df_ke.pkl')\n",
    "\n",
    "counts_ke1 = {}\n",
    "counts_ke2 = {}\n",
    "\n",
    "wordfreq_ke1 = []\n",
    "wordfreq_ke2 = []\n",
    "\n",
    "for index, row in df_ke.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_ke1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_ke2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_ke1 = Counter(word for word in wordfreq_ke1)\n",
    "counts_ke2 = Counter(word for word in wordfreq_ke2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_ke1:\n",
    "        del counts_ke1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_ke2:\n",
    "        del counts_ke2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_ke1:\n",
    "        del counts_ke1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_ke2:\n",
    "        del counts_ke2[i]\n",
    "        \n",
    "df_counts_ke1 = pd.DataFrame(Counter(counts_ke1).most_common(500))\n",
    "df_counts_ke2 = pd.DataFrame(Counter(counts_ke2).most_common(500))\n",
    "\n",
    "df_counts_ke1.to_pickle('df_counts_ke1.pkl')\n",
    "df_counts_ke2.to_pickle('df_counts_ke2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 07 Minutes : 45 Seconds'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_lk = pd.read_pickle('df_lk.pkl')\n",
    "\n",
    "counts_lk1 = {}\n",
    "counts_lk2 = {}\n",
    "\n",
    "wordfreq_lk1 = []\n",
    "wordfreq_lk2 = []\n",
    "\n",
    "for index, row in df_lk.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_lk1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_lk2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_lk1 = Counter(word for word in wordfreq_lk1)\n",
    "counts_lk2 = Counter(word for word in wordfreq_lk2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_lk1:\n",
    "        del counts_lk1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_lk2:\n",
    "        del counts_lk2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_lk1:\n",
    "        del counts_lk1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_lk2:\n",
    "        del counts_lk2[i]\n",
    "        \n",
    "df_counts_lk1 = pd.DataFrame(Counter(counts_lk1).most_common(500))\n",
    "df_counts_lk2 = pd.DataFrame(Counter(counts_lk2).most_common(500))\n",
    "\n",
    "df_counts_lk1.to_pickle('df_counts_lk1.pkl')\n",
    "df_counts_lk2.to_pickle('df_counts_lk2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 18 Minutes : 24 Seconds'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_my = pd.read_pickle('df_my.pkl')\n",
    "\n",
    "counts_my1 = {}\n",
    "counts_my2 = {}\n",
    "\n",
    "wordfreq_my1 = []\n",
    "wordfreq_my2 = []\n",
    "\n",
    "for index, row in df_my.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_my1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_my2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_my1 = Counter(word for word in wordfreq_my1)\n",
    "counts_my2 = Counter(word for word in wordfreq_my2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_my1:\n",
    "        del counts_my1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_my2:\n",
    "        del counts_my2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_my1:\n",
    "        del counts_my1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_my2:\n",
    "        del counts_my2[i]\n",
    "        \n",
    "df_counts_my1 = pd.DataFrame(Counter(counts_my1).most_common(500))\n",
    "df_counts_my2 = pd.DataFrame(Counter(counts_my2).most_common(500))\n",
    "\n",
    "df_counts_my1.to_pickle('df_counts_my1.pkl')\n",
    "df_counts_my2.to_pickle('df_counts_my2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 53 Minutes : 38 Seconds'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_ng = pd.read_pickle('df_ng.pkl')\n",
    "\n",
    "counts_ng1 = {}\n",
    "counts_ng2 = {}\n",
    "\n",
    "wordfreq_ng1 = []\n",
    "wordfreq_ng2 = []\n",
    "\n",
    "for index, row in df_ng.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_ng1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_ng2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_ng1 = Counter(word for word in wordfreq_ng1)\n",
    "counts_ng2 = Counter(word for word in wordfreq_ng2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_ng1:\n",
    "        del counts_ng1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_ng2:\n",
    "        del counts_ng2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_ng1:\n",
    "        del counts_ng1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_ng2:\n",
    "        del counts_ng2[i]\n",
    "        \n",
    "df_counts_ng1 = pd.DataFrame(Counter(counts_ng1).most_common(500))\n",
    "df_counts_ng2 = pd.DataFrame(Counter(counts_ng2).most_common(500))\n",
    "\n",
    "df_counts_ng1.to_pickle('df_counts_ng1.pkl')\n",
    "df_counts_ng2.to_pickle('df_counts_ng2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 35 Minutes : 50 Seconds'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_nz = pd.read_pickle('df_nz.pkl')\n",
    "\n",
    "counts_nz1 = {}\n",
    "counts_nz2 = {}\n",
    "\n",
    "wordfreq_nz1 = []\n",
    "wordfreq_nz2 = []\n",
    "\n",
    "for index, row in df_nz.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_nz1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_nz2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_nz1 = Counter(word for word in wordfreq_nz1)\n",
    "counts_nz2 = Counter(word for word in wordfreq_nz2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\", \n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_nz1:\n",
    "        del counts_nz1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_nz2:\n",
    "        del counts_nz2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_nz1:\n",
    "        del counts_nz1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_nz2:\n",
    "        del counts_nz2[i]\n",
    "        \n",
    "df_counts_nz1 = pd.DataFrame(Counter(counts_nz1).most_common(500))\n",
    "df_counts_nz2 = pd.DataFrame(Counter(counts_nz2).most_common(500))\n",
    "\n",
    "df_counts_nz1.to_pickle('df_counts_nz1.pkl')\n",
    "df_counts_nz2.to_pickle('df_counts_nz2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 54 Minutes : 00 Seconds'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_ph = pd.read_pickle('df_ph.pkl')\n",
    "\n",
    "counts_ph1 = {}\n",
    "counts_ph2 = {}\n",
    "\n",
    "wordfreq_ph1 = []\n",
    "wordfreq_ph2 = []\n",
    "\n",
    "for index, row in df_ph.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_ph1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_ph2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_ph1 = Counter(word for word in wordfreq_ph1)\n",
    "counts_ph2 = Counter(word for word in wordfreq_ph2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_ph1:\n",
    "        del counts_ph1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_ph2:\n",
    "        del counts_ph2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_ph1:\n",
    "        del counts_ph1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_ph2:\n",
    "        del counts_ph2[i]\n",
    "        \n",
    "df_counts_ph1 = pd.DataFrame(Counter(counts_ph1).most_common(500))\n",
    "df_counts_ph2 = pd.DataFrame(Counter(counts_ph2).most_common(500))\n",
    "\n",
    "df_counts_ph1.to_pickle('df_counts_ph1.pkl')\n",
    "df_counts_ph2.to_pickle('df_counts_ph2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 10 Minutes : 40 Seconds'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_sg = pd.read_pickle('df_sg.pkl')\n",
    "\n",
    "counts_sg1 = {}\n",
    "counts_sg2 = {}\n",
    "\n",
    "wordfreq_sg1 = []\n",
    "wordfreq_sg2 = []\n",
    "\n",
    "for index, row in df_sg.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_sg1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_sg2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_sg1 = Counter(word for word in wordfreq_sg1)\n",
    "counts_sg2 = Counter(word for word in wordfreq_sg2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_sg1:\n",
    "        del counts_sg1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_sg2:\n",
    "        del counts_sg2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_sg1:\n",
    "        del counts_sg1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_sg2:\n",
    "        del counts_sg2[i]\n",
    "        \n",
    "df_counts_sg1 = pd.DataFrame(Counter(counts_sg1).most_common(500))\n",
    "df_counts_sg2 = pd.DataFrame(Counter(counts_sg2).most_common(500))\n",
    "\n",
    "df_counts_sg1.to_pickle('df_counts_sg1.pkl')\n",
    "df_counts_sg2.to_pickle('df_counts_sg2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 00 Hours : 03 Minutes : 25 Seconds'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One article greater than 1000000 characters, preventing NLP from compiling, article removed\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_tz = pd.read_pickle('df_tz.pkl')\n",
    "\n",
    "counts_tz1 = {}\n",
    "counts_tz2 = {}\n",
    "\n",
    "wordfreq_tz1 = []\n",
    "wordfreq_tz2 = []\n",
    "\n",
    "for index, row in df_tz.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str) and (len(row['article_text_Ngram_stopword_lemmatize']) < 1000000): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_tz1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_tz2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_tz1 = Counter(word for word in wordfreq_tz1)\n",
    "counts_tz2 = Counter(word for word in wordfreq_tz2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_tz1:\n",
    "        del counts_tz1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_tz2:\n",
    "        del counts_tz2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_tz1:\n",
    "        del counts_tz1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_tz2:\n",
    "        del counts_tz2[i]\n",
    "        \n",
    "df_counts_tz1 = pd.DataFrame(Counter(counts_tz1).most_common(500))\n",
    "df_counts_tz2 = pd.DataFrame(Counter(counts_tz2).most_common(500))\n",
    "\n",
    "df_counts_tz1.to_pickle('df_counts_tz1.pkl')\n",
    "df_counts_tz2.to_pickle('df_counts_tz2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 01 Hours : 49 Minutes : 16 Seconds'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_us = pd.read_pickle('df_us.pkl')\n",
    "\n",
    "counts_us1 = {}\n",
    "counts_us2 = {}\n",
    "\n",
    "wordfreq_us1 = []\n",
    "wordfreq_us2 = []\n",
    "\n",
    "for index, row in df_us.iterrows():\n",
    "    if isinstance(row['article_text_Ngram_stopword_lemmatize'], str): \n",
    "        text_data = row['article_text_Ngram_stopword_lemmatize'].lower()\n",
    "        new_text = replace_content(redict, text_data)\n",
    "        document = nlp(new_text)\n",
    "\n",
    "        [wordfreq_us1.append(ent.text) for ent in document if ent.ent_type_]\n",
    "        [wordfreq_us2.append(ent.text) for ent in document if not ent.ent_type_]\n",
    "        \n",
    "        \n",
    "counts_us1 = Counter(word for word in wordfreq_us1)\n",
    "counts_us2 = Counter(word for word in wordfreq_us2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"Runtime: %H Hours : %M Minutes : %S Seconds\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = [\"'s\", \"n't\", \"'re\", \"u\", \"ve\", \\\n",
    "               \"'\", \"''\", \"'m\", \"/\", \"'ll\", \\\n",
    "               \"*\", \"'d\", \"'ve\", \"m\", \"mr\", \\\n",
    "               \"ms\", \"dr\", \"mrs\", \".\"\n",
    "              ]\n",
    "\n",
    "for i in remove_dict:\n",
    "    if i in counts_us1:\n",
    "        del counts_us1[i]\n",
    "        \n",
    "for i in remove_dict:\n",
    "    if i in counts_us2:\n",
    "        del counts_us2[i]\n",
    "    \n",
    "for i in stopwords:\n",
    "    if i in counts_us1:\n",
    "        del counts_us1[i]\n",
    "        \n",
    "for i in stopwords:\n",
    "    if i in counts_us2:\n",
    "        del counts_us2[i]\n",
    "        \n",
    "df_counts_us1 = pd.DataFrame(Counter(counts_us1).most_common(500))\n",
    "df_counts_us2 = pd.DataFrame(Counter(counts_us2).most_common(500))\n",
    "\n",
    "df_counts_us1.to_pickle('df_counts_us1.pkl')\n",
    "df_counts_us2.to_pickle('df_counts_us2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_au1 = pd.read_pickle('df_counts_au1.pkl')\n",
    "df_au2 = pd.read_pickle('df_counts_au2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = dict(zip(df_au2[0], df_au2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 269815),\n",
       " ('i', 254188),\n",
       " ('say', 239182),\n",
       " ('it', 105241),\n",
       " ('time', 99918),\n",
       " ('get', 96721),\n",
       " ('would', 96121),\n",
       " ('make', 95997),\n",
       " ('go', 87308),\n",
       " ('people', 83357),\n",
       " ('also', 77914),\n",
       " ('like', 73990),\n",
       " ('take', 70096),\n",
       " ('new', 69815),\n",
       " ('well', 67913),\n",
       " ('work', 65096),\n",
       " ('use', 63113),\n",
       " ('we', 61148),\n",
       " ('come', 57492),\n",
       " ('see', 55034),\n",
       " ('in', 53390),\n",
       " ('could', 52368),\n",
       " ('but', 52294),\n",
       " ('need', 51243),\n",
       " ('government', 51129),\n",
       " ('look', 49322),\n",
       " ('a', 48396),\n",
       " ('he', 47798),\n",
       " ('mr', 47673),\n",
       " ('think', 47609),\n",
       " ('back', 46378),\n",
       " ('know', 46152),\n",
       " ('way', 45194),\n",
       " ('include', 45178),\n",
       " ('this', 43424),\n",
       " ('game', 43195),\n",
       " ('good', 41740),\n",
       " ('want', 41397),\n",
       " ('give', 40138),\n",
       " ('even', 39772),\n",
       " ('one', 39453),\n",
       " ('high', 39172),\n",
       " ('market', 38863),\n",
       " ('world', 38654),\n",
       " ('many', 38414),\n",
       " ('much', 38028),\n",
       " ('show', 37106),\n",
       " ('company', 36958),\n",
       " ('may', 36352),\n",
       " ('change', 36302),\n",
       " ('thing', 34861),\n",
       " ('big', 34666),\n",
       " ('and', 34662),\n",
       " ('right', 34097),\n",
       " ('business', 33743),\n",
       " ('still', 33432),\n",
       " ('report', 32855),\n",
       " ('team', 32075),\n",
       " ('start', 31979),\n",
       " ('if', 31768),\n",
       " ('day', 31683),\n",
       " ('home', 31660),\n",
       " ('really', 31612),\n",
       " ('there', 31611),\n",
       " ('around', 31448),\n",
       " ('information', 31310),\n",
       " ('great', 30983),\n",
       " ('play', 30863),\n",
       " ('state', 30849),\n",
       " ('life', 30432),\n",
       " ('provide', 29711),\n",
       " ('service', 29554),\n",
       " ('part', 29359),\n",
       " ('point', 28701),\n",
       " ('car', 28522),\n",
       " ('they', 28280),\n",
       " ('long', 28221),\n",
       " ('call', 27876),\n",
       " ('best', 27873),\n",
       " ('country', 27651),\n",
       " ('help', 27638),\n",
       " ('year', 27623),\n",
       " ('end', 26499),\n",
       " ('place', 25950),\n",
       " ('run', 25677),\n",
       " ('family', 25601),\n",
       " ('you', 25366),\n",
       " ('support', 25273),\n",
       " ('public', 24968),\n",
       " ('put', 24968),\n",
       " ('lot', 24948),\n",
       " ('number', 24731),\n",
       " ('lead', 24718),\n",
       " ('player', 24558),\n",
       " ('since', 24547),\n",
       " ('price', 24464),\n",
       " ('woman', 24353),\n",
       " ('issue', 24159),\n",
       " ('share', 24125),\n",
       " ('that', 24093),\n",
       " ('community', 23635),\n",
       " ('try', 23485),\n",
       " ('child', 23364),\n",
       " ('set', 23218),\n",
       " ('group', 23206),\n",
       " ('another', 22938),\n",
       " ('told', 22611),\n",
       " ('become', 22608),\n",
       " ('as', 22235),\n",
       " ('local', 22196),\n",
       " ('cost', 22151),\n",
       " ('industry', 22082),\n",
       " ('site', 21816),\n",
       " ('system', 21351),\n",
       " ('win', 21263),\n",
       " ('base', 21223),\n",
       " ('move', 20940),\n",
       " ('health', 20934),\n",
       " ('us', 20774),\n",
       " ('case', 20720),\n",
       " ('medium', 20647),\n",
       " ('so', 20502),\n",
       " ('increase', 20455),\n",
       " ('term', 20366),\n",
       " ('find', 20038),\n",
       " ('found', 19982),\n",
       " ('area', 19960),\n",
       " ('something', 19882),\n",
       " ('top', 19834),\n",
       " ('police', 19828),\n",
       " ('interest', 19633),\n",
       " ('school', 19563),\n",
       " ('little', 19516),\n",
       " ('level', 19255),\n",
       " ('do', 19248),\n",
       " ('never', 19146),\n",
       " ('follow', 19127),\n",
       " ('power', 19091),\n",
       " ('might', 19039),\n",
       " ('low', 19038),\n",
       " ('large', 19031),\n",
       " ('news', 18993),\n",
       " ('every', 18903),\n",
       " ('test', 18887),\n",
       " ('mean', 18846),\n",
       " ('minister', 18782),\n",
       " ('believe', 18754),\n",
       " ('across', 18704),\n",
       " ('real', 18676),\n",
       " ('date', 18635),\n",
       " ('however', 18478),\n",
       " ('expect', 18475),\n",
       " ('city', 18469),\n",
       " ('for', 18465),\n",
       " ('result', 18457),\n",
       " ('keep', 18381),\n",
       " ('project', 18281),\n",
       " ('small', 18203),\n",
       " ('policy', 18170),\n",
       " ('young', 18126),\n",
       " ('what', 17941),\n",
       " ('love', 17904),\n",
       " ('plan', 17880),\n",
       " ('different', 17824),\n",
       " ('feel', 17811),\n",
       " ('man', 17619),\n",
       " ('problem', 17559),\n",
       " ('final', 17489),\n",
       " ('head', 17423),\n",
       " ('club', 17408),\n",
       " ('research', 17367),\n",
       " ('return', 17356),\n",
       " ('national', 17322),\n",
       " ('story', 17094),\n",
       " ('always', 17052),\n",
       " ('she', 16936),\n",
       " ('without', 16920),\n",
       " ('job', 16883),\n",
       " ('open', 16853),\n",
       " ('line', 16847),\n",
       " ('side', 16769),\n",
       " ('face', 16694),\n",
       " ('turn', 16683),\n",
       " ('money', 16639),\n",
       " ('rate', 16574),\n",
       " ('court', 16314),\n",
       " ('far', 16309),\n",
       " ('free', 16302),\n",
       " ('future', 16296),\n",
       " ('able', 16132),\n",
       " ('away', 16086),\n",
       " ('act', 16069),\n",
       " ('continue', 15934),\n",
       " ('name', 15863),\n",
       " ('pay', 15848),\n",
       " ('law', 15764),\n",
       " ('ask', 15747),\n",
       " ('left', 15627),\n",
       " ('sell', 15615),\n",
       " ('lose', 15612),\n",
       " ('already', 15609),\n",
       " ('important', 15552),\n",
       " ('when', 15526),\n",
       " ('seem', 15465),\n",
       " ('drive', 15453),\n",
       " ('party', 15425),\n",
       " ('member', 15422),\n",
       " ('event', 15399),\n",
       " ('bad', 15376),\n",
       " ('deal', 15365),\n",
       " ('international', 15262),\n",
       " ('record', 15250),\n",
       " ('live', 15179),\n",
       " ('former', 15152),\n",
       " ('bit', 15094),\n",
       " ('add', 15051),\n",
       " ('ca', 15032),\n",
       " ('experience', 15014),\n",
       " ('hard', 14958),\n",
       " ('decision', 14897),\n",
       " ('hand', 14875),\n",
       " ('match', 14851),\n",
       " ('write', 14841),\n",
       " ('program', 14684),\n",
       " ('social', 14676),\n",
       " ('talk', 14674),\n",
       " ('whether', 14669),\n",
       " ('ms', 14659),\n",
       " ('question', 14630),\n",
       " ('per', 14602),\n",
       " ('risk', 14541),\n",
       " ('amp', 14530),\n",
       " ('centre', 14525),\n",
       " ('road', 14450),\n",
       " ('old', 14443),\n",
       " ('fact', 14390),\n",
       " ('offer', 14382),\n",
       " ('content', 14280),\n",
       " ('release', 14256),\n",
       " ('actually', 14246),\n",
       " ('view', 14241),\n",
       " ('yet', 14108),\n",
       " ('enough', 13975),\n",
       " ('house', 13941),\n",
       " ('receive', 13932),\n",
       " ('design', 13900),\n",
       " ('less', 13802),\n",
       " ('tax', 13795),\n",
       " ('water', 13788),\n",
       " ('major', 13761),\n",
       " ('close', 13756),\n",
       " ('full', 13754),\n",
       " ('last', 13737),\n",
       " ('happen', 13735),\n",
       " ('strong', 13495),\n",
       " ('create', 13417),\n",
       " ('product', 13381),\n",
       " ('growth', 13373),\n",
       " ('development', 13263),\n",
       " ('force', 13202),\n",
       " ('energy', 13087),\n",
       " ('ever', 13013),\n",
       " ('watch', 13012),\n",
       " ('grow', 12977),\n",
       " ('next', 12966),\n",
       " ('sydney', 12923),\n",
       " ('require', 12895),\n",
       " ('access', 12895),\n",
       " ('now', 12856),\n",
       " ('let', 12788),\n",
       " ('investment', 12751),\n",
       " ('form', 12735),\n",
       " ('though', 12712),\n",
       " ('opportunity', 12696),\n",
       " ('while', 12692),\n",
       " ('comment', 12656),\n",
       " ('photo', 12648),\n",
       " ('sport', 12641),\n",
       " ('within', 12637),\n",
       " ('likely', 12634),\n",
       " ('user', 12590),\n",
       " ('control', 12581),\n",
       " ('allow', 12566),\n",
       " ('available', 12565),\n",
       " ('tell', 12557),\n",
       " ('consider', 12535),\n",
       " ('model', 12534),\n",
       " ('global', 12530),\n",
       " ('role', 12475),\n",
       " ('reason', 12454),\n",
       " ('current', 12428),\n",
       " ('value', 12378),\n",
       " ('key', 12337),\n",
       " ('focus', 12281),\n",
       " ('race', 12272),\n",
       " ('often', 12261),\n",
       " ('friend', 12259),\n",
       " ('process', 12229),\n",
       " ('begin', 12198),\n",
       " ('sale', 12187)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_dict).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au, bd, ca, gb, gh, hk, ie, 'in', jm, ke, lk, my, ng, nz, ph, sg, tz, us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd1 = pd.read_pickle('df_counts_bd1.pkl')\n",
    "df_bd2 = pd.read_pickle('df_counts_bd2.pkl')\n",
    "counts_bd2 = dict(zip(df_bd1[0], df_bd1[1]))\n",
    "counts_bd2 = dict(zip(df_bd2[0], df_bd2[1]))\n",
    "\n",
    "for i in enumerate(Counter(counts_bd2).most_common()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('the', 65882))\n",
      "(1, ('say', 62243))\n",
      "(2, ('bangladesh', 46684))\n",
      "(3, ('also', 23693))\n",
      "(4, ('country', 22510))\n",
      "(5, ('i', 21203))\n",
      "(6, ('government', 20966))\n",
      "(7, ('people', 19966))\n",
      "(8, ('take', 17661))\n",
      "(9, ('make', 14395))\n",
      "(10, ('he', 13801))\n",
      "(11, ('would', 13464))\n",
      "(12, ('we', 12738))\n",
      "(13, ('it', 12664))\n",
      "(14, ('minister', 12601))\n",
      "(15, ('time', 12261))\n",
      "(16, ('in', 11991))\n",
      "(17, ('work', 11527))\n",
      "(18, ('go', 10421))\n",
      "(19, ('police', 10164))\n",
      "(20, ('get', 9947))\n",
      "(21, ('come', 9810))\n",
      "(22, ('a', 9730))\n",
      "(23, ('high', 9434))\n",
      "(24, ('use', 8819))\n",
      "(25, ('new', 8614))\n",
      "(26, ('but', 8401))\n",
      "(27, ('many', 8383))\n",
      "(28, ('well', 8348))\n",
      "(29, ('include', 8222))\n",
      "(30, ('case', 8117))\n",
      "(31, ('add', 8097))\n",
      "(32, ('area', 7970))\n",
      "(33, ('world', 7732))\n",
      "(34, ('need', 7700))\n",
      "(35, ('give', 7644))\n",
      "(36, ('bank', 7421))\n",
      "(37, ('official', 7324))\n",
      "(38, ('report', 7164))\n",
      "(39, ('day', 7131))\n",
      "(40, ('like', 7089))\n",
      "(41, ('student', 7014))\n",
      "(42, ('party', 6932))\n",
      "(43, ('state', 6917))\n",
      "(44, ('law', 6701))\n",
      "(45, ('project', 6697))\n",
      "(46, ('could', 6640))\n",
      "(47, ('this', 6634))\n",
      "(48, ('they', 6613))\n",
      "(49, ('member', 6590))\n",
      "(50, ('development', 6557))\n",
      "(51, ('told', 6551))\n",
      "(52, ('international', 6515))\n",
      "(53, ('leader', 6462))\n",
      "(54, ('issue', 6457))\n",
      "(55, ('national', 6452))\n",
      "(56, ('even', 6337))\n",
      "(57, ('number', 6256))\n",
      "(58, ('however', 6235))\n",
      "(59, ('city', 6221))\n",
      "(60, ('general', 5950))\n",
      "(61, ('tk', 5947))\n",
      "(62, ('power', 5937))\n",
      "(63, ('around', 5875))\n",
      "(64, ('woman', 5865))\n",
      "(65, ('local', 5832))\n",
      "(66, ('start', 5831))\n",
      "(67, ('court', 5822))\n",
      "(68, ('prime', 5749))\n",
      "(69, ('market', 5736))\n",
      "(70, ('one', 5732))\n",
      "(71, ('child', 5697))\n",
      "(72, ('life', 5680))\n",
      "(73, ('company', 5640))\n",
      "(74, ('sector', 5576))\n",
      "(75, ('right', 5575))\n",
      "(76, ('service', 5541))\n",
      "(77, ('may', 5530))\n",
      "(78, ('water', 5507))\n",
      "(79, ('good', 5485))\n",
      "(80, ('place', 5472))\n",
      "(81, ('bnp', 5439))\n",
      "(82, ('political', 5308))\n",
      "(83, ('increase', 5303))\n",
      "(84, ('election', 5273))\n",
      "(85, ('see', 5152))\n",
      "(86, ('president', 5125))\n",
      "(87, ('land', 5105))\n",
      "(88, ('director', 5096))\n",
      "(89, ('since', 5091))\n",
      "(90, ('know', 5079))\n",
      "(91, ('home', 5075))\n",
      "(92, ('public', 5063))\n",
      "(93, ('different', 5053))\n",
      "(94, ('part', 5047))\n",
      "(95, ('group', 5040))\n",
      "(96, ('set', 5035))\n",
      "(97, ('help', 5007))\n",
      "(98, ('lead', 4998))\n",
      "(99, ('family', 4970))\n",
      "(100, ('road', 4951))\n",
      "(101, ('provide', 4943))\n",
      "(102, ('secretary', 4934))\n",
      "(103, ('change', 4895))\n",
      "(104, ('team', 4866))\n",
      "(105, ('due', 4852))\n",
      "(106, ('health', 4846))\n",
      "(107, ('demand', 4832))\n",
      "(108, ('sheikh', 4790))\n",
      "(109, ('way', 4789))\n",
      "(110, ('business', 4729))\n",
      "(111, ('call', 4727))\n",
      "(112, ('foreign', 4719))\n",
      "(113, ('want', 4716))\n",
      "(114, ('school', 4556))\n",
      "(115, ('back', 4538))\n",
      "(116, ('become', 4505))\n",
      "(117, ('rahman', 4478))\n",
      "(118, ('district', 4477))\n",
      "(119, ('meeting', 4428))\n",
      "(120, ('authority', 4399))\n",
      "(121, ('there', 4396))\n",
      "(122, ('long', 4376))\n",
      "(123, ('show', 4345))\n",
      "(124, ('photo', 4336))\n",
      "(125, ('war', 4316))\n",
      "(126, ('on', 4282))\n",
      "(127, ('order', 4282))\n",
      "(128, ('run', 4274))\n",
      "(129, ('follow', 4267))\n",
      "(130, ('worker', 4264))\n",
      "(131, ('programme', 4219))\n",
      "(132, ('among', 4218))\n",
      "(133, ('price', 4214))\n",
      "(134, ('another', 4193))\n",
      "(135, ('support', 4101))\n",
      "(136, ('share', 4097))\n",
      "(137, ('chief', 4081))\n",
      "(138, ('kill', 4070))\n",
      "(139, ('found', 4053))\n",
      "(140, ('system', 4046))\n",
      "(141, ('as', 4019))\n",
      "(142, ('hossain', 3993))\n",
      "(143, ('level', 3992))\n",
      "(144, ('play', 3983))\n",
      "(145, ('visit', 3960))\n",
      "(146, ('university', 3960))\n",
      "(147, ('ask', 3935))\n",
      "(148, ('face', 3910))\n",
      "(149, ('much', 3848))\n",
      "(150, ('ministry', 3846))\n",
      "(151, ('if', 3798))\n",
      "(152, ('capital', 3791))\n",
      "(153, ('try', 3788))\n",
      "(154, ('office', 3778))\n",
      "(155, ('must', 3773))\n",
      "(156, ('base', 3762))\n",
      "(157, ('year', 3755))\n",
      "(158, ('education', 3732))\n",
      "(159, ('force', 3730))\n",
      "(160, ('present', 3711))\n",
      "(161, ('act', 3705))\n",
      "(162, ('economic', 3702))\n",
      "(163, ('end', 3685))\n",
      "(164, ('plan', 3677))\n",
      "(165, ('death', 3676))\n",
      "(166, ('hasina', 3673))\n",
      "(167, ('medium', 3671))\n",
      "(168, ('held', 3670))\n",
      "(169, ('form', 3660))\n",
      "(170, ('bangladeshi', 3639))\n",
      "(171, ('human', 3592))\n",
      "(172, ('she', 3590))\n",
      "(173, ('policy', 3511))\n",
      "(174, ('already', 3510))\n",
      "(175, ('building', 3502))\n",
      "(176, ('still', 3496))\n",
      "(177, ('industry', 3489))\n",
      "(178, ('ensure', 3488))\n",
      "(179, ('hand', 3485))\n",
      "(180, ('nation', 3472))\n",
      "(181, ('large', 3453))\n",
      "(182, ('officer', 3439))\n",
      "(183, ('point', 3437))\n",
      "(184, ('test', 3415))\n",
      "(185, ('attack', 3414))\n",
      "(186, ('dr', 3385))\n",
      "(187, ('committee', 3376))\n",
      "(188, ('body', 3375))\n",
      "(189, ('social', 3365))\n",
      "(190, ('cost', 3363))\n",
      "(191, ('think', 3359))\n",
      "(192, ('money', 3357))\n",
      "(193, ('without', 3348))\n",
      "(194, ('hospital', 3341))\n",
      "(195, ('charge', 3336))\n",
      "(196, ('export', 3333))\n",
      "(197, ('per', 3328))\n",
      "(198, ('according', 3324))\n",
      "(199, ('name', 3319))\n",
      "(200, ('river', 3308))\n",
      "(201, ('total', 3296))\n",
      "(202, ('accord', 3283))\n",
      "(203, ('every', 3238))\n",
      "(204, ('create', 3225))\n",
      "(205, ('look', 3217))\n",
      "(206, ('food', 3211))\n",
      "(207, ('security', 3210))\n",
      "(208, ('growth', 3208))\n",
      "(209, ('problem', 3198))\n",
      "(210, ('rate', 3184))\n",
      "(211, ('situation', 3162))\n",
      "(212, ('match', 3147))\n",
      "(213, ('crore', 3145))\n",
      "(214, ('low', 3137))\n",
      "(215, ('begin', 3133))\n",
      "(216, ('house', 3127))\n",
      "(217, ('global', 3118))\n",
      "(218, ('arrest', 3103))\n",
      "(219, ('receive', 3058))\n",
      "(220, ('us', 3057))\n",
      "(221, ('private', 3052))\n",
      "(222, ('great', 3050))\n",
      "(223, ('product', 3037))\n",
      "(224, ('term', 2996))\n",
      "(225, ('within', 2994))\n",
      "(226, ('remain', 2988))\n",
      "(227, ('organisation', 2985))\n",
      "(228, ('big', 2967))\n",
      "(229, ('centre', 2964))\n",
      "(230, ('information', 2961))\n",
      "(231, ('move', 2949))\n",
      "(232, ('source', 2946))\n",
      "(233, ('upazila', 2921))\n",
      "(234, ('action', 2890))\n",
      "(235, ('across', 2890))\n",
      "(236, ('department', 2890))\n",
      "(237, ('station', 2884))\n",
      "(238, ('others', 2865))\n",
      "(239, ('cause', 2858))\n",
      "(240, ('former', 2856))\n",
      "(241, ('return', 2828))\n",
      "(242, ('press', 2824))\n",
      "(243, ('commission', 2815))\n",
      "(244, ('trade', 2811))\n",
      "(245, ('several', 2810))\n",
      "(246, ('fire', 2796))\n",
      "(247, ('study', 2775))\n",
      "(248, ('community', 2761))\n",
      "(249, ('process', 2758))\n",
      "(250, ('role', 2743))\n",
      "(251, ('reach', 2741))\n",
      "(252, ('open', 2716))\n",
      "(253, ('decision', 2712))\n",
      "(254, ('bangabandhu', 2705))\n",
      "(255, ('star', 2702))\n",
      "(256, ('pay', 2702))\n",
      "(257, ('activity', 2701))\n",
      "(258, ('after', 2696))\n",
      "(259, ('interest', 2691))\n",
      "(260, ('yet', 2685))\n",
      "(261, ('rise', 2674))\n",
      "(262, ('along', 2668))\n",
      "(263, ('file', 2662))\n",
      "(264, ('left', 2650))\n",
      "(265, ('continue', 2648))\n",
      "(266, ('later', 2644))\n",
      "(267, ('investment', 2632))\n",
      "(268, ('keep', 2622))\n",
      "(269, ('result', 2610))\n",
      "(270, ('film', 2610))\n",
      "(271, ('major', 2597))\n",
      "(272, ('chairman', 2586))\n",
      "(273, ('free', 2570))\n",
      "(274, ('lose', 2566))\n",
      "(275, ('so', 2554))\n",
      "(276, ('side', 2540))\n",
      "(277, ('job', 2527))\n",
      "(278, ('when', 2523))\n",
      "(279, ('gas', 2516))\n",
      "(280, ('college', 2496))\n",
      "(281, ('top', 2490))\n",
      "(282, ('question', 2476))\n",
      "(283, ('bangla', 2473))\n",
      "(284, ('stop', 2459))\n",
      "(285, ('indian', 2457))\n",
      "(286, ('post', 2455))\n",
      "(287, ('carry', 2449))\n",
      "(288, ('young', 2447))\n",
      "(289, ('important', 2440))\n",
      "(290, ('society', 2439))\n",
      "(291, ('rule', 2426))\n",
      "(292, ('do', 2419))\n",
      "(293, ('put', 2417))\n",
      "(294, ('and', 2415))\n",
      "(295, ('last', 2412))\n",
      "(296, ('at', 2395))\n",
      "(297, ('plant', 2394))\n",
      "(298, ('talk', 2393))\n",
      "(299, ('step', 2388))\n",
      "(300, ('future', 2379))\n",
      "(301, ('army', 2379))\n",
      "(302, ('write', 2372))\n",
      "(303, ('myanmar', 2370))\n",
      "(304, ('release', 2369))\n",
      "(305, ('far', 2365))\n",
      "(306, ('find', 2363))\n",
      "(307, ('head', 2358))\n",
      "(308, ('production', 2355))\n",
      "(309, ('incident', 2351))\n",
      "(310, ('statement', 2349))\n",
      "(311, ('best', 2344))\n",
      "(312, ('involve', 2334))\n",
      "(313, ('import', 2333))\n",
      "(314, ('unit', 2329))\n",
      "(315, ('technology', 2325))\n",
      "(316, ('event', 2324))\n",
      "(317, ('factory', 2318))\n",
      "(318, ('bring', 2317))\n",
      "(319, ('fund', 2312))\n",
      "(320, ('book', 2311))\n",
      "(321, ('special', 2302))\n",
      "(322, ('able', 2298))\n",
      "(323, ('person', 2298))\n",
      "(324, ('men', 2284))\n",
      "(325, ('chittagong', 2284))\n",
      "(326, ('division', 2282))\n",
      "(327, ('matter', 2270))\n",
      "(328, ('economy', 2269))\n",
      "(329, ('hold', 2265))\n",
      "(330, ('cricket', 2261))\n",
      "(331, ('justice', 2260))\n",
      "(332, ('allow', 2236))\n",
      "(333, ('bus', 2223))\n",
      "(334, ('various', 2214))\n",
      "(335, ('agency', 2212))\n",
      "(336, ('village', 2211))\n",
      "(337, ('though', 2210))\n",
      "(338, ('opportunity', 2204))\n",
      "(339, ('teacher', 2202))\n",
      "(340, ('supply', 2198))\n",
      "(341, ('border', 2195))\n",
      "(342, ('poor', 2190))\n",
      "(343, ('regard', 2190))\n",
      "(344, ('condition', 2186))\n",
      "(345, ('produce', 2186))\n",
      "(346, ('consider', 2182))\n",
      "(347, ('father', 2172))\n",
      "(348, ('senior', 2168))\n",
      "(349, ('launch', 2165))\n",
      "(350, ('league', 2163))\n",
      "(351, ('sell', 2155))\n",
      "(352, ('line', 2151))\n",
      "(353, ('traffic', 2143))\n",
      "(354, ('movement', 2139))\n",
      "(355, ('non', 2135))\n",
      "(356, ('expect', 2128))\n",
      "(357, ('crisis', 2127))\n",
      "(358, ('stand', 2124))\n",
      "(359, ('turn', 2113))\n",
      "(360, ('khan', 2113))\n",
      "(361, ('population', 2112))\n",
      "(362, ('research', 2112))\n",
      "(363, ('rohingya', 2110))\n",
      "(364, ('crime', 2109))\n",
      "(365, ('never', 2104))\n",
      "(366, ('central', 2099))\n",
      "(367, ('region', 2097))\n",
      "(368, ('believe', 2095))\n",
      "(369, ('lack', 2094))\n",
      "(370, ('initiative', 2090))\n",
      "(371, ('control', 2089))\n",
      "(372, ('award', 2085))\n",
      "(373, ('coronavirus', 2084))\n",
      "(374, ('tax', 2083))\n",
      "(375, ('financial', 2075))\n",
      "(376, ('chowdhury', 2074))\n",
      "(377, ('raise', 2074))\n",
      "(378, ('management', 2070))\n",
      "(379, ('quality', 2067))\n",
      "(380, ('effort', 2067))\n",
      "(381, ('conduct', 2065))\n",
      "(382, ('thing', 2057))\n",
      "(383, ('for', 2052))\n",
      "(384, ('always', 2051))\n",
      "(385, ('recently', 2045))\n",
      "(386, ('join', 2043))\n",
      "(387, ('grow', 2041))\n",
      "(388, ('conference', 2034))\n",
      "(389, ('medical', 2033))\n",
      "(390, ('address', 2033))\n",
      "(391, ('field', 2032))\n",
      "(392, ('complete', 2029))\n",
      "(393, ('current', 2026))\n",
      "(394, ('commissioner', 2016))\n",
      "(395, ('list', 2015))\n",
      "(396, ('win', 2012))\n",
      "(397, ('soon', 2012))\n",
      "(398, ('away', 2011))\n",
      "(399, ('bridge', 2009))\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(Counter(counts_bd2).most_common()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'your', '‘d', 'more', 'was', 'my', 'either', 'seemed', 'can', 'really', 'she', 'above', 'than', 're', 'myself', 'noone', 'behind', \"'m\", 'how', 'we', 'six', 'go', 'full', 'another', 'do', 'whereas', 'at', 'except', 'mine', \"'d\", 'still', 'her', 'mostly', 'to', 'along', 'or', 'well', 'had', 'meanwhile', 'it', 'moreover', 'four', 'under', 'forty', 'afterwards', 'thus', '’ve', 'per', 'ours', 'nothing', 'hers', 'across', 'although', 'thereafter', 'never', 'me', 'most', 'fifty', 'formerly', '’s', 'if', 'an', 'up', 'n’t', 'in', 'that', 'otherwise', 'nevertheless', 'became', 'over', 'thru', '‘ve', 'each', 'nine', 'sixty', 'he', 'almost', 'someone', \"'s\", 'former', 'himself', 'without', 'already', 'elsewhere', 'am', 'latterly', 'done', 'various', 'whereafter', 'within', 'before', 'herein', 'name', 'namely', 'beside', 'become', 'those', 'other', 'any', 'put', 'does', 'so', 'they', 'wherein', 'whereupon', 'yet', 'ten', \"n't\", 'eleven', 'their', 'much', 'besides', 'there', 'i', 'ca', 'will', 'which', 'made', 'few', 'when', 'who', 'again', \"'re\", 'least', 'three', 'everyone', 'thereby', 'nowhere', 'therefore', 'anyway', '‘s', 'after', 'has', 'therein', 'cannot', 'thereupon', 'third', 'five', 'get', 'all', 'throughout', 'during', 'with', 'whole', '‘ll', 'among', 'may', 'regarding', 'should', 'together', 'amount', 'both', 'its', 'between', 'also', 'these', 'whereby', 'perhaps', 'be', 'are', 'themselves', 'fifteen', 'keep', 'now', 'might', 'anyhow', 'doing', 'down', 'hence', \"'ve\", 'back', 'whatever', 'below', 'something', 'empty', 'only', 'else', 'hereby', 'part', 'because', 'you', 'anything', 'see', 'none', 'the', 'n‘t', 'twenty', 'anywhere', 'enough', 'wherever', 'seeming', 'out', 'onto', 'please', 'once', 'somewhere', 'whose', 'must', \"'ll\", 'last', 'ourselves', 'toward', 'his', 'somehow', 'no', 'amongst', 'first', 'herself', 'just', 'were', 'rather', 'show', 'twelve', 'quite', '’d', 'through', 'very', 'further', 'two', 'several', 'as', 'own', 'everywhere', 'is', 'our', 'move', 'since', 'whenever', 'often', 'anyone', 'this', 'towards', 'sometime', 'indeed', 'from', 'beyond', 'us', 'whence', 'while', 'via', 'here', 'a', '‘re', 'around', 'such', 'thence', '’m', 'latter', 'could', 'into', 'not', 'nobody', 'yourself', 'have', 'did', 'neither', 'however', 'side', 'on', 'becoming', 'bottom', 'nor', 'why', 'same', 'ever', 'what', 'less', 'always', 'next', 'and', 'due', 'being', 'then', 'top', 'though', 'whoever', 'whether', 'itself', 'beforehand', 'used', 'whither', 'hereafter', 'upon', 'would', 'but', 'many', 'until', 'against', 'alone', 'others', 'call', 'about', 'of', '’re', 'every', 'seems', 'off', '‘m', 'hereupon', 'by', 'whom', 'yours', 'where', 'one', 'becomes', 'say', 'for', 'him', 'been', 'sometimes', 'everything', 'front', 'make', 'too', 'seem', 'even', 'take', 'some', 'give', '’ll', 'them', 'using', 'eight', 'yourselves', 'serious', 'unless', 'hundred'}\n"
     ]
    }
   ],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts_bd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stopwords:\n",
    "    if i in counts_bd2:\n",
    "        del counts_bd2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts_bd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ('bangladesh', 46684))\n",
      "(1, ('country', 22510))\n",
      "(2, ('government', 20966))\n",
      "(3, ('people', 19966))\n",
      "(4, ('minister', 12601))\n",
      "(5, ('time', 12261))\n",
      "(6, ('work', 11527))\n",
      "(7, ('police', 10164))\n",
      "(8, ('come', 9810))\n",
      "(9, ('high', 9434))\n",
      "(10, ('use', 8819))\n",
      "(11, ('new', 8614))\n",
      "(12, ('include', 8222))\n",
      "(13, ('case', 8117))\n",
      "(14, ('add', 8097))\n",
      "(15, ('area', 7970))\n",
      "(16, ('world', 7732))\n",
      "(17, ('need', 7700))\n",
      "(18, ('bank', 7421))\n",
      "(19, ('official', 7324))\n",
      "(20, ('report', 7164))\n",
      "(21, ('day', 7131))\n",
      "(22, ('like', 7089))\n",
      "(23, ('student', 7014))\n",
      "(24, ('party', 6932))\n",
      "(25, ('state', 6917))\n",
      "(26, ('law', 6701))\n",
      "(27, ('project', 6697))\n",
      "(28, ('member', 6590))\n",
      "(29, ('development', 6557))\n",
      "(30, ('told', 6551))\n",
      "(31, ('international', 6515))\n",
      "(32, ('leader', 6462))\n",
      "(33, ('issue', 6457))\n",
      "(34, ('national', 6452))\n",
      "(35, ('number', 6256))\n",
      "(36, ('city', 6221))\n",
      "(37, ('general', 5950))\n",
      "(38, ('tk', 5947))\n",
      "(39, ('power', 5937))\n",
      "(40, ('woman', 5865))\n",
      "(41, ('local', 5832))\n",
      "(42, ('start', 5831))\n",
      "(43, ('court', 5822))\n",
      "(44, ('prime', 5749))\n",
      "(45, ('market', 5736))\n",
      "(46, ('child', 5697))\n",
      "(47, ('life', 5680))\n",
      "(48, ('company', 5640))\n",
      "(49, ('sector', 5576))\n",
      "(50, ('right', 5575))\n",
      "(51, ('service', 5541))\n",
      "(52, ('water', 5507))\n",
      "(53, ('good', 5485))\n",
      "(54, ('place', 5472))\n",
      "(55, ('bnp', 5439))\n",
      "(56, ('political', 5308))\n",
      "(57, ('increase', 5303))\n",
      "(58, ('election', 5273))\n",
      "(59, ('president', 5125))\n",
      "(60, ('land', 5105))\n",
      "(61, ('director', 5096))\n",
      "(62, ('know', 5079))\n",
      "(63, ('home', 5075))\n",
      "(64, ('public', 5063))\n",
      "(65, ('different', 5053))\n",
      "(66, ('group', 5040))\n",
      "(67, ('set', 5035))\n",
      "(68, ('help', 5007))\n",
      "(69, ('lead', 4998))\n",
      "(70, ('family', 4970))\n",
      "(71, ('road', 4951))\n",
      "(72, ('provide', 4943))\n",
      "(73, ('secretary', 4934))\n",
      "(74, ('change', 4895))\n",
      "(75, ('team', 4866))\n",
      "(76, ('health', 4846))\n",
      "(77, ('demand', 4832))\n",
      "(78, ('sheikh', 4790))\n",
      "(79, ('way', 4789))\n",
      "(80, ('business', 4729))\n",
      "(81, ('foreign', 4719))\n",
      "(82, ('want', 4716))\n",
      "(83, ('school', 4556))\n",
      "(84, ('rahman', 4478))\n",
      "(85, ('district', 4477))\n",
      "(86, ('meeting', 4428))\n",
      "(87, ('authority', 4399))\n",
      "(88, ('long', 4376))\n",
      "(89, ('photo', 4336))\n",
      "(90, ('war', 4316))\n",
      "(91, ('order', 4282))\n",
      "(92, ('run', 4274))\n",
      "(93, ('follow', 4267))\n",
      "(94, ('worker', 4264))\n",
      "(95, ('programme', 4219))\n",
      "(96, ('price', 4214))\n",
      "(97, ('support', 4101))\n",
      "(98, ('share', 4097))\n",
      "(99, ('chief', 4081))\n",
      "(100, ('kill', 4070))\n",
      "(101, ('found', 4053))\n",
      "(102, ('system', 4046))\n",
      "(103, ('hossain', 3993))\n",
      "(104, ('level', 3992))\n",
      "(105, ('play', 3983))\n",
      "(106, ('visit', 3960))\n",
      "(107, ('university', 3960))\n",
      "(108, ('ask', 3935))\n",
      "(109, ('face', 3910))\n",
      "(110, ('ministry', 3846))\n",
      "(111, ('capital', 3791))\n",
      "(112, ('try', 3788))\n",
      "(113, ('office', 3778))\n",
      "(114, ('base', 3762))\n",
      "(115, ('year', 3755))\n",
      "(116, ('education', 3732))\n",
      "(117, ('force', 3730))\n",
      "(118, ('present', 3711))\n",
      "(119, ('act', 3705))\n",
      "(120, ('economic', 3702))\n",
      "(121, ('end', 3685))\n",
      "(122, ('plan', 3677))\n",
      "(123, ('death', 3676))\n",
      "(124, ('hasina', 3673))\n",
      "(125, ('medium', 3671))\n",
      "(126, ('held', 3670))\n",
      "(127, ('form', 3660))\n",
      "(128, ('bangladeshi', 3639))\n",
      "(129, ('human', 3592))\n",
      "(130, ('policy', 3511))\n",
      "(131, ('building', 3502))\n",
      "(132, ('industry', 3489))\n",
      "(133, ('ensure', 3488))\n",
      "(134, ('hand', 3485))\n",
      "(135, ('nation', 3472))\n",
      "(136, ('large', 3453))\n",
      "(137, ('officer', 3439))\n",
      "(138, ('point', 3437))\n",
      "(139, ('test', 3415))\n",
      "(140, ('attack', 3414))\n",
      "(141, ('dr', 3385))\n",
      "(142, ('committee', 3376))\n",
      "(143, ('body', 3375))\n",
      "(144, ('social', 3365))\n",
      "(145, ('cost', 3363))\n",
      "(146, ('think', 3359))\n",
      "(147, ('money', 3357))\n",
      "(148, ('hospital', 3341))\n",
      "(149, ('charge', 3336))\n",
      "(150, ('export', 3333))\n",
      "(151, ('according', 3324))\n",
      "(152, ('river', 3308))\n",
      "(153, ('total', 3296))\n",
      "(154, ('accord', 3283))\n",
      "(155, ('create', 3225))\n",
      "(156, ('look', 3217))\n",
      "(157, ('food', 3211))\n",
      "(158, ('security', 3210))\n",
      "(159, ('growth', 3208))\n",
      "(160, ('problem', 3198))\n",
      "(161, ('rate', 3184))\n",
      "(162, ('situation', 3162))\n",
      "(163, ('match', 3147))\n",
      "(164, ('crore', 3145))\n",
      "(165, ('low', 3137))\n",
      "(166, ('begin', 3133))\n",
      "(167, ('house', 3127))\n",
      "(168, ('global', 3118))\n",
      "(169, ('arrest', 3103))\n",
      "(170, ('receive', 3058))\n",
      "(171, ('private', 3052))\n",
      "(172, ('great', 3050))\n",
      "(173, ('product', 3037))\n",
      "(174, ('term', 2996))\n",
      "(175, ('remain', 2988))\n",
      "(176, ('organisation', 2985))\n",
      "(177, ('big', 2967))\n",
      "(178, ('centre', 2964))\n",
      "(179, ('information', 2961))\n",
      "(180, ('source', 2946))\n",
      "(181, ('upazila', 2921))\n",
      "(182, ('action', 2890))\n",
      "(183, ('department', 2890))\n",
      "(184, ('station', 2884))\n",
      "(185, ('cause', 2858))\n",
      "(186, ('return', 2828))\n",
      "(187, ('press', 2824))\n",
      "(188, ('commission', 2815))\n",
      "(189, ('trade', 2811))\n",
      "(190, ('fire', 2796))\n",
      "(191, ('study', 2775))\n",
      "(192, ('community', 2761))\n",
      "(193, ('process', 2758))\n",
      "(194, ('role', 2743))\n",
      "(195, ('reach', 2741))\n",
      "(196, ('open', 2716))\n",
      "(197, ('decision', 2712))\n",
      "(198, ('bangabandhu', 2705))\n",
      "(199, ('star', 2702))\n",
      "(200, ('pay', 2702))\n",
      "(201, ('activity', 2701))\n",
      "(202, ('interest', 2691))\n",
      "(203, ('rise', 2674))\n",
      "(204, ('file', 2662))\n",
      "(205, ('left', 2650))\n",
      "(206, ('continue', 2648))\n",
      "(207, ('later', 2644))\n",
      "(208, ('investment', 2632))\n",
      "(209, ('result', 2610))\n",
      "(210, ('film', 2610))\n",
      "(211, ('major', 2597))\n",
      "(212, ('chairman', 2586))\n",
      "(213, ('free', 2570))\n",
      "(214, ('lose', 2566))\n",
      "(215, ('job', 2527))\n",
      "(216, ('gas', 2516))\n",
      "(217, ('college', 2496))\n",
      "(218, ('question', 2476))\n",
      "(219, ('bangla', 2473))\n",
      "(220, ('stop', 2459))\n",
      "(221, ('indian', 2457))\n",
      "(222, ('post', 2455))\n",
      "(223, ('carry', 2449))\n",
      "(224, ('young', 2447))\n",
      "(225, ('important', 2440))\n",
      "(226, ('society', 2439))\n",
      "(227, ('rule', 2426))\n",
      "(228, ('plant', 2394))\n",
      "(229, ('talk', 2393))\n",
      "(230, ('step', 2388))\n",
      "(231, ('future', 2379))\n",
      "(232, ('army', 2379))\n",
      "(233, ('write', 2372))\n",
      "(234, ('myanmar', 2370))\n",
      "(235, ('release', 2369))\n",
      "(236, ('far', 2365))\n",
      "(237, ('find', 2363))\n",
      "(238, ('head', 2358))\n",
      "(239, ('production', 2355))\n",
      "(240, ('incident', 2351))\n",
      "(241, ('statement', 2349))\n",
      "(242, ('best', 2344))\n",
      "(243, ('involve', 2334))\n",
      "(244, ('import', 2333))\n",
      "(245, ('unit', 2329))\n",
      "(246, ('technology', 2325))\n",
      "(247, ('event', 2324))\n",
      "(248, ('factory', 2318))\n",
      "(249, ('bring', 2317))\n",
      "(250, ('fund', 2312))\n",
      "(251, ('book', 2311))\n",
      "(252, ('special', 2302))\n",
      "(253, ('able', 2298))\n",
      "(254, ('person', 2298))\n",
      "(255, ('men', 2284))\n",
      "(256, ('chittagong', 2284))\n",
      "(257, ('division', 2282))\n",
      "(258, ('matter', 2270))\n",
      "(259, ('economy', 2269))\n",
      "(260, ('hold', 2265))\n",
      "(261, ('cricket', 2261))\n",
      "(262, ('justice', 2260))\n",
      "(263, ('allow', 2236))\n",
      "(264, ('bus', 2223))\n",
      "(265, ('agency', 2212))\n",
      "(266, ('village', 2211))\n",
      "(267, ('opportunity', 2204))\n",
      "(268, ('teacher', 2202))\n",
      "(269, ('supply', 2198))\n",
      "(270, ('border', 2195))\n",
      "(271, ('poor', 2190))\n",
      "(272, ('regard', 2190))\n",
      "(273, ('condition', 2186))\n",
      "(274, ('produce', 2186))\n",
      "(275, ('consider', 2182))\n",
      "(276, ('father', 2172))\n",
      "(277, ('senior', 2168))\n",
      "(278, ('launch', 2165))\n",
      "(279, ('league', 2163))\n",
      "(280, ('sell', 2155))\n",
      "(281, ('line', 2151))\n",
      "(282, ('traffic', 2143))\n",
      "(283, ('movement', 2139))\n",
      "(284, ('non', 2135))\n",
      "(285, ('expect', 2128))\n",
      "(286, ('crisis', 2127))\n",
      "(287, ('stand', 2124))\n",
      "(288, ('turn', 2113))\n",
      "(289, ('khan', 2113))\n",
      "(290, ('population', 2112))\n",
      "(291, ('research', 2112))\n",
      "(292, ('rohingya', 2110))\n",
      "(293, ('crime', 2109))\n",
      "(294, ('central', 2099))\n",
      "(295, ('region', 2097))\n",
      "(296, ('believe', 2095))\n",
      "(297, ('lack', 2094))\n",
      "(298, ('initiative', 2090))\n",
      "(299, ('control', 2089))\n",
      "(300, ('award', 2085))\n",
      "(301, ('coronavirus', 2084))\n",
      "(302, ('tax', 2083))\n",
      "(303, ('financial', 2075))\n",
      "(304, ('chowdhury', 2074))\n",
      "(305, ('raise', 2074))\n",
      "(306, ('management', 2070))\n",
      "(307, ('quality', 2067))\n",
      "(308, ('effort', 2067))\n",
      "(309, ('conduct', 2065))\n",
      "(310, ('thing', 2057))\n",
      "(311, ('recently', 2045))\n",
      "(312, ('join', 2043))\n",
      "(313, ('grow', 2041))\n",
      "(314, ('conference', 2034))\n",
      "(315, ('medical', 2033))\n",
      "(316, ('address', 2033))\n",
      "(317, ('field', 2032))\n",
      "(318, ('complete', 2029))\n",
      "(319, ('current', 2026))\n",
      "(320, ('commissioner', 2016))\n",
      "(321, ('list', 2015))\n",
      "(322, ('win', 2012))\n",
      "(323, ('soon', 2012))\n",
      "(324, ('away', 2011))\n",
      "(325, ('bridge', 2009))\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(Counter(counts_bd2).most_common()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dict = ['tk', 'bnp', 'bangabandhu', 'bangladesh', \\\n",
    "               'rahman', 'hossain', 'hasina', 'upazila', \\\n",
    "               'bangabandhu', 'bangla', 'indian', 'myanmar', \\\n",
    "               'chittagong', 'khan', 'rohingya', 'chowdhury', \\\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in remove_dict:\n",
    "    if i in counts_bd2:\n",
    "        del counts_bd2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts_bd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
