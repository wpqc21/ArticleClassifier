{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print(glob.glob(\"/Physics Project GITHUB/\"))\n",
    "\n",
    "import glob\n",
    "# All files ending with .txt\n",
    "print(glob.glob(\"/Physics Project GITHUB/*.txt\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "txtfiles_all = []\n",
    "for file in glob.glob('**/10*'):\n",
    "    txtfiles_all.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_10-01\\\\10-01-au.txt',\n",
       " 'text_10-01\\\\10-01-bd.txt',\n",
       " 'text_10-01\\\\10-01-ca.txt',\n",
       " 'text_10-01\\\\10-01-gb.txt',\n",
       " 'text_10-01\\\\10-01-gh.txt',\n",
       " 'text_10-01\\\\10-01-hk.txt',\n",
       " 'text_10-01\\\\10-01-ie.txt',\n",
       " 'text_10-01\\\\10-01-in.txt',\n",
       " 'text_10-01\\\\10-01-jm.txt',\n",
       " 'text_10-01\\\\10-01-ke.txt',\n",
       " 'text_10-01\\\\10-01-lk.txt',\n",
       " 'text_10-01\\\\10-01-my.txt',\n",
       " 'text_10-01\\\\10-01-ng.txt',\n",
       " 'text_10-01\\\\10-01-nz.txt',\n",
       " 'text_10-01\\\\10-01-ph.txt',\n",
       " 'text_10-01\\\\10-01-pk.txt',\n",
       " 'text_10-01\\\\10-01-sg.txt',\n",
       " 'text_10-01\\\\10-01-tz.txt',\n",
       " 'text_10-01\\\\10-01-us.txt',\n",
       " 'text_10-01\\\\10-01-za.txt',\n",
       " 'text_10-02\\\\10-02-au.txt',\n",
       " 'text_10-02\\\\10-02-bd.txt',\n",
       " 'text_10-02\\\\10-02-ca.txt',\n",
       " 'text_10-02\\\\10-02-gb.txt',\n",
       " 'text_10-02\\\\10-02-gh.txt',\n",
       " 'text_10-02\\\\10-02-hk.txt',\n",
       " 'text_10-02\\\\10-02-ie.txt',\n",
       " 'text_10-02\\\\10-02-in.txt',\n",
       " 'text_10-02\\\\10-02-jm.txt',\n",
       " 'text_10-02\\\\10-02-ke.txt',\n",
       " 'text_10-02\\\\10-02-lk.txt',\n",
       " 'text_10-02\\\\10-02-my.txt',\n",
       " 'text_10-02\\\\10-02-ng.txt',\n",
       " 'text_10-02\\\\10-02-nz.txt',\n",
       " 'text_10-02\\\\10-02-ph.txt',\n",
       " 'text_10-02\\\\10-02-pk.txt',\n",
       " 'text_10-02\\\\10-02-sg.txt',\n",
       " 'text_10-02\\\\10-02-tz.txt',\n",
       " 'text_10-02\\\\10-02-us.txt',\n",
       " 'text_10-02\\\\10-02-za.txt',\n",
       " 'text_10-03\\\\10-03-au.txt',\n",
       " 'text_10-03\\\\10-03-bd.txt',\n",
       " 'text_10-03\\\\10-03-ca.txt',\n",
       " 'text_10-03\\\\10-03-gb.txt',\n",
       " 'text_10-03\\\\10-03-gh.txt',\n",
       " 'text_10-03\\\\10-03-hk.txt',\n",
       " 'text_10-03\\\\10-03-ie.txt',\n",
       " 'text_10-03\\\\10-03-in.txt',\n",
       " 'text_10-03\\\\10-03-jm.txt',\n",
       " 'text_10-03\\\\10-03-ke.txt',\n",
       " 'text_10-03\\\\10-03-lk.txt',\n",
       " 'text_10-03\\\\10-03-my.txt',\n",
       " 'text_10-03\\\\10-03-ng.txt',\n",
       " 'text_10-03\\\\10-03-nz.txt',\n",
       " 'text_10-03\\\\10-03-ph.txt',\n",
       " 'text_10-03\\\\10-03-pk.txt',\n",
       " 'text_10-03\\\\10-03-sg.txt',\n",
       " 'text_10-03\\\\10-03-tz.txt',\n",
       " 'text_10-03\\\\10-03-us.txt',\n",
       " 'text_10-03\\\\10-03-za.txt',\n",
       " 'text_10-04\\\\10-04-au.txt',\n",
       " 'text_10-04\\\\10-04-bd.txt',\n",
       " 'text_10-04\\\\10-04-ca.txt',\n",
       " 'text_10-04\\\\10-04-gb.txt',\n",
       " 'text_10-04\\\\10-04-gh.txt',\n",
       " 'text_10-04\\\\10-04-hk.txt',\n",
       " 'text_10-04\\\\10-04-ie.txt',\n",
       " 'text_10-04\\\\10-04-in.txt',\n",
       " 'text_10-04\\\\10-04-jm.txt',\n",
       " 'text_10-04\\\\10-04-ke.txt',\n",
       " 'text_10-04\\\\10-04-lk.txt',\n",
       " 'text_10-04\\\\10-04-my.txt',\n",
       " 'text_10-04\\\\10-04-ng.txt',\n",
       " 'text_10-04\\\\10-04-nz.txt',\n",
       " 'text_10-04\\\\10-04-ph.txt',\n",
       " 'text_10-04\\\\10-04-pk.txt',\n",
       " 'text_10-04\\\\10-04-sg.txt',\n",
       " 'text_10-04\\\\10-04-us.txt',\n",
       " 'text_10-04\\\\10-04-za.txt',\n",
       " 'text_10-05\\\\10-05-au.txt',\n",
       " 'text_10-05\\\\10-05-bd.txt',\n",
       " 'text_10-05\\\\10-05-ca.txt',\n",
       " 'text_10-05\\\\10-05-gb.txt',\n",
       " 'text_10-05\\\\10-05-gh.txt',\n",
       " 'text_10-05\\\\10-05-hk.txt',\n",
       " 'text_10-05\\\\10-05-ie.txt',\n",
       " 'text_10-05\\\\10-05-in.txt',\n",
       " 'text_10-05\\\\10-05-jm.txt',\n",
       " 'text_10-05\\\\10-05-ke.txt',\n",
       " 'text_10-05\\\\10-05-lk.txt',\n",
       " 'text_10-05\\\\10-05-my.txt',\n",
       " 'text_10-05\\\\10-05-ng.txt',\n",
       " 'text_10-05\\\\10-05-nz.txt',\n",
       " 'text_10-05\\\\10-05-ph.txt',\n",
       " 'text_10-05\\\\10-05-pk.txt',\n",
       " 'text_10-05\\\\10-05-sg.txt',\n",
       " 'text_10-05\\\\10-05-tz.txt',\n",
       " 'text_10-05\\\\10-05-us.txt',\n",
       " 'text_10-05\\\\10-05-za.txt',\n",
       " 'text_10-06\\\\10-06-au.txt',\n",
       " 'text_10-06\\\\10-06-bd.txt',\n",
       " 'text_10-06\\\\10-06-ca.txt',\n",
       " 'text_10-06\\\\10-06-gb.txt',\n",
       " 'text_10-06\\\\10-06-gh.txt',\n",
       " 'text_10-06\\\\10-06-hk.txt',\n",
       " 'text_10-06\\\\10-06-ie.txt',\n",
       " 'text_10-06\\\\10-06-in.txt',\n",
       " 'text_10-06\\\\10-06-jm.txt',\n",
       " 'text_10-06\\\\10-06-ke.txt',\n",
       " 'text_10-06\\\\10-06-lk.txt',\n",
       " 'text_10-06\\\\10-06-my.txt',\n",
       " 'text_10-06\\\\10-06-ng.txt',\n",
       " 'text_10-06\\\\10-06-nz.txt',\n",
       " 'text_10-06\\\\10-06-ph.txt',\n",
       " 'text_10-06\\\\10-06-pk.txt',\n",
       " 'text_10-06\\\\10-06-sg.txt',\n",
       " 'text_10-06\\\\10-06-tz.txt',\n",
       " 'text_10-06\\\\10-06-us.txt',\n",
       " 'text_10-06\\\\10-06-za.txt',\n",
       " 'text_10-07\\\\10-07-au.txt',\n",
       " 'text_10-07\\\\10-07-bd.txt',\n",
       " 'text_10-07\\\\10-07-ca.txt',\n",
       " 'text_10-07\\\\10-07-gb.txt',\n",
       " 'text_10-07\\\\10-07-gh.txt',\n",
       " 'text_10-07\\\\10-07-hk.txt',\n",
       " 'text_10-07\\\\10-07-ie.txt',\n",
       " 'text_10-07\\\\10-07-in.txt',\n",
       " 'text_10-07\\\\10-07-jm.txt',\n",
       " 'text_10-07\\\\10-07-ke.txt',\n",
       " 'text_10-07\\\\10-07-lk.txt',\n",
       " 'text_10-07\\\\10-07-my.txt',\n",
       " 'text_10-07\\\\10-07-ng.txt',\n",
       " 'text_10-07\\\\10-07-nz.txt',\n",
       " 'text_10-07\\\\10-07-ph.txt',\n",
       " 'text_10-07\\\\10-07-pk.txt',\n",
       " 'text_10-07\\\\10-07-sg.txt',\n",
       " 'text_10-07\\\\10-07-us.txt',\n",
       " 'text_10-07\\\\10-07-za.txt',\n",
       " 'text_10-08\\\\10-08-au.txt',\n",
       " 'text_10-08\\\\10-08-bd.txt',\n",
       " 'text_10-08\\\\10-08-ca.txt',\n",
       " 'text_10-08\\\\10-08-gb.txt',\n",
       " 'text_10-08\\\\10-08-gh.txt',\n",
       " 'text_10-08\\\\10-08-hk.txt',\n",
       " 'text_10-08\\\\10-08-ie.txt',\n",
       " 'text_10-08\\\\10-08-in.txt',\n",
       " 'text_10-08\\\\10-08-jm.txt',\n",
       " 'text_10-08\\\\10-08-ke.txt',\n",
       " 'text_10-08\\\\10-08-lk.txt',\n",
       " 'text_10-08\\\\10-08-my.txt',\n",
       " 'text_10-08\\\\10-08-ng.txt',\n",
       " 'text_10-08\\\\10-08-nz.txt',\n",
       " 'text_10-08\\\\10-08-ph.txt',\n",
       " 'text_10-08\\\\10-08-pk.txt',\n",
       " 'text_10-08\\\\10-08-sg.txt',\n",
       " 'text_10-08\\\\10-08-tz.txt',\n",
       " 'text_10-08\\\\10-08-us.txt',\n",
       " 'text_10-08\\\\10-08-za.txt',\n",
       " 'text_10-09\\\\10-09-au.txt',\n",
       " 'text_10-09\\\\10-09-bd.txt',\n",
       " 'text_10-09\\\\10-09-ca.txt',\n",
       " 'text_10-09\\\\10-09-gb.txt',\n",
       " 'text_10-09\\\\10-09-gh.txt',\n",
       " 'text_10-09\\\\10-09-hk.txt',\n",
       " 'text_10-09\\\\10-09-ie.txt',\n",
       " 'text_10-09\\\\10-09-in.txt',\n",
       " 'text_10-09\\\\10-09-jm.txt',\n",
       " 'text_10-09\\\\10-09-ke.txt',\n",
       " 'text_10-09\\\\10-09-lk.txt',\n",
       " 'text_10-09\\\\10-09-my.txt',\n",
       " 'text_10-09\\\\10-09-ng.txt',\n",
       " 'text_10-09\\\\10-09-nz.txt',\n",
       " 'text_10-09\\\\10-09-ph.txt',\n",
       " 'text_10-09\\\\10-09-pk.txt',\n",
       " 'text_10-09\\\\10-09-sg.txt',\n",
       " 'text_10-09\\\\10-09-us.txt',\n",
       " 'text_10-09\\\\10-09-za.txt',\n",
       " 'text_10-10\\\\10-10-au.txt',\n",
       " 'text_10-10\\\\10-10-bd.txt',\n",
       " 'text_10-10\\\\10-10-ca.txt',\n",
       " 'text_10-10\\\\10-10-gb.txt',\n",
       " 'text_10-10\\\\10-10-gh.txt',\n",
       " 'text_10-10\\\\10-10-hk.txt',\n",
       " 'text_10-10\\\\10-10-ie.txt',\n",
       " 'text_10-10\\\\10-10-in.txt',\n",
       " 'text_10-10\\\\10-10-jm.txt',\n",
       " 'text_10-10\\\\10-10-ke.txt',\n",
       " 'text_10-10\\\\10-10-lk.txt',\n",
       " 'text_10-10\\\\10-10-my.txt',\n",
       " 'text_10-10\\\\10-10-ng.txt',\n",
       " 'text_10-10\\\\10-10-nz.txt',\n",
       " 'text_10-10\\\\10-10-ph.txt',\n",
       " 'text_10-10\\\\10-10-pk.txt',\n",
       " 'text_10-10\\\\10-10-sg.txt',\n",
       " 'text_10-10\\\\10-10-tz.txt',\n",
       " 'text_10-10\\\\10-10-us.txt',\n",
       " 'text_10-10\\\\10-10-za.txt',\n",
       " 'text_10-11\\\\10-11-au.txt',\n",
       " 'text_10-11\\\\10-11-bd.txt',\n",
       " 'text_10-11\\\\10-11-ca.txt',\n",
       " 'text_10-11\\\\10-11-gb.txt',\n",
       " 'text_10-11\\\\10-11-gh.txt',\n",
       " 'text_10-11\\\\10-11-hk.txt',\n",
       " 'text_10-11\\\\10-11-ie.txt',\n",
       " 'text_10-11\\\\10-11-in.txt',\n",
       " 'text_10-11\\\\10-11-jm.txt',\n",
       " 'text_10-11\\\\10-11-ke.txt',\n",
       " 'text_10-11\\\\10-11-lk.txt',\n",
       " 'text_10-11\\\\10-11-my.txt',\n",
       " 'text_10-11\\\\10-11-ng.txt',\n",
       " 'text_10-11\\\\10-11-nz.txt',\n",
       " 'text_10-11\\\\10-11-ph.txt',\n",
       " 'text_10-11\\\\10-11-pk.txt',\n",
       " 'text_10-11\\\\10-11-sg.txt',\n",
       " 'text_10-11\\\\10-11-tz.txt',\n",
       " 'text_10-11\\\\10-11-us.txt',\n",
       " 'text_10-11\\\\10-11-za.txt',\n",
       " 'text_10-12\\\\10-12-au.txt',\n",
       " 'text_10-12\\\\10-12-bd.txt',\n",
       " 'text_10-12\\\\10-12-ca.txt',\n",
       " 'text_10-12\\\\10-12-gb.txt',\n",
       " 'text_10-12\\\\10-12-gh.txt',\n",
       " 'text_10-12\\\\10-12-hk.txt',\n",
       " 'text_10-12\\\\10-12-ie.txt',\n",
       " 'text_10-12\\\\10-12-in.txt',\n",
       " 'text_10-12\\\\10-12-jm.txt',\n",
       " 'text_10-12\\\\10-12-ke.txt',\n",
       " 'text_10-12\\\\10-12-lk.txt',\n",
       " 'text_10-12\\\\10-12-my.txt',\n",
       " 'text_10-12\\\\10-12-ng.txt',\n",
       " 'text_10-12\\\\10-12-nz.txt',\n",
       " 'text_10-12\\\\10-12-ph.txt',\n",
       " 'text_10-12\\\\10-12-pk.txt',\n",
       " 'text_10-12\\\\10-12-sg.txt',\n",
       " 'text_10-12\\\\10-12-tz.txt',\n",
       " 'text_10-12\\\\10-12-us.txt',\n",
       " 'text_10-12\\\\10-12-za.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2010 = []\n",
    "for file in glob.glob('**/10*au*.txt'):\n",
    "    txtfiles_au_2010.append(file)\n",
    "txtfiles_au_2011 = []\n",
    "for file in glob.glob('**/11*au*.txt'):\n",
    "    txtfiles_au_2011.append(file)\n",
    "txtfiles_au_2012 = []\n",
    "for file in glob.glob('**/12*au*.txt'):\n",
    "    txtfiles_au_2012.append(file)\n",
    "txtfiles_au_2013 = []\n",
    "for file in glob.glob('**/13*au*.txt'):\n",
    "    txtfiles_au_2013.append(file)\n",
    "txtfiles_au_2014 = []\n",
    "for file in glob.glob('**/14*au*.txt'):\n",
    "    txtfiles_au_2014.append(file)\n",
    "txtfiles_au_2015 = []\n",
    "for file in glob.glob('**/15*au*.txt'):\n",
    "    txtfiles_au_2015.append(file)\n",
    "txtfiles_au_2016 = []\n",
    "for file in glob.glob('**/16*au*.txt'):\n",
    "    txtfiles_au_2016.append(file)\n",
    "txtfiles_au_2017 = []\n",
    "for file in glob.glob('**/17*au*.txt'):\n",
    "    txtfiles_au_2017.append(file)\n",
    "txtfiles_au_2018 = []\n",
    "for file in glob.glob('**/18*au*.txt'):\n",
    "    txtfiles_au_2018.append(file)\n",
    "txtfiles_au_2019 = []\n",
    "for file in glob.glob('**/19*au*.txt'):\n",
    "    txtfiles_au_2019.append(file)\n",
    "txtfiles_au_2020 = []\n",
    "for file in glob.glob('**/20*au*.txt'):\n",
    "    txtfiles_au_2020.append(file)\n",
    "txtfiles_bd_2010 = []\n",
    "for file in glob.glob('**/10*bd*.txt'):\n",
    "    txtfiles_bd_2010.append(file)\n",
    "txtfiles_bd_2011 = []\n",
    "for file in glob.glob('**/11*bd*.txt'):\n",
    "    txtfiles_bd_2011.append(file)\n",
    "txtfiles_bd_2012 = []\n",
    "for file in glob.glob('**/12*bd*.txt'):\n",
    "    txtfiles_bd_2012.append(file)\n",
    "txtfiles_bd_2013 = []\n",
    "for file in glob.glob('**/13*bd*.txt'):\n",
    "    txtfiles_bd_2013.append(file)\n",
    "txtfiles_bd_2014 = []\n",
    "for file in glob.glob('**/14*bd*.txt'):\n",
    "    txtfiles_bd_2014.append(file)\n",
    "txtfiles_bd_2015 = []\n",
    "for file in glob.glob('**/15*bd*.txt'):\n",
    "    txtfiles_bd_2015.append(file)\n",
    "txtfiles_bd_2016 = []\n",
    "for file in glob.glob('**/16*bd*.txt'):\n",
    "    txtfiles_bd_2016.append(file)\n",
    "txtfiles_bd_2017 = []\n",
    "for file in glob.glob('**/17*bd*.txt'):\n",
    "    txtfiles_bd_2017.append(file)\n",
    "txtfiles_bd_2018 = []\n",
    "for file in glob.glob('**/18*bd*.txt'):\n",
    "    txtfiles_bd_2018.append(file)\n",
    "txtfiles_bd_2019 = []\n",
    "for file in glob.glob('**/19*bd*.txt'):\n",
    "    txtfiles_bd_2019.append(file)\n",
    "txtfiles_bd_2020 = []\n",
    "for file in glob.glob('**/20*bd*.txt'):\n",
    "    txtfiles_bd_2020.append(file)\n",
    "txtfiles_ca_2010 = []\n",
    "for file in glob.glob('**/10*ca*.txt'):\n",
    "    txtfiles_ca_2010.append(file)\n",
    "txtfiles_ca_2011 = []\n",
    "for file in glob.glob('**/11*ca*.txt'):\n",
    "    txtfiles_ca_2011.append(file)\n",
    "txtfiles_ca_2012 = []\n",
    "for file in glob.glob('**/12*ca*.txt'):\n",
    "    txtfiles_ca_2012.append(file)\n",
    "txtfiles_ca_2013 = []\n",
    "for file in glob.glob('**/13*ca*.txt'):\n",
    "    txtfiles_ca_2013.append(file)\n",
    "txtfiles_ca_2014 = []\n",
    "for file in glob.glob('**/14*ca*.txt'):\n",
    "    txtfiles_ca_2014.append(file)\n",
    "txtfiles_ca_2015 = []\n",
    "for file in glob.glob('**/15*ca*.txt'):\n",
    "    txtfiles_ca_2015.append(file)\n",
    "txtfiles_ca_2016 = []\n",
    "for file in glob.glob('**/16*ca*.txt'):\n",
    "    txtfiles_ca_2016.append(file)\n",
    "txtfiles_ca_2017 = []\n",
    "for file in glob.glob('**/17*ca*.txt'):\n",
    "    txtfiles_ca_2017.append(file)\n",
    "txtfiles_ca_2018 = []\n",
    "for file in glob.glob('**/18*ca*.txt'):\n",
    "    txtfiles_ca_2018.append(file)\n",
    "txtfiles_ca_2019 = []\n",
    "for file in glob.glob('**/19*ca*.txt'):\n",
    "    txtfiles_ca_2019.append(file)\n",
    "txtfiles_ca_2020 = []\n",
    "for file in glob.glob('**/20*ca*.txt'):\n",
    "    txtfiles_ca_2020.append(file)\n",
    "txtfiles_gb_2010 = []\n",
    "for file in glob.glob('**/10*gb*.txt'):\n",
    "    txtfiles_gb_2010.append(file)\n",
    "txtfiles_gb_2011 = []\n",
    "for file in glob.glob('**/11*gb*.txt'):\n",
    "    txtfiles_gb_2011.append(file)\n",
    "txtfiles_gb_2012 = []\n",
    "for file in glob.glob('**/12*gb*.txt'):\n",
    "    txtfiles_gb_2012.append(file)\n",
    "txtfiles_gb_2013 = []\n",
    "for file in glob.glob('**/13*gb*.txt'):\n",
    "    txtfiles_gb_2013.append(file)\n",
    "txtfiles_gb_2014 = []\n",
    "for file in glob.glob('**/14*gb*.txt'):\n",
    "    txtfiles_gb_2014.append(file)\n",
    "txtfiles_gb_2015 = []\n",
    "for file in glob.glob('**/15*gb*.txt'):\n",
    "    txtfiles_gb_2015.append(file)\n",
    "txtfiles_gb_2016 = []\n",
    "for file in glob.glob('**/16*gb*.txt'):\n",
    "    txtfiles_gb_2016.append(file)\n",
    "txtfiles_gb_2017 = []\n",
    "for file in glob.glob('**/17*gb*.txt'):\n",
    "    txtfiles_gb_2017.append(file)\n",
    "txtfiles_gb_2018 = []\n",
    "for file in glob.glob('**/18*gb*.txt'):\n",
    "    txtfiles_gb_2018.append(file)\n",
    "txtfiles_gb_2019 = []\n",
    "for file in glob.glob('**/19*gb*.txt'):\n",
    "    txtfiles_gb_2019.append(file)\n",
    "txtfiles_gb_2020 = []\n",
    "for file in glob.glob('**/20*gb*.txt'):\n",
    "    txtfiles_gb_2020.append(file)\n",
    "txtfiles_gh_2010 = []\n",
    "for file in glob.glob('**/10*gh*.txt'):\n",
    "    txtfiles_gh_2010.append(file)\n",
    "txtfiles_gh_2011 = []\n",
    "for file in glob.glob('**/11*gh*.txt'):\n",
    "    txtfiles_gh_2011.append(file)\n",
    "txtfiles_gh_2012 = []\n",
    "for file in glob.glob('**/12*gh*.txt'):\n",
    "    txtfiles_gh_2012.append(file)\n",
    "txtfiles_gh_2013 = []\n",
    "for file in glob.glob('**/13*gh*.txt'):\n",
    "    txtfiles_gh_2013.append(file)\n",
    "txtfiles_gh_2014 = []\n",
    "for file in glob.glob('**/14*gh*.txt'):\n",
    "    txtfiles_gh_2014.append(file)\n",
    "txtfiles_gh_2015 = []\n",
    "for file in glob.glob('**/15*gh*.txt'):\n",
    "    txtfiles_gh_2015.append(file)\n",
    "txtfiles_gh_2016 = []\n",
    "for file in glob.glob('**/16*gh*.txt'):\n",
    "    txtfiles_gh_2016.append(file)\n",
    "txtfiles_gh_2017 = []\n",
    "for file in glob.glob('**/17*gh*.txt'):\n",
    "    txtfiles_gh_2017.append(file)\n",
    "txtfiles_gh_2018 = []\n",
    "for file in glob.glob('**/18*gh*.txt'):\n",
    "    txtfiles_gh_2018.append(file)\n",
    "txtfiles_gh_2019 = []\n",
    "for file in glob.glob('**/19*gh*.txt'):\n",
    "    txtfiles_gh_2019.append(file)\n",
    "txtfiles_gh_2020 = []\n",
    "for file in glob.glob('**/20*gh*.txt'):\n",
    "    txtfiles_gh_2020.append(file)\n",
    "txtfiles_hk_2010 = []\n",
    "for file in glob.glob('**/10*hk*.txt'):\n",
    "    txtfiles_hk_2010.append(file)\n",
    "txtfiles_hk_2011 = []\n",
    "for file in glob.glob('**/11*hk*.txt'):\n",
    "    txtfiles_hk_2011.append(file)\n",
    "txtfiles_hk_2012 = []\n",
    "for file in glob.glob('**/12*hk*.txt'):\n",
    "    txtfiles_hk_2012.append(file)\n",
    "txtfiles_hk_2013 = []\n",
    "for file in glob.glob('**/13*hk*.txt'):\n",
    "    txtfiles_hk_2013.append(file)\n",
    "txtfiles_hk_2014 = []\n",
    "for file in glob.glob('**/14*hk*.txt'):\n",
    "    txtfiles_hk_2014.append(file)\n",
    "txtfiles_hk_2015 = []\n",
    "for file in glob.glob('**/15*hk*.txt'):\n",
    "    txtfiles_hk_2015.append(file)\n",
    "txtfiles_hk_2016 = []\n",
    "for file in glob.glob('**/16*hk*.txt'):\n",
    "    txtfiles_hk_2016.append(file)\n",
    "txtfiles_hk_2017 = []\n",
    "for file in glob.glob('**/17*hk*.txt'):\n",
    "    txtfiles_hk_2017.append(file)\n",
    "txtfiles_hk_2018 = []\n",
    "for file in glob.glob('**/18*hk*.txt'):\n",
    "    txtfiles_hk_2018.append(file)\n",
    "txtfiles_hk_2019 = []\n",
    "for file in glob.glob('**/19*hk*.txt'):\n",
    "    txtfiles_hk_2019.append(file)\n",
    "txtfiles_hk_2020 = []\n",
    "for file in glob.glob('**/20*hk*.txt'):\n",
    "    txtfiles_hk_2020.append(file)\n",
    "txtfiles_ie_2010 = []\n",
    "for file in glob.glob('**/10*ie*.txt'):\n",
    "    txtfiles_ie_2010.append(file)\n",
    "txtfiles_ie_2011 = []\n",
    "for file in glob.glob('**/11*ie*.txt'):\n",
    "    txtfiles_ie_2011.append(file)\n",
    "txtfiles_ie_2012 = []\n",
    "for file in glob.glob('**/12*ie*.txt'):\n",
    "    txtfiles_ie_2012.append(file)\n",
    "txtfiles_ie_2013 = []\n",
    "for file in glob.glob('**/13*ie*.txt'):\n",
    "    txtfiles_ie_2013.append(file)\n",
    "txtfiles_ie_2014 = []\n",
    "for file in glob.glob('**/14*ie*.txt'):\n",
    "    txtfiles_ie_2014.append(file)\n",
    "txtfiles_ie_2015 = []\n",
    "for file in glob.glob('**/15*ie*.txt'):\n",
    "    txtfiles_ie_2015.append(file)\n",
    "txtfiles_ie_2016 = []\n",
    "for file in glob.glob('**/16*ie*.txt'):\n",
    "    txtfiles_ie_2016.append(file)\n",
    "txtfiles_ie_2017 = []\n",
    "for file in glob.glob('**/17*ie*.txt'):\n",
    "    txtfiles_ie_2017.append(file)\n",
    "txtfiles_ie_2018 = []\n",
    "for file in glob.glob('**/18*ie*.txt'):\n",
    "    txtfiles_ie_2018.append(file)\n",
    "txtfiles_ie_2019 = []\n",
    "for file in glob.glob('**/19*ie*.txt'):\n",
    "    txtfiles_ie_2019.append(file)\n",
    "txtfiles_ie_2020 = []\n",
    "for file in glob.glob('**/20*ie*.txt'):\n",
    "    txtfiles_ie_2020.append(file)\n",
    "txtfiles_in_2010 = []\n",
    "for file in glob.glob('**/10*in*.txt'):\n",
    "    txtfiles_in_2010.append(file)\n",
    "txtfiles_in_2011 = []\n",
    "for file in glob.glob('**/11*in*.txt'):\n",
    "    txtfiles_in_2011.append(file)\n",
    "txtfiles_in_2012 = []\n",
    "for file in glob.glob('**/12*in*.txt'):\n",
    "    txtfiles_in_2012.append(file)\n",
    "txtfiles_in_2013 = []\n",
    "for file in glob.glob('**/13*in*.txt'):\n",
    "    txtfiles_in_2013.append(file)\n",
    "txtfiles_in_2014 = []\n",
    "for file in glob.glob('**/14*in*.txt'):\n",
    "    txtfiles_in_2014.append(file)\n",
    "txtfiles_in_2015 = []\n",
    "for file in glob.glob('**/15*in*.txt'):\n",
    "    txtfiles_in_2015.append(file)\n",
    "txtfiles_in_2016 = []\n",
    "for file in glob.glob('**/16*in*.txt'):\n",
    "    txtfiles_in_2016.append(file)\n",
    "txtfiles_in_2017 = []\n",
    "for file in glob.glob('**/17*in*.txt'):\n",
    "    txtfiles_in_2017.append(file)\n",
    "txtfiles_in_2018 = []\n",
    "for file in glob.glob('**/18*in*.txt'):\n",
    "    txtfiles_in_2018.append(file)\n",
    "txtfiles_in_2019 = []\n",
    "for file in glob.glob('**/19*in*.txt'):\n",
    "    txtfiles_in_2019.append(file)\n",
    "txtfiles_in_2020 = []\n",
    "for file in glob.glob('**/20*in*.txt'):\n",
    "    txtfiles_in_2020.append(file)\n",
    "txtfiles_jm_2010 = []\n",
    "for file in glob.glob('**/10*jm*.txt'):\n",
    "    txtfiles_jm_2010.append(file)\n",
    "txtfiles_jm_2011 = []\n",
    "for file in glob.glob('**/11*jm*.txt'):\n",
    "    txtfiles_jm_2011.append(file)\n",
    "txtfiles_jm_2012 = []\n",
    "for file in glob.glob('**/12*jm*.txt'):\n",
    "    txtfiles_jm_2012.append(file)\n",
    "txtfiles_jm_2013 = []\n",
    "for file in glob.glob('**/13*jm*.txt'):\n",
    "    txtfiles_jm_2013.append(file)\n",
    "txtfiles_jm_2014 = []\n",
    "for file in glob.glob('**/14*jm*.txt'):\n",
    "    txtfiles_jm_2014.append(file)\n",
    "txtfiles_jm_2015 = []\n",
    "for file in glob.glob('**/15*jm*.txt'):\n",
    "    txtfiles_jm_2015.append(file)\n",
    "txtfiles_jm_2016 = []\n",
    "for file in glob.glob('**/16*jm*.txt'):\n",
    "    txtfiles_jm_2016.append(file)\n",
    "txtfiles_jm_2017 = []\n",
    "for file in glob.glob('**/17*jm*.txt'):\n",
    "    txtfiles_jm_2017.append(file)\n",
    "txtfiles_jm_2018 = []\n",
    "for file in glob.glob('**/18*jm*.txt'):\n",
    "    txtfiles_jm_2018.append(file)\n",
    "txtfiles_jm_2019 = []\n",
    "for file in glob.glob('**/19*jm*.txt'):\n",
    "    txtfiles_jm_2019.append(file)\n",
    "txtfiles_jm_2020 = []\n",
    "for file in glob.glob('**/20*jm*.txt'):\n",
    "    txtfiles_jm_2020.append(file)\n",
    "txtfiles_ke_2010 = []\n",
    "for file in glob.glob('**/10*ke*.txt'):\n",
    "    txtfiles_ke_2010.append(file)\n",
    "txtfiles_ke_2011 = []\n",
    "for file in glob.glob('**/11*ke*.txt'):\n",
    "    txtfiles_ke_2011.append(file)\n",
    "txtfiles_ke_2012 = []\n",
    "for file in glob.glob('**/12*ke*.txt'):\n",
    "    txtfiles_ke_2012.append(file)\n",
    "txtfiles_ke_2013 = []\n",
    "for file in glob.glob('**/13*ke*.txt'):\n",
    "    txtfiles_ke_2013.append(file)\n",
    "txtfiles_ke_2014 = []\n",
    "for file in glob.glob('**/14*ke*.txt'):\n",
    "    txtfiles_ke_2014.append(file)\n",
    "txtfiles_ke_2015 = []\n",
    "for file in glob.glob('**/15*ke*.txt'):\n",
    "    txtfiles_ke_2015.append(file)\n",
    "txtfiles_ke_2016 = []\n",
    "for file in glob.glob('**/16*ke*.txt'):\n",
    "    txtfiles_ke_2016.append(file)\n",
    "txtfiles_ke_2017 = []\n",
    "for file in glob.glob('**/17*ke*.txt'):\n",
    "    txtfiles_ke_2017.append(file)\n",
    "txtfiles_ke_2018 = []\n",
    "for file in glob.glob('**/18*ke*.txt'):\n",
    "    txtfiles_ke_2018.append(file)\n",
    "txtfiles_ke_2019 = []\n",
    "for file in glob.glob('**/19*ke*.txt'):\n",
    "    txtfiles_ke_2019.append(file)\n",
    "txtfiles_ke_2020 = []\n",
    "for file in glob.glob('**/20*ke*.txt'):\n",
    "    txtfiles_ke_2020.append(file)\n",
    "txtfiles_lk_2010 = []\n",
    "for file in glob.glob('**/10*lk*.txt'):\n",
    "    txtfiles_lk_2010.append(file)\n",
    "txtfiles_lk_2011 = []\n",
    "for file in glob.glob('**/11*lk*.txt'):\n",
    "    txtfiles_lk_2011.append(file)\n",
    "txtfiles_lk_2012 = []\n",
    "for file in glob.glob('**/12*lk*.txt'):\n",
    "    txtfiles_lk_2012.append(file)\n",
    "txtfiles_lk_2013 = []\n",
    "for file in glob.glob('**/13*lk*.txt'):\n",
    "    txtfiles_lk_2013.append(file)\n",
    "txtfiles_lk_2014 = []\n",
    "for file in glob.glob('**/14*lk*.txt'):\n",
    "    txtfiles_lk_2014.append(file)\n",
    "txtfiles_lk_2015 = []\n",
    "for file in glob.glob('**/15*lk*.txt'):\n",
    "    txtfiles_lk_2015.append(file)\n",
    "txtfiles_lk_2016 = []\n",
    "for file in glob.glob('**/16*lk*.txt'):\n",
    "    txtfiles_lk_2016.append(file)\n",
    "txtfiles_lk_2017 = []\n",
    "for file in glob.glob('**/17*lk*.txt'):\n",
    "    txtfiles_lk_2017.append(file)\n",
    "txtfiles_lk_2018 = []\n",
    "for file in glob.glob('**/18*lk*.txt'):\n",
    "    txtfiles_lk_2018.append(file)\n",
    "txtfiles_lk_2019 = []\n",
    "for file in glob.glob('**/19*lk*.txt'):\n",
    "    txtfiles_lk_2019.append(file)\n",
    "txtfiles_lk_2020 = []\n",
    "for file in glob.glob('**/20*lk*.txt'):\n",
    "    txtfiles_lk_2020.append(file)\n",
    "txtfiles_my_2010 = []\n",
    "for file in glob.glob('**/10*my*.txt'):\n",
    "    txtfiles_my_2010.append(file)\n",
    "txtfiles_my_2011 = []\n",
    "for file in glob.glob('**/11*my*.txt'):\n",
    "    txtfiles_my_2011.append(file)\n",
    "txtfiles_my_2012 = []\n",
    "for file in glob.glob('**/12*my*.txt'):\n",
    "    txtfiles_my_2012.append(file)\n",
    "txtfiles_my_2013 = []\n",
    "for file in glob.glob('**/13*my*.txt'):\n",
    "    txtfiles_my_2013.append(file)\n",
    "txtfiles_my_2014 = []\n",
    "for file in glob.glob('**/14*my*.txt'):\n",
    "    txtfiles_my_2014.append(file)\n",
    "txtfiles_my_2015 = []\n",
    "for file in glob.glob('**/15*my*.txt'):\n",
    "    txtfiles_my_2015.append(file)\n",
    "txtfiles_my_2016 = []\n",
    "for file in glob.glob('**/16*my*.txt'):\n",
    "    txtfiles_my_2016.append(file)\n",
    "txtfiles_my_2017 = []\n",
    "for file in glob.glob('**/17*my*.txt'):\n",
    "    txtfiles_my_2017.append(file)\n",
    "txtfiles_my_2018 = []\n",
    "for file in glob.glob('**/18*my*.txt'):\n",
    "    txtfiles_my_2018.append(file)\n",
    "txtfiles_my_2019 = []\n",
    "for file in glob.glob('**/19*my*.txt'):\n",
    "    txtfiles_my_2019.append(file)\n",
    "txtfiles_my_2020 = []\n",
    "for file in glob.glob('**/20*my*.txt'):\n",
    "    txtfiles_my_2020.append(file)\n",
    "txtfiles_ng_2010 = []\n",
    "for file in glob.glob('**/10*ng*.txt'):\n",
    "    txtfiles_ng_2010.append(file)\n",
    "txtfiles_ng_2011 = []\n",
    "for file in glob.glob('**/11*ng*.txt'):\n",
    "    txtfiles_ng_2011.append(file)\n",
    "txtfiles_ng_2012 = []\n",
    "for file in glob.glob('**/12*ng*.txt'):\n",
    "    txtfiles_ng_2012.append(file)\n",
    "txtfiles_ng_2013 = []\n",
    "for file in glob.glob('**/13*ng*.txt'):\n",
    "    txtfiles_ng_2013.append(file)\n",
    "txtfiles_ng_2014 = []\n",
    "for file in glob.glob('**/14*ng*.txt'):\n",
    "    txtfiles_ng_2014.append(file)\n",
    "txtfiles_ng_2015 = []\n",
    "for file in glob.glob('**/15*ng*.txt'):\n",
    "    txtfiles_ng_2015.append(file)\n",
    "txtfiles_ng_2016 = []\n",
    "for file in glob.glob('**/16*ng*.txt'):\n",
    "    txtfiles_ng_2016.append(file)\n",
    "txtfiles_ng_2017 = []\n",
    "for file in glob.glob('**/17*ng*.txt'):\n",
    "    txtfiles_ng_2017.append(file)\n",
    "txtfiles_ng_2018 = []\n",
    "for file in glob.glob('**/18*ng*.txt'):\n",
    "    txtfiles_ng_2018.append(file)\n",
    "txtfiles_ng_2019 = []\n",
    "for file in glob.glob('**/19*ng*.txt'):\n",
    "    txtfiles_ng_2019.append(file)\n",
    "txtfiles_ng_2020 = []\n",
    "for file in glob.glob('**/20*ng*.txt'):\n",
    "    txtfiles_ng_2020.append(file)\n",
    "txtfiles_nz_2010 = []\n",
    "for file in glob.glob('**/10*nz*.txt'):\n",
    "    txtfiles_nz_2010.append(file)\n",
    "txtfiles_nz_2011 = []\n",
    "for file in glob.glob('**/11*nz*.txt'):\n",
    "    txtfiles_nz_2011.append(file)\n",
    "txtfiles_nz_2012 = []\n",
    "for file in glob.glob('**/12*nz*.txt'):\n",
    "    txtfiles_nz_2012.append(file)\n",
    "txtfiles_nz_2013 = []\n",
    "for file in glob.glob('**/13*nz*.txt'):\n",
    "    txtfiles_nz_2013.append(file)\n",
    "txtfiles_nz_2014 = []\n",
    "for file in glob.glob('**/14*nz*.txt'):\n",
    "    txtfiles_nz_2014.append(file)\n",
    "txtfiles_nz_2015 = []\n",
    "for file in glob.glob('**/15*nz*.txt'):\n",
    "    txtfiles_nz_2015.append(file)\n",
    "txtfiles_nz_2016 = []\n",
    "for file in glob.glob('**/16*nz*.txt'):\n",
    "    txtfiles_nz_2016.append(file)\n",
    "txtfiles_nz_2017 = []\n",
    "for file in glob.glob('**/17*nz*.txt'):\n",
    "    txtfiles_nz_2017.append(file)\n",
    "txtfiles_nz_2018 = []\n",
    "for file in glob.glob('**/18*nz*.txt'):\n",
    "    txtfiles_nz_2018.append(file)\n",
    "txtfiles_nz_2019 = []\n",
    "for file in glob.glob('**/19*nz*.txt'):\n",
    "    txtfiles_nz_2019.append(file)\n",
    "txtfiles_nz_2020 = []\n",
    "for file in glob.glob('**/20*nz*.txt'):\n",
    "    txtfiles_nz_2020.append(file)\n",
    "txtfiles_ph_2010 = []\n",
    "for file in glob.glob('**/10*ph*.txt'):\n",
    "    txtfiles_ph_2010.append(file)\n",
    "txtfiles_ph_2011 = []\n",
    "for file in glob.glob('**/11*ph*.txt'):\n",
    "    txtfiles_ph_2011.append(file)\n",
    "txtfiles_ph_2012 = []\n",
    "for file in glob.glob('**/12*ph*.txt'):\n",
    "    txtfiles_ph_2012.append(file)\n",
    "txtfiles_ph_2013 = []\n",
    "for file in glob.glob('**/13*ph*.txt'):\n",
    "    txtfiles_ph_2013.append(file)\n",
    "txtfiles_ph_2014 = []\n",
    "for file in glob.glob('**/14*ph*.txt'):\n",
    "    txtfiles_ph_2014.append(file)\n",
    "txtfiles_ph_2015 = []\n",
    "for file in glob.glob('**/15*ph*.txt'):\n",
    "    txtfiles_ph_2015.append(file)\n",
    "txtfiles_ph_2016 = []\n",
    "for file in glob.glob('**/16*ph*.txt'):\n",
    "    txtfiles_ph_2016.append(file)\n",
    "txtfiles_ph_2017 = []\n",
    "for file in glob.glob('**/17*ph*.txt'):\n",
    "    txtfiles_ph_2017.append(file)\n",
    "txtfiles_ph_2018 = []\n",
    "for file in glob.glob('**/18*ph*.txt'):\n",
    "    txtfiles_ph_2018.append(file)\n",
    "txtfiles_ph_2019 = []\n",
    "for file in glob.glob('**/19*ph*.txt'):\n",
    "    txtfiles_ph_2019.append(file)\n",
    "txtfiles_ph_2020 = []\n",
    "for file in glob.glob('**/20*ph*.txt'):\n",
    "    txtfiles_ph_2020.append(file)\n",
    "txtfiles_pk_2010 = []\n",
    "for file in glob.glob('**/10*pk*.txt'):\n",
    "    txtfiles_pk_2010.append(file)\n",
    "txtfiles_pk_2011 = []\n",
    "for file in glob.glob('**/11*pk*.txt'):\n",
    "    txtfiles_pk_2011.append(file)\n",
    "txtfiles_pk_2012 = []\n",
    "for file in glob.glob('**/12*pk*.txt'):\n",
    "    txtfiles_pk_2012.append(file)\n",
    "txtfiles_pk_2013 = []\n",
    "for file in glob.glob('**/13*pk*.txt'):\n",
    "    txtfiles_pk_2013.append(file)\n",
    "txtfiles_pk_2014 = []\n",
    "for file in glob.glob('**/14*pk*.txt'):\n",
    "    txtfiles_pk_2014.append(file)\n",
    "txtfiles_pk_2015 = []\n",
    "for file in glob.glob('**/15*pk*.txt'):\n",
    "    txtfiles_pk_2015.append(file)\n",
    "txtfiles_pk_2016 = []\n",
    "for file in glob.glob('**/16*pk*.txt'):\n",
    "    txtfiles_pk_2016.append(file)\n",
    "txtfiles_pk_2017 = []\n",
    "for file in glob.glob('**/17*pk*.txt'):\n",
    "    txtfiles_pk_2017.append(file)\n",
    "txtfiles_pk_2018 = []\n",
    "for file in glob.glob('**/18*pk*.txt'):\n",
    "    txtfiles_pk_2018.append(file)\n",
    "txtfiles_pk_2019 = []\n",
    "for file in glob.glob('**/19*pk*.txt'):\n",
    "    txtfiles_pk_2019.append(file)\n",
    "txtfiles_pk_2020 = []\n",
    "for file in glob.glob('**/20*pk*.txt'):\n",
    "    txtfiles_pk_2020.append(file)\n",
    "txtfiles_sg_2010 = []\n",
    "for file in glob.glob('**/10*sg*.txt'):\n",
    "    txtfiles_sg_2010.append(file)\n",
    "txtfiles_sg_2011 = []\n",
    "for file in glob.glob('**/11*sg*.txt'):\n",
    "    txtfiles_sg_2011.append(file)\n",
    "txtfiles_sg_2012 = []\n",
    "for file in glob.glob('**/12*sg*.txt'):\n",
    "    txtfiles_sg_2012.append(file)\n",
    "txtfiles_sg_2013 = []\n",
    "for file in glob.glob('**/13*sg*.txt'):\n",
    "    txtfiles_sg_2013.append(file)\n",
    "txtfiles_sg_2014 = []\n",
    "for file in glob.glob('**/14*sg*.txt'):\n",
    "    txtfiles_sg_2014.append(file)\n",
    "txtfiles_sg_2015 = []\n",
    "for file in glob.glob('**/15*sg*.txt'):\n",
    "    txtfiles_sg_2015.append(file)\n",
    "txtfiles_sg_2016 = []\n",
    "for file in glob.glob('**/16*sg*.txt'):\n",
    "    txtfiles_sg_2016.append(file)\n",
    "txtfiles_sg_2017 = []\n",
    "for file in glob.glob('**/17*sg*.txt'):\n",
    "    txtfiles_sg_2017.append(file)\n",
    "txtfiles_sg_2018 = []\n",
    "for file in glob.glob('**/18*sg*.txt'):\n",
    "    txtfiles_sg_2018.append(file)\n",
    "txtfiles_sg_2019 = []\n",
    "for file in glob.glob('**/19*sg*.txt'):\n",
    "    txtfiles_sg_2019.append(file)\n",
    "txtfiles_sg_2020 = []\n",
    "for file in glob.glob('**/20*sg*.txt'):\n",
    "    txtfiles_sg_2020.append(file)\n",
    "txtfiles_tz_2010 = []\n",
    "for file in glob.glob('**/10*tz*.txt'):\n",
    "    txtfiles_tz_2010.append(file)\n",
    "txtfiles_tz_2011 = []\n",
    "for file in glob.glob('**/11*tz*.txt'):\n",
    "    txtfiles_tz_2011.append(file)\n",
    "txtfiles_tz_2012 = []\n",
    "for file in glob.glob('**/12*tz*.txt'):\n",
    "    txtfiles_tz_2012.append(file)\n",
    "txtfiles_tz_2013 = []\n",
    "for file in glob.glob('**/13*tz*.txt'):\n",
    "    txtfiles_tz_2013.append(file)\n",
    "txtfiles_tz_2014 = []\n",
    "for file in glob.glob('**/14*tz*.txt'):\n",
    "    txtfiles_tz_2014.append(file)\n",
    "txtfiles_tz_2015 = []\n",
    "for file in glob.glob('**/15*tz*.txt'):\n",
    "    txtfiles_tz_2015.append(file)\n",
    "txtfiles_tz_2016 = []\n",
    "for file in glob.glob('**/16*tz*.txt'):\n",
    "    txtfiles_tz_2016.append(file)\n",
    "txtfiles_tz_2017 = []\n",
    "for file in glob.glob('**/17*tz*.txt'):\n",
    "    txtfiles_tz_2017.append(file)\n",
    "txtfiles_tz_2018 = []\n",
    "for file in glob.glob('**/18*tz*.txt'):\n",
    "    txtfiles_tz_2018.append(file)\n",
    "txtfiles_tz_2019 = []\n",
    "for file in glob.glob('**/19*tz*.txt'):\n",
    "    txtfiles_tz_2019.append(file)\n",
    "txtfiles_tz_2020 = []\n",
    "for file in glob.glob('**/20*tz*.txt'):\n",
    "    txtfiles_tz_2020.append(file)\n",
    "txtfiles_us_2010 = []\n",
    "for file in glob.glob('**/10*us*.txt'):\n",
    "    txtfiles_us_2010.append(file)\n",
    "txtfiles_us_2011 = []\n",
    "for file in glob.glob('**/11*us*.txt'):\n",
    "    txtfiles_us_2011.append(file)\n",
    "txtfiles_us_2012 = []\n",
    "for file in glob.glob('**/12*us*.txt'):\n",
    "    txtfiles_us_2012.append(file)\n",
    "txtfiles_us_2013 = []\n",
    "for file in glob.glob('**/13*us*.txt'):\n",
    "    txtfiles_us_2013.append(file)\n",
    "txtfiles_us_2014 = []\n",
    "for file in glob.glob('**/14*us*.txt'):\n",
    "    txtfiles_us_2014.append(file)\n",
    "txtfiles_us_2015 = []\n",
    "for file in glob.glob('**/15*us*.txt'):\n",
    "    txtfiles_us_2015.append(file)\n",
    "txtfiles_us_2016 = []\n",
    "for file in glob.glob('**/16*us*.txt'):\n",
    "    txtfiles_us_2016.append(file)\n",
    "txtfiles_us_2017 = []\n",
    "for file in glob.glob('**/17*us*.txt'):\n",
    "    txtfiles_us_2017.append(file)\n",
    "txtfiles_us_2018 = []\n",
    "for file in glob.glob('**/18*us*.txt'):\n",
    "    txtfiles_us_2018.append(file)\n",
    "txtfiles_us_2019 = []\n",
    "for file in glob.glob('**/19*us*.txt'):\n",
    "    txtfiles_us_2019.append(file)\n",
    "txtfiles_us_2020 = []\n",
    "for file in glob.glob('**/20*us*.txt'):\n",
    "    txtfiles_us_2020.append(file)\n",
    "txtfiles_za_2010 = []\n",
    "for file in glob.glob('**/10*za*.txt'):\n",
    "    txtfiles_za_2010.append(file)\n",
    "txtfiles_za_2011 = []\n",
    "for file in glob.glob('**/11*za*.txt'):\n",
    "    txtfiles_za_2011.append(file)\n",
    "txtfiles_za_2012 = []\n",
    "for file in glob.glob('**/12*za*.txt'):\n",
    "    txtfiles_za_2012.append(file)\n",
    "txtfiles_za_2013 = []\n",
    "for file in glob.glob('**/13*za*.txt'):\n",
    "    txtfiles_za_2013.append(file)\n",
    "txtfiles_za_2014 = []\n",
    "for file in glob.glob('**/14*za*.txt'):\n",
    "    txtfiles_za_2014.append(file)\n",
    "txtfiles_za_2015 = []\n",
    "for file in glob.glob('**/15*za*.txt'):\n",
    "    txtfiles_za_2015.append(file)\n",
    "txtfiles_za_2016 = []\n",
    "for file in glob.glob('**/16*za*.txt'):\n",
    "    txtfiles_za_2016.append(file)\n",
    "txtfiles_za_2017 = []\n",
    "for file in glob.glob('**/17*za*.txt'):\n",
    "    txtfiles_za_2017.append(file)\n",
    "txtfiles_za_2018 = []\n",
    "for file in glob.glob('**/18*za*.txt'):\n",
    "    txtfiles_za_2018.append(file)\n",
    "txtfiles_za_2019 = []\n",
    "for file in glob.glob('**/19*za*.txt'):\n",
    "    txtfiles_za_2019.append(file)\n",
    "txtfiles_za_2020 = []\n",
    "for file in glob.glob('**/20*za*.txt'):\n",
    "    txtfiles_za_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_12-01\\\\12-01-bd.txt',\n",
       " 'text_12-02\\\\12-02-bd.txt',\n",
       " 'text_12-03\\\\12-03-bd.txt',\n",
       " 'text_12-04\\\\12-04-bd.txt',\n",
       " 'text_12-05\\\\12-05-bd.txt',\n",
       " 'text_12-06\\\\12-06-bd.txt',\n",
       " 'text_12-07\\\\12-07-bd.txt',\n",
       " 'text_12-08\\\\12-08-bd.txt',\n",
       " 'text_12-09\\\\12-09-bd.txt',\n",
       " 'text_12-10\\\\12-10-bd.txt',\n",
       " 'text_12-11\\\\12-11-bd.txt',\n",
       " 'text_12-12\\\\12-12-bd.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_bd_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-17-01\\\\17-01-au.txt',\n",
       " 'text-17-02\\\\17-02-au.txt',\n",
       " 'text-17-03\\\\17-03-au.txt',\n",
       " 'text-17-04\\\\17-04-au.txt',\n",
       " 'text-17-05\\\\17-05-au.txt',\n",
       " 'text-17-06\\\\17-06-au.txt',\n",
       " 'text-17-07\\\\17-07-au.txt',\n",
       " 'text-17-08\\\\17-08-au.txt',\n",
       " 'text-17-09\\\\17-09-au.txt',\n",
       " 'text-17-10\\\\17-10-au.txt',\n",
       " 'text-17-11\\\\17-11-au.txt',\n",
       " 'text-17-12\\\\17-12-au.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_au_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_15-01\\\\15-01-au.txt',\n",
       " 'text_15-02\\\\15-02-au.txt',\n",
       " 'text_15-03\\\\15-03-au.txt',\n",
       " 'text_15-04\\\\15-04-au.txt',\n",
       " 'text_15-05\\\\15-05-au.txt',\n",
       " 'text_15-07\\\\15-07-au.txt',\n",
       " 'text_15-08\\\\15-08-au.txt',\n",
       " 'text_15-09\\\\15-09-au.txt',\n",
       " 'text_15-10\\\\15-10-au.txt',\n",
       " 'text_15-11\\\\15-11-au.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_au_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-16-11\\\\16-11-us.txt',\n",
       " 'text-16-12\\\\16-12-us.txt',\n",
       " 'text_16-01\\\\16-01-us.txt',\n",
       " 'text_16-02\\\\16-02-us.txt',\n",
       " 'text_16-03\\\\16-03-us.txt',\n",
       " 'text_16-04\\\\16-04-us.txt',\n",
       " 'text_16-05\\\\16-05-us.txt',\n",
       " 'text_16-06\\\\16-06-us.txt',\n",
       " 'text_16-07\\\\16-07-us.txt',\n",
       " 'text_16-08\\\\16-08-us.txt',\n",
       " 'text_16-09\\\\16-09-us.txt',\n",
       " 'text_16-10\\\\16-10-us.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_us_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-17-01\\\\17-01-us.txt',\n",
       " 'text-17-02\\\\17-02-us.txt',\n",
       " 'text-17-03\\\\17-03-us.txt',\n",
       " 'text-17-04\\\\17-04-us.txt',\n",
       " 'text-17-05\\\\17-05-us.txt',\n",
       " 'text-17-06\\\\17-06-us.txt',\n",
       " 'text-17-07\\\\17-07-us.txt',\n",
       " 'text-17-08\\\\17-08-us.txt',\n",
       " 'text-17-09\\\\17-09-us.txt',\n",
       " 'text-17-10\\\\17-10-us.txt',\n",
       " 'text-17-11\\\\17-11-us.txt',\n",
       " 'text-17-12\\\\17-12-us.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_us_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-18-01\\\\18-01-us.txt',\n",
       " 'text-18-02\\\\18-02-us.txt',\n",
       " 'text-18-03\\\\18-03-us.txt',\n",
       " 'text-18-04\\\\18-04-us.txt',\n",
       " 'text-18-05\\\\18-05-us.txt',\n",
       " 'text-18-06\\\\18-06-us.txt',\n",
       " 'text-18-07\\\\18-07-us.txt',\n",
       " 'text-18-08\\\\18-08-us.txt',\n",
       " 'text-18-09\\\\18-09-us.txt',\n",
       " 'text-18-10\\\\18-10-us.txt',\n",
       " 'text-18-11\\\\18-11-us.txt',\n",
       " 'text-18-12\\\\18-12-us.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_us_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-19-01\\\\19-01-us.txt',\n",
       " 'text-19-02\\\\19-02-us.txt',\n",
       " 'text-19-03\\\\19-03-us.txt',\n",
       " 'text-19-04\\\\19-04-us.txt',\n",
       " 'text-19-05\\\\19-05-us.txt',\n",
       " 'text-19-06\\\\19-06-us.txt',\n",
       " 'text-19-07\\\\19-07-us.txt',\n",
       " 'text-19-08\\\\19-08-us.txt',\n",
       " 'text-19-09\\\\19-09-us.txt',\n",
       " 'text-19-10\\\\19-10-us.txt',\n",
       " 'text-19-11\\\\19-11-us.txt',\n",
       " 'text-19-12\\\\19-12-us.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_us_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-20-01\\\\20-01-us.txt',\n",
       " 'text-20-02\\\\20-02-us.txt',\n",
       " 'text-20-03\\\\20-03-us.txt',\n",
       " 'text-20-04\\\\20-04-us.txt',\n",
       " 'text-20-05\\\\20-05-us.txt',\n",
       " 'text-20-06\\\\20_06-us.txt',\n",
       " 'text-20-07\\\\20_07-us.txt',\n",
       " 'text-20-08\\\\20-08-us.txt',\n",
       " 'text-20-09\\\\20-09-us.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_us_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_10-08\\10-08-au.txt\n"
     ]
    }
   ],
   "source": [
    "print(txtfiles_au_2010[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re, random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from array import *\n",
    "df_2010 = pd.DataFrame()\n",
    "df_2011 = pd.DataFrame()\n",
    "df_2012 = pd.DataFrame()\n",
    "df_2013 = pd.DataFrame()\n",
    "df_2014 = pd.DataFrame()\n",
    "df_2015 = pd.DataFrame()\n",
    "df_2016 = pd.DataFrame()\n",
    "df_2017 = pd.DataFrame()\n",
    "df_2018 = pd.DataFrame()\n",
    "df_2019 = pd.DataFrame()\n",
    "df_2020 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_au_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_au_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_au_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_au_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_au_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_au_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_au_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_au_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_au_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_au_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_au_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_bd_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_bd_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_bd_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_bd_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_bd_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_bd_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_bd_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_bd_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_bd_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_bd_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_bd_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ca_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ca_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ca_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ca_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ca_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ca_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ca_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ca_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ca_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ca_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ca_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_gb_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_gb_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_gb_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_gb_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_gb_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_gb_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_gb_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_gb_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_gb_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_gb_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_gb_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_gh_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_gh_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_gh_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_gh_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_gh_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_gh_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_gh_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_gh_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_gh_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_gh_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_gh_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_hk_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_hk_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_hk_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_hk_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_hk_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_hk_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_hk_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_hk_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_hk_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_hk_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_hk_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ie_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ie_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ie_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ie_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ie_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ie_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ie_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ie_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ie_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ie_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ie_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_in_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_in_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_in_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_in_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_in_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_in_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_in_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_in_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_in_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_in_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_in_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_jm_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_jm_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_jm_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_jm_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_jm_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_jm_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_jm_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_jm_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_jm_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_jm_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_jm_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ke_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ke_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ke_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ke_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ke_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ke_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ke_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ke_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ke_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ke_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ke_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_lk_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_lk_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_lk_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_lk_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_lk_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_lk_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_lk_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_lk_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_lk_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_lk_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_lk_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_my_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_my_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_my_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_my_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_my_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_my_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_my_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_my_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_my_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_my_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_my_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ng_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ng_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ng_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ng_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ng_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ng_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ng_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ng_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ng_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ng_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ng_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_nz_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_nz_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_nz_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_nz_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_nz_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_nz_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_nz_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_nz_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_nz_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_nz_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_nz_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ph_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ph_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ph_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ph_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ph_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ph_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ph_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ph_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ph_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ph_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ph_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_pk_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_pk_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_pk_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_pk_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_pk_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_pk_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_pk_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_pk_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_pk_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_pk_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_pk_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_sg_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_sg_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_sg_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_sg_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_sg_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_sg_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_sg_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_sg_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_sg_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_sg_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_sg_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_tz_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_tz_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_tz_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_tz_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_tz_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_tz_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_tz_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_tz_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_tz_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_tz_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_tz_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_us_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_us_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_us_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_us_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_us_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_us_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_us_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_us_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_us_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_us_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_us_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_za_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_za_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_za_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_za_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_za_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_za_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_za_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_za_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_za_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_za_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    # new1=article1.replace('“','\"')\n",
    "\n",
    "    # new2=new1.replace('”','\"') \n",
    "\n",
    "   # new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall(r\"([^.]*?said[^.]*\\.)\", article1)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_za_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2017</th>\n",
       "      <th>txtfiles_bd_2017</th>\n",
       "      <th>txtfiles_ca_2017</th>\n",
       "      <th>txtfiles_gb_2017</th>\n",
       "      <th>txtfiles_gh_2017</th>\n",
       "      <th>txtfiles_hk_2017</th>\n",
       "      <th>txtfiles_ie_2017</th>\n",
       "      <th>txtfiles_in_2017</th>\n",
       "      <th>txtfiles_jm_2017</th>\n",
       "      <th>txtfiles_ke_2017</th>\n",
       "      <th>txtfiles_lk_2017</th>\n",
       "      <th>txtfiles_my_2017</th>\n",
       "      <th>txtfiles_ng_2017</th>\n",
       "      <th>txtfiles_nz_2017</th>\n",
       "      <th>txtfiles_ph_2017</th>\n",
       "      <th>txtfiles_pk_2017</th>\n",
       "      <th>txtfiles_sg_2017</th>\n",
       "      <th>txtfiles_tz_2017</th>\n",
       "      <th>txtfiles_us_2017</th>\n",
       "      <th>txtfiles_za_2017</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ &lt;p&gt; In a tweet , Reynolds said that reports ...</td>\n",
       "      <td>[ &lt;p&gt; \" We ask all involved to take steps to p...</td>\n",
       "      <td>[ &lt;p&gt; \" My opponents said I would be looking u...</td>\n",
       "      <td>[ If you 're a relatively new user then your c...</td>\n",
       "      <td>[ @ @ @ @ @ @ @ @ @ @ @ 's unflinching commitm...</td>\n",
       "      <td>[ She had experienced war , and she said @ @ @...</td>\n",
       "      <td>[ &lt;p&gt; \" Well , Happy New Year ! We ca n't hear...</td>\n",
       "      <td>[ If unnecessarily I am being looked as the re...</td>\n",
       "      <td>[ &lt;p&gt; Samuda said while the arrangements for t...</td>\n",
       "      <td>[ This is what Jubilee is doing , coming out l...</td>\n",
       "      <td>[lk , Fax : 011 - 2 514 753 \\n@@16179284 ---- ...</td>\n",
       "      <td>[ &lt;p&gt; In addition , its minister Tawfiq Abu Ba...</td>\n",
       "      <td>[ Kalu said despite the current economic chall...</td>\n",
       "      <td>[ \" &lt;p&gt; She said Cresswell was an advocate for...</td>\n",
       "      <td>[ AP/Alexander Zemlianichenko Jr \\n@@16179360 ...</td>\n",
       "      <td>[ &lt;p&gt; \" So far two people are dead , 17 injure...</td>\n",
       "      <td>[ &lt;p&gt; Photo : The Indian Express &lt;p&gt; The 34-ye...</td>\n",
       "      <td>[ We have been spending a lot of money on prot...</td>\n",
       "      <td>[ There are some turtles on the beach , \" said...</td>\n",
       "      <td>[ &lt;p&gt; Speaking to Malawi24 in the aftermath of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ &lt;p&gt; \" It was tough going from the outset , \"...</td>\n",
       "      <td>[ &lt;p&gt; Ban said at an unscheduled news conferen...</td>\n",
       "      <td>[ &lt;p&gt; National Lawyers Guild attorney Rachel L...</td>\n",
       "      <td>[ Brian Sandoval 's office said a lease-agreem...</td>\n",
       "      <td>[ &lt;p&gt; He said agricultural modernisation was n...</td>\n",
       "      <td>[ &lt;p&gt; Organisers said a second march is to tak...</td>\n",
       "      <td>[ &lt;p&gt; \" You can tell that Cristiano really lov...</td>\n",
       "      <td>[ Ahamed , Prime Minister Narendra Modi on Wed...</td>\n",
       "      <td>[ Jamaica is where the people look to for the ...</td>\n",
       "      <td>[ &lt;p&gt; \" As KUPPET we welcome the idea of a new...</td>\n",
       "      <td>[ &lt;p&gt; The government has identified it as one ...</td>\n",
       "      <td>[ &lt;p&gt; \" You can tell that Cristiano really lov...</td>\n",
       "      <td>[ &lt;p&gt; - Advertisement - &lt;p&gt; A source said the ...</td>\n",
       "      <td>[ Made this look like summer , and the show wa...</td>\n",
       "      <td>[ Parang , at that very moment she felt that m...</td>\n",
       "      <td>[ \\n@@16682994 &lt;p&gt; Zubair said that he is a pr...</td>\n",
       "      <td>[ &lt;p&gt; \" You can tell that Cristiano really lov...</td>\n",
       "      <td>[ In addition @ @ @ @ @ @ @ @ @ @ @ pollution ...</td>\n",
       "      <td>[ \" &lt;p&gt; The @ @ @ @ @ @ @ @ @ @ @ said \" Roger...</td>\n",
       "      <td>[ \" &lt;p&gt; \" At the end of the fiscal year , our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ &lt;p&gt; Mr Nicholls said the trains were \" order...</td>\n",
       "      <td>[ &lt;p&gt; \" The strike is causing much suffering ,...</td>\n",
       "      <td>[ &lt;p&gt; \" We tested 17 colours of green before w...</td>\n",
       "      <td>[ &lt;p&gt; \" I said clearly that it 's not for me ,...</td>\n",
       "      <td>[ And I thought I should come here to repeat t...</td>\n",
       "      <td>[ &lt;p&gt; \" Quite straightforwardly , I said that ...</td>\n",
       "      <td>[ \\n@@17174024 &lt;h&gt; FF TD Cowen says water row ...</td>\n",
       "      <td>[ Some of the numbers beneath the surface howe...</td>\n",
       "      <td>[ &lt;p&gt; She said during this phase it is importa...</td>\n",
       "      <td>[ &lt;p&gt; Attorney-General Githu Muigai said an in...</td>\n",
       "      <td>[ He said sex was often a form of exercise \" a...</td>\n",
       "      <td>[ And we will release it to the rightful peopl...</td>\n",
       "      <td>[ &lt;p&gt; A statement by the media department , sa...</td>\n",
       "      <td>[ &lt;p&gt; \" The hardest thing for these kids to re...</td>\n",
       "      <td>[ \" &lt;p&gt; \" After decades of consistent struggle...</td>\n",
       "      <td>[ &lt;p&gt; It was unclear when or if Snap 's drones...</td>\n",
       "      <td>[ The defence ministry said it was reaching ou...</td>\n",
       "      <td>[ The project , which began construction on 1 ...</td>\n",
       "      <td>[@@17173838 &lt;p&gt; \" If you 're inside your home ...</td>\n",
       "      <td>[ &lt;p&gt; Attila on Wednesday said restarting oper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[@@17721352 &lt;p&gt; Cr Smith said 250 homes across...</td>\n",
       "      <td>[@@17721578 &lt;h&gt; The assault on a militant hide...</td>\n",
       "      <td>[ &lt;p&gt; \" It felt good just to be out there on a...</td>\n",
       "      <td>[ &lt;p&gt; The report said : \" Women should be info...</td>\n",
       "      <td>[ Delle is arrogating to himself powers he doe...</td>\n",
       "      <td>[ Civic Party lawmaker Dennis Kwok , co-ordina...</td>\n",
       "      <td>[ Photo Maire BreathnachThat said , in a group...</td>\n",
       "      <td>[ Now it turns out he had said the exact same ...</td>\n",
       "      <td>[ I knew that I would do it because my coach j...</td>\n",
       "      <td>[ &lt;p&gt; \" This initiative accords Kenyans an opp...</td>\n",
       "      <td>[ &lt;p&gt; Quite amoo-sing &lt;p&gt; This story was sent ...</td>\n",
       "      <td>[ -- Reuters picSAN FRANCISCO , April 1 -- Sna...</td>\n",
       "      <td>[@@17721740 &lt;p&gt; He said : \" Is there anything ...</td>\n",
       "      <td>[ &lt;p&gt; Testing will also begin in Miami and San...</td>\n",
       "      <td>[ &lt;p&gt; The fracture was discovered after an MRI...</td>\n",
       "      <td>[@@17721538 &lt;p&gt; Snap Inc said on Friday its Sn...</td>\n",
       "      <td>[ \\n@@17721604 &lt;p&gt; HULU TERENGGANU : The Feder...</td>\n",
       "      <td>[ That said , companies surveyed for the repor...</td>\n",
       "      <td>[ &lt;p&gt; Point guard Joel Berry II returned to pr...</td>\n",
       "      <td>[ I gave them the facts and they said everythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ &lt;p&gt; \" If the size of our operation downsizes...</td>\n",
       "      <td>[ \" &lt;p&gt; In October 2016 , the EC had said it w...</td>\n",
       "      <td>[ There is no way we can run away from it , \" ...</td>\n",
       "      <td>[ &lt;p&gt; Mercurio , the series creator , said : \"...</td>\n",
       "      <td>[ \" &lt;p&gt; Bawumia said the rebuilding of the Gha...</td>\n",
       "      <td>[ &lt;p&gt; Having said that , we have come a long w...</td>\n",
       "      <td>[ Speaking to The Irish Times from his base in...</td>\n",
       "      <td>[ &lt;p&gt; He had also said that the total control ...</td>\n",
       "      <td>[@@18207776 &lt;h&gt; PM zeroes in on value of Falmo...</td>\n",
       "      <td>[ Huddah said the government and the oppositio...</td>\n",
       "      <td>[ &lt;p&gt; The UAE 's official news agency , WAM , ...</td>\n",
       "      <td>[@@18207546 &lt;h&gt; Football &lt;h&gt; West Ham game cru...</td>\n",
       "      <td>[9 FM , Ibadan , who confirmed the incident sa...</td>\n",
       "      <td>[@@18207396 &lt;h&gt; US police kill beer-drinking p...</td>\n",
       "      <td>[ ( Keith Bacongco/Manila Bulletin ) &lt;p&gt; Comel...</td>\n",
       "      <td>[@@18207500 &lt;p&gt; NEW DELHI : Turkish President ...</td>\n",
       "      <td>[ &lt;p&gt; Prime Minister Datuk Seri Najib Razak , ...</td>\n",
       "      <td>[ &lt;p&gt; \" Eka 's Commodity Analytics Cloud analy...</td>\n",
       "      <td>[ It can be said that Third Culture Kids ( TCK...</td>\n",
       "      <td>[@@18207661 &lt;p&gt; Last week Zimbabwe weekly , Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ He said he was the only one in the Governmen...</td>\n",
       "      <td>[@@18748353 &lt;h&gt; IS news agency Amaq founder ki...</td>\n",
       "      <td>[ This underlines our effort to expand Baader ...</td>\n",
       "      <td>[ &lt;p&gt; Former chief cashier , 93-year-old Bill ...</td>\n",
       "      <td>[ &lt;p&gt; Mr Stephen Asamoah-Boateng , the Chief E...</td>\n",
       "      <td>[ \" The PMI index is just a relative value and...</td>\n",
       "      <td>[ &lt;p&gt; Doyle said : \" It was an honour playing ...</td>\n",
       "      <td>[ &lt;p&gt; However , they said , based on the crime...</td>\n",
       "      <td>[ &lt;p&gt; \" I am happy to have a meeting with thes...</td>\n",
       "      <td>[ &lt;p&gt; Mr Kiunjuri said the proposed budget cei...</td>\n",
       "      <td>[8 billion &lt;p&gt; John Keells Hotels said Bentota...</td>\n",
       "      <td>[ \" &lt;p&gt; Although earlier reports said he was a...</td>\n",
       "      <td>[ &lt;p&gt; He said in the course of investigation t...</td>\n",
       "      <td>[ &lt;p&gt; \" After around half an hour the teacher ...</td>\n",
       "      <td>[@@18748499 &lt;h&gt; More than 3,000 trapped in Mar...</td>\n",
       "      <td>[ In an interview with DNA , Katrina was asked...</td>\n",
       "      <td>[ \\n@@18748447 &lt;h&gt; Subscribe to our newsletter...</td>\n",
       "      <td>[ Asherman , CB&amp;amp;I 's President and Chief E...</td>\n",
       "      <td>[@@18747847 &lt;p&gt; Nance was found unresponsive a...</td>\n",
       "      <td>[We 're on top of the group , \" said Brockie ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ &lt;h&gt; You will now receive updates fromSport N...</td>\n",
       "      <td>[ &lt;p&gt; \" The past has shown : the big firms wil...</td>\n",
       "      <td>[ &lt;p&gt; \" Vernon has not had any new residences ...</td>\n",
       "      <td>[com , said : \" We hope that the new test will...</td>\n",
       "      <td>[ &lt;p&gt; Following Friday 's vote , Mrs Merkel sa...</td>\n",
       "      <td>[ ' Even though he said the differences betwee...</td>\n",
       "      <td>[@@19257775 &lt;p&gt; Mr Adams said : \" I do n't bel...</td>\n",
       "      <td>[ When asked about the recent battle with the ...</td>\n",
       "      <td>[@@19261456 &lt;h&gt; Ricketts promises Coley role i...</td>\n",
       "      <td>[ &lt;p&gt; Speaking during the launch of the projec...</td>\n",
       "      <td>[ \\n@@19260124 &lt;p&gt; Stressing that \" Sri Lanka ...</td>\n",
       "      <td>[@@19257852 &lt;h&gt; Sport &lt;h&gt; Former Lions coach M...</td>\n",
       "      <td>[ &lt;p&gt; Speaking on the development , the Oyo St...</td>\n",
       "      <td>[ And the thing that I said is , you have to b...</td>\n",
       "      <td>[ Rand Paul suggested this very idea to the pr...</td>\n",
       "      <td>[ &lt;p&gt; But Merkel said @ @ @ @ @ @ @ @ @ @ conv...</td>\n",
       "      <td>[ &lt;p&gt; The police said that a number of complai...</td>\n",
       "      <td>[ Whitney said : \" With this event , ILTA cont...</td>\n",
       "      <td>[ &lt;p&gt; \" If he is here Saturday , I will let yo...</td>\n",
       "      <td>[ She said the facility will open its doors on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ \\n@@19785819 &lt;h&gt; News break - July 31 &lt;p&gt; \" ...</td>\n",
       "      <td>[ &lt;p&gt; The court declared Section 5 , which emp...</td>\n",
       "      <td>[ &lt;p&gt; Sogou CEO Wang Xiaochuan said in an inte...</td>\n",
       "      <td>[ &lt;p&gt; Speaking to Entertainment Weekly , Varma...</td>\n",
       "      <td>[ &lt;p&gt; Madam Akua Donkor said in her submission...</td>\n",
       "      <td>[ Democratic Party legislator Andrew Wan , who...</td>\n",
       "      <td>[@@19785751 &lt;p&gt; \" The fleet does need to be ov...</td>\n",
       "      <td>[ &lt;p&gt; Digital connectivity plays a critical ro...</td>\n",
       "      <td>[ &lt;p&gt; \" We have started that process , and we ...</td>\n",
       "      <td>[ \\n@@19786378 &lt;p&gt; Raila said those compensate...</td>\n",
       "      <td>[ Breastfeeding is simply one of the most effe...</td>\n",
       "      <td>[ &lt;p&gt; Excel Force said both parties are engage...</td>\n",
       "      <td>[ &lt;p&gt; \" The CAN President referred to the news...</td>\n",
       "      <td>[ \" Everyone knows that I 've just accepted , ...</td>\n",
       "      <td>[ Topacio earlier said they would question the...</td>\n",
       "      <td>[ &lt;p&gt; \" Did not feel like getting up so early ...</td>\n",
       "      <td>[ Doing so is not good clinical practice and i...</td>\n",
       "      <td>[ &lt;p&gt; Kai Twest , Head of Ships and Equipment ...</td>\n",
       "      <td>[ \" said Nationwide Chief Economist Robert Gar...</td>\n",
       "      <td>[ &lt;p&gt; Mashaba said he was personally getting i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ &lt;p&gt; Mr Ferguson said people can be personall...</td>\n",
       "      <td>[ &lt;p&gt; Worshippers will gather at the Baitul Mu...</td>\n",
       "      <td>[ &lt;p&gt; \" I do what I can to make people underst...</td>\n",
       "      <td>[9423 ( drawn from the November 2016 low to th...</td>\n",
       "      <td>[ &lt;p&gt; \" You know very well that , for me , the...</td>\n",
       "      <td>[ The spokesperson said that the notice \" seek...</td>\n",
       "      <td>[ \" &lt;p&gt; Yahama said a medical update woud be i...</td>\n",
       "      <td>[ &lt;p&gt; A bench comprising Chief Justice Dipak M...</td>\n",
       "      <td>[ What I do know is that I am not satisfied wi...</td>\n",
       "      <td>[ PHOTO : NATION &lt;p&gt; The legislator took her d...</td>\n",
       "      <td>[2% in the June 2017 quarter from a year ago w...</td>\n",
       "      <td>[ &lt;p&gt; \" It will be a challenge and a lot of in...</td>\n",
       "      <td>[ &lt;p&gt; Ambode preaches discipline , sacrifice b...</td>\n",
       "      <td>[ The girl who gave to him said it was used by...</td>\n",
       "      <td>[ Source : Reuters &lt;p&gt; \" Clearly the question ...</td>\n",
       "      <td>[ &lt;p&gt; It is expected NAB Rawalpindi will also ...</td>\n",
       "      <td>[ &lt;h&gt; Get The Straits Times newsletters in you...</td>\n",
       "      <td>[ &lt;p&gt; Toshihiko Kai , President and CEO of Nik...</td>\n",
       "      <td>[ &lt;p&gt; The sheriff 's office received a 911 cal...</td>\n",
       "      <td>[ &lt;p&gt; PUST - home to the largest concentration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ &lt;p&gt; \" Today is a very difficult day to annou...</td>\n",
       "      <td>[ &lt;p&gt; In 2000 , \" The Simpsons \" joked in an e...</td>\n",
       "      <td>[ &lt;p&gt; \" Today , we are taking significant acti...</td>\n",
       "      <td>[ &lt;p&gt; The curator of Newark Town Hall museum ,...</td>\n",
       "      <td>[ &lt;p&gt; Tech firms will be monitored by the EU i...</td>\n",
       "      <td>[ &lt;p&gt; \" Even if authoritarianism has already b...</td>\n",
       "      <td>[ I 'd say I had to meet someone but she said ...</td>\n",
       "      <td>[ &lt;p&gt; A sailor on duty at the naval base susta...</td>\n",
       "      <td>[ &lt;p&gt; \" I walked up in Mount Salem , this is w...</td>\n",
       "      <td>[ &lt;p&gt; Wu Zhiqiang , deputy director-general of...</td>\n",
       "      <td>[@@20846988 &lt;h&gt; President , PM stress on well-...</td>\n",
       "      <td>[ \\n@@20847020 &lt;h&gt; Advertisement &lt;h&gt; More stor...</td>\n",
       "      <td>[ &lt;p&gt; He said , \" Recent calls on restructurin...</td>\n",
       "      <td>[ The Greens do n't just have the option of ta...</td>\n",
       "      <td>[ Inspiration came because the first thing I s...</td>\n",
       "      <td>[ There was also a consensus on regular and fo...</td>\n",
       "      <td>[ &lt;p&gt; He said , \" In Kashmir , what ideologica...</td>\n",
       "      <td>[ &lt;p&gt; \" BASF makes it a priority to provide cu...</td>\n",
       "      <td>[ Every day at HPD , we build on the organizat...</td>\n",
       "      <td>[com &lt;p&gt; In an published interview with Drum ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ &lt;p&gt; Speaking on Monday , Coulter-Nile had sa...</td>\n",
       "      <td>[ \\n@@21424371 &lt;p&gt; The second span of the much...</td>\n",
       "      <td>[ He is now an expert on countering violent ex...</td>\n",
       "      <td>[ \" &lt;p&gt; NSPCC chief executive officer Peter Wa...</td>\n",
       "      <td>[ I do not believe that after all is said and ...</td>\n",
       "      <td>[ &lt;p&gt; Kwok said that the Complaints Against Po...</td>\n",
       "      <td>[ &lt;p&gt; \" It 's also an exceedingly tricky proce...</td>\n",
       "      <td>[ &lt;p&gt; However , there is some confusion over t...</td>\n",
       "      <td>[ He said that his recipe for greatness is sim...</td>\n",
       "      <td>[ &lt;p&gt; Unclaimed Financial Assets Authority ( U...</td>\n",
       "      <td>[ They said that girls were negatively affecte...</td>\n",
       "      <td>[@@21423454 &lt;h&gt; Economists : Focus on reforms ...</td>\n",
       "      <td>[ &lt;p&gt; The NJC said the committee led by Justic...</td>\n",
       "      <td>[ &lt;p&gt; 1 November 2017 &lt;p&gt; She said : \" Well , ...</td>\n",
       "      <td>[ &lt;p&gt; Reports from international press said th...</td>\n",
       "      <td>[ She said it is important to understand Pakis...</td>\n",
       "      <td>[ &lt;p&gt; Akshay Kumar has said in the video , \" F...</td>\n",
       "      <td>[ &lt;p&gt; Shane Fitzsimmons , Global Practice Exec...</td>\n",
       "      <td>[ &lt;p&gt; The victims buried in Priceville include...</td>\n",
       "      <td>[ &lt;p&gt; The same ca n't be said about your face ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[ &lt;p&gt; \" As I said yesterday , my decision to r...</td>\n",
       "      <td>[ &lt;p&gt; By December 2018 , no citizen eligible t...</td>\n",
       "      <td>[ &lt;p&gt; Activist supporters and party aides of t...</td>\n",
       "      <td>[ &lt;p&gt; Mary Matthews , mayor of Bungay , said i...</td>\n",
       "      <td>[ &lt;p&gt; Speaking at the Meet-The-Press series to...</td>\n",
       "      <td>[ &lt;p&gt; The number of heavy air pollution days d...</td>\n",
       "      <td>[ &lt;p&gt; Greencore 's chief executive Patrick Cov...</td>\n",
       "      <td>[ &lt;p&gt; A bench of Justice RK Agrawal , Justice ...</td>\n",
       "      <td>[ &lt;p&gt; \" I was telling my team-mates to give me...</td>\n",
       "      <td>[ She said in an interview that it was a bulle...</td>\n",
       "      <td>[ &lt;p&gt; A noted economist , Professor Razeen Sal...</td>\n",
       "      <td>[@@21986758 &lt;p&gt; French pharmaceutical giant Sa...</td>\n",
       "      <td>[ &lt;p&gt; The result , according to them , was all...</td>\n",
       "      <td>[ &lt;p&gt; TIRANA , Albania ( AP ) -- At least one ...</td>\n",
       "      <td>[ &lt;p&gt; In its forecast , the Philippine Atmosph...</td>\n",
       "      <td>[ &lt;p&gt; PM Abbasi is participating in the CHG me...</td>\n",
       "      <td>[ &lt;h&gt; DetailsWhat did India convey to Pakistan...</td>\n",
       "      <td>[ &lt;p&gt; Mr Kigaigai said in the statement that t...</td>\n",
       "      <td>[ &lt;p&gt; \" With these new interesting data we hav...</td>\n",
       "      <td>[ &lt;p&gt; Lambrechts said they were on call that n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     txtfiles_au_2017  \\\n",
       "0   [ <p> In a tweet , Reynolds said that reports ...   \n",
       "1   [ <p> \" It was tough going from the outset , \"...   \n",
       "2   [ <p> Mr Nicholls said the trains were \" order...   \n",
       "3   [@@17721352 <p> Cr Smith said 250 homes across...   \n",
       "4   [ <p> \" If the size of our operation downsizes...   \n",
       "5   [ He said he was the only one in the Governmen...   \n",
       "6   [ <h> You will now receive updates fromSport N...   \n",
       "7   [ \\n@@19785819 <h> News break - July 31 <p> \" ...   \n",
       "8   [ <p> Mr Ferguson said people can be personall...   \n",
       "9   [ <p> \" Today is a very difficult day to annou...   \n",
       "10  [ <p> Speaking on Monday , Coulter-Nile had sa...   \n",
       "11  [ <p> \" As I said yesterday , my decision to r...   \n",
       "\n",
       "                                     txtfiles_bd_2017  \\\n",
       "0   [ <p> \" We ask all involved to take steps to p...   \n",
       "1   [ <p> Ban said at an unscheduled news conferen...   \n",
       "2   [ <p> \" The strike is causing much suffering ,...   \n",
       "3   [@@17721578 <h> The assault on a militant hide...   \n",
       "4   [ \" <p> In October 2016 , the EC had said it w...   \n",
       "5   [@@18748353 <h> IS news agency Amaq founder ki...   \n",
       "6   [ <p> \" The past has shown : the big firms wil...   \n",
       "7   [ <p> The court declared Section 5 , which emp...   \n",
       "8   [ <p> Worshippers will gather at the Baitul Mu...   \n",
       "9   [ <p> In 2000 , \" The Simpsons \" joked in an e...   \n",
       "10  [ \\n@@21424371 <p> The second span of the much...   \n",
       "11  [ <p> By December 2018 , no citizen eligible t...   \n",
       "\n",
       "                                     txtfiles_ca_2017  \\\n",
       "0   [ <p> \" My opponents said I would be looking u...   \n",
       "1   [ <p> National Lawyers Guild attorney Rachel L...   \n",
       "2   [ <p> \" We tested 17 colours of green before w...   \n",
       "3   [ <p> \" It felt good just to be out there on a...   \n",
       "4   [ There is no way we can run away from it , \" ...   \n",
       "5   [ This underlines our effort to expand Baader ...   \n",
       "6   [ <p> \" Vernon has not had any new residences ...   \n",
       "7   [ <p> Sogou CEO Wang Xiaochuan said in an inte...   \n",
       "8   [ <p> \" I do what I can to make people underst...   \n",
       "9   [ <p> \" Today , we are taking significant acti...   \n",
       "10  [ He is now an expert on countering violent ex...   \n",
       "11  [ <p> Activist supporters and party aides of t...   \n",
       "\n",
       "                                     txtfiles_gb_2017  \\\n",
       "0   [ If you 're a relatively new user then your c...   \n",
       "1   [ Brian Sandoval 's office said a lease-agreem...   \n",
       "2   [ <p> \" I said clearly that it 's not for me ,...   \n",
       "3   [ <p> The report said : \" Women should be info...   \n",
       "4   [ <p> Mercurio , the series creator , said : \"...   \n",
       "5   [ <p> Former chief cashier , 93-year-old Bill ...   \n",
       "6   [com , said : \" We hope that the new test will...   \n",
       "7   [ <p> Speaking to Entertainment Weekly , Varma...   \n",
       "8   [9423 ( drawn from the November 2016 low to th...   \n",
       "9   [ <p> The curator of Newark Town Hall museum ,...   \n",
       "10  [ \" <p> NSPCC chief executive officer Peter Wa...   \n",
       "11  [ <p> Mary Matthews , mayor of Bungay , said i...   \n",
       "\n",
       "                                     txtfiles_gh_2017  \\\n",
       "0   [ @ @ @ @ @ @ @ @ @ @ @ 's unflinching commitm...   \n",
       "1   [ <p> He said agricultural modernisation was n...   \n",
       "2   [ And I thought I should come here to repeat t...   \n",
       "3   [ Delle is arrogating to himself powers he doe...   \n",
       "4   [ \" <p> Bawumia said the rebuilding of the Gha...   \n",
       "5   [ <p> Mr Stephen Asamoah-Boateng , the Chief E...   \n",
       "6   [ <p> Following Friday 's vote , Mrs Merkel sa...   \n",
       "7   [ <p> Madam Akua Donkor said in her submission...   \n",
       "8   [ <p> \" You know very well that , for me , the...   \n",
       "9   [ <p> Tech firms will be monitored by the EU i...   \n",
       "10  [ I do not believe that after all is said and ...   \n",
       "11  [ <p> Speaking at the Meet-The-Press series to...   \n",
       "\n",
       "                                     txtfiles_hk_2017  \\\n",
       "0   [ She had experienced war , and she said @ @ @...   \n",
       "1   [ <p> Organisers said a second march is to tak...   \n",
       "2   [ <p> \" Quite straightforwardly , I said that ...   \n",
       "3   [ Civic Party lawmaker Dennis Kwok , co-ordina...   \n",
       "4   [ <p> Having said that , we have come a long w...   \n",
       "5   [ \" The PMI index is just a relative value and...   \n",
       "6   [ ' Even though he said the differences betwee...   \n",
       "7   [ Democratic Party legislator Andrew Wan , who...   \n",
       "8   [ The spokesperson said that the notice \" seek...   \n",
       "9   [ <p> \" Even if authoritarianism has already b...   \n",
       "10  [ <p> Kwok said that the Complaints Against Po...   \n",
       "11  [ <p> The number of heavy air pollution days d...   \n",
       "\n",
       "                                     txtfiles_ie_2017  \\\n",
       "0   [ <p> \" Well , Happy New Year ! We ca n't hear...   \n",
       "1   [ <p> \" You can tell that Cristiano really lov...   \n",
       "2   [ \\n@@17174024 <h> FF TD Cowen says water row ...   \n",
       "3   [ Photo Maire BreathnachThat said , in a group...   \n",
       "4   [ Speaking to The Irish Times from his base in...   \n",
       "5   [ <p> Doyle said : \" It was an honour playing ...   \n",
       "6   [@@19257775 <p> Mr Adams said : \" I do n't bel...   \n",
       "7   [@@19785751 <p> \" The fleet does need to be ov...   \n",
       "8   [ \" <p> Yahama said a medical update woud be i...   \n",
       "9   [ I 'd say I had to meet someone but she said ...   \n",
       "10  [ <p> \" It 's also an exceedingly tricky proce...   \n",
       "11  [ <p> Greencore 's chief executive Patrick Cov...   \n",
       "\n",
       "                                     txtfiles_in_2017  \\\n",
       "0   [ If unnecessarily I am being looked as the re...   \n",
       "1   [ Ahamed , Prime Minister Narendra Modi on Wed...   \n",
       "2   [ Some of the numbers beneath the surface howe...   \n",
       "3   [ Now it turns out he had said the exact same ...   \n",
       "4   [ <p> He had also said that the total control ...   \n",
       "5   [ <p> However , they said , based on the crime...   \n",
       "6   [ When asked about the recent battle with the ...   \n",
       "7   [ <p> Digital connectivity plays a critical ro...   \n",
       "8   [ <p> A bench comprising Chief Justice Dipak M...   \n",
       "9   [ <p> A sailor on duty at the naval base susta...   \n",
       "10  [ <p> However , there is some confusion over t...   \n",
       "11  [ <p> A bench of Justice RK Agrawal , Justice ...   \n",
       "\n",
       "                                     txtfiles_jm_2017  \\\n",
       "0   [ <p> Samuda said while the arrangements for t...   \n",
       "1   [ Jamaica is where the people look to for the ...   \n",
       "2   [ <p> She said during this phase it is importa...   \n",
       "3   [ I knew that I would do it because my coach j...   \n",
       "4   [@@18207776 <h> PM zeroes in on value of Falmo...   \n",
       "5   [ <p> \" I am happy to have a meeting with thes...   \n",
       "6   [@@19261456 <h> Ricketts promises Coley role i...   \n",
       "7   [ <p> \" We have started that process , and we ...   \n",
       "8   [ What I do know is that I am not satisfied wi...   \n",
       "9   [ <p> \" I walked up in Mount Salem , this is w...   \n",
       "10  [ He said that his recipe for greatness is sim...   \n",
       "11  [ <p> \" I was telling my team-mates to give me...   \n",
       "\n",
       "                                     txtfiles_ke_2017  \\\n",
       "0   [ This is what Jubilee is doing , coming out l...   \n",
       "1   [ <p> \" As KUPPET we welcome the idea of a new...   \n",
       "2   [ <p> Attorney-General Githu Muigai said an in...   \n",
       "3   [ <p> \" This initiative accords Kenyans an opp...   \n",
       "4   [ Huddah said the government and the oppositio...   \n",
       "5   [ <p> Mr Kiunjuri said the proposed budget cei...   \n",
       "6   [ <p> Speaking during the launch of the projec...   \n",
       "7   [ \\n@@19786378 <p> Raila said those compensate...   \n",
       "8   [ PHOTO : NATION <p> The legislator took her d...   \n",
       "9   [ <p> Wu Zhiqiang , deputy director-general of...   \n",
       "10  [ <p> Unclaimed Financial Assets Authority ( U...   \n",
       "11  [ She said in an interview that it was a bulle...   \n",
       "\n",
       "                                     txtfiles_lk_2017  \\\n",
       "0   [lk , Fax : 011 - 2 514 753 \\n@@16179284 ---- ...   \n",
       "1   [ <p> The government has identified it as one ...   \n",
       "2   [ He said sex was often a form of exercise \" a...   \n",
       "3   [ <p> Quite amoo-sing <p> This story was sent ...   \n",
       "4   [ <p> The UAE 's official news agency , WAM , ...   \n",
       "5   [8 billion <p> John Keells Hotels said Bentota...   \n",
       "6   [ \\n@@19260124 <p> Stressing that \" Sri Lanka ...   \n",
       "7   [ Breastfeeding is simply one of the most effe...   \n",
       "8   [2% in the June 2017 quarter from a year ago w...   \n",
       "9   [@@20846988 <h> President , PM stress on well-...   \n",
       "10  [ They said that girls were negatively affecte...   \n",
       "11  [ <p> A noted economist , Professor Razeen Sal...   \n",
       "\n",
       "                                     txtfiles_my_2017  \\\n",
       "0   [ <p> In addition , its minister Tawfiq Abu Ba...   \n",
       "1   [ <p> \" You can tell that Cristiano really lov...   \n",
       "2   [ And we will release it to the rightful peopl...   \n",
       "3   [ -- Reuters picSAN FRANCISCO , April 1 -- Sna...   \n",
       "4   [@@18207546 <h> Football <h> West Ham game cru...   \n",
       "5   [ \" <p> Although earlier reports said he was a...   \n",
       "6   [@@19257852 <h> Sport <h> Former Lions coach M...   \n",
       "7   [ <p> Excel Force said both parties are engage...   \n",
       "8   [ <p> \" It will be a challenge and a lot of in...   \n",
       "9   [ \\n@@20847020 <h> Advertisement <h> More stor...   \n",
       "10  [@@21423454 <h> Economists : Focus on reforms ...   \n",
       "11  [@@21986758 <p> French pharmaceutical giant Sa...   \n",
       "\n",
       "                                     txtfiles_ng_2017  \\\n",
       "0   [ Kalu said despite the current economic chall...   \n",
       "1   [ <p> - Advertisement - <p> A source said the ...   \n",
       "2   [ <p> A statement by the media department , sa...   \n",
       "3   [@@17721740 <p> He said : \" Is there anything ...   \n",
       "4   [9 FM , Ibadan , who confirmed the incident sa...   \n",
       "5   [ <p> He said in the course of investigation t...   \n",
       "6   [ <p> Speaking on the development , the Oyo St...   \n",
       "7   [ <p> \" The CAN President referred to the news...   \n",
       "8   [ <p> Ambode preaches discipline , sacrifice b...   \n",
       "9   [ <p> He said , \" Recent calls on restructurin...   \n",
       "10  [ <p> The NJC said the committee led by Justic...   \n",
       "11  [ <p> The result , according to them , was all...   \n",
       "\n",
       "                                     txtfiles_nz_2017  \\\n",
       "0   [ \" <p> She said Cresswell was an advocate for...   \n",
       "1   [ Made this look like summer , and the show wa...   \n",
       "2   [ <p> \" The hardest thing for these kids to re...   \n",
       "3   [ <p> Testing will also begin in Miami and San...   \n",
       "4   [@@18207396 <h> US police kill beer-drinking p...   \n",
       "5   [ <p> \" After around half an hour the teacher ...   \n",
       "6   [ And the thing that I said is , you have to b...   \n",
       "7   [ \" Everyone knows that I 've just accepted , ...   \n",
       "8   [ The girl who gave to him said it was used by...   \n",
       "9   [ The Greens do n't just have the option of ta...   \n",
       "10  [ <p> 1 November 2017 <p> She said : \" Well , ...   \n",
       "11  [ <p> TIRANA , Albania ( AP ) -- At least one ...   \n",
       "\n",
       "                                     txtfiles_ph_2017  \\\n",
       "0   [ AP/Alexander Zemlianichenko Jr \\n@@16179360 ...   \n",
       "1   [ Parang , at that very moment she felt that m...   \n",
       "2   [ \" <p> \" After decades of consistent struggle...   \n",
       "3   [ <p> The fracture was discovered after an MRI...   \n",
       "4   [ ( Keith Bacongco/Manila Bulletin ) <p> Comel...   \n",
       "5   [@@18748499 <h> More than 3,000 trapped in Mar...   \n",
       "6   [ Rand Paul suggested this very idea to the pr...   \n",
       "7   [ Topacio earlier said they would question the...   \n",
       "8   [ Source : Reuters <p> \" Clearly the question ...   \n",
       "9   [ Inspiration came because the first thing I s...   \n",
       "10  [ <p> Reports from international press said th...   \n",
       "11  [ <p> In its forecast , the Philippine Atmosph...   \n",
       "\n",
       "                                     txtfiles_pk_2017  \\\n",
       "0   [ <p> \" So far two people are dead , 17 injure...   \n",
       "1   [ \\n@@16682994 <p> Zubair said that he is a pr...   \n",
       "2   [ <p> It was unclear when or if Snap 's drones...   \n",
       "3   [@@17721538 <p> Snap Inc said on Friday its Sn...   \n",
       "4   [@@18207500 <p> NEW DELHI : Turkish President ...   \n",
       "5   [ In an interview with DNA , Katrina was asked...   \n",
       "6   [ <p> But Merkel said @ @ @ @ @ @ @ @ @ @ conv...   \n",
       "7   [ <p> \" Did not feel like getting up so early ...   \n",
       "8   [ <p> It is expected NAB Rawalpindi will also ...   \n",
       "9   [ There was also a consensus on regular and fo...   \n",
       "10  [ She said it is important to understand Pakis...   \n",
       "11  [ <p> PM Abbasi is participating in the CHG me...   \n",
       "\n",
       "                                     txtfiles_sg_2017  \\\n",
       "0   [ <p> Photo : The Indian Express <p> The 34-ye...   \n",
       "1   [ <p> \" You can tell that Cristiano really lov...   \n",
       "2   [ The defence ministry said it was reaching ou...   \n",
       "3   [ \\n@@17721604 <p> HULU TERENGGANU : The Feder...   \n",
       "4   [ <p> Prime Minister Datuk Seri Najib Razak , ...   \n",
       "5   [ \\n@@18748447 <h> Subscribe to our newsletter...   \n",
       "6   [ <p> The police said that a number of complai...   \n",
       "7   [ Doing so is not good clinical practice and i...   \n",
       "8   [ <h> Get The Straits Times newsletters in you...   \n",
       "9   [ <p> He said , \" In Kashmir , what ideologica...   \n",
       "10  [ <p> Akshay Kumar has said in the video , \" F...   \n",
       "11  [ <h> DetailsWhat did India convey to Pakistan...   \n",
       "\n",
       "                                     txtfiles_tz_2017  \\\n",
       "0   [ We have been spending a lot of money on prot...   \n",
       "1   [ In addition @ @ @ @ @ @ @ @ @ @ @ pollution ...   \n",
       "2   [ The project , which began construction on 1 ...   \n",
       "3   [ That said , companies surveyed for the repor...   \n",
       "4   [ <p> \" Eka 's Commodity Analytics Cloud analy...   \n",
       "5   [ Asherman , CB&amp;I 's President and Chief E...   \n",
       "6   [ Whitney said : \" With this event , ILTA cont...   \n",
       "7   [ <p> Kai Twest , Head of Ships and Equipment ...   \n",
       "8   [ <p> Toshihiko Kai , President and CEO of Nik...   \n",
       "9   [ <p> \" BASF makes it a priority to provide cu...   \n",
       "10  [ <p> Shane Fitzsimmons , Global Practice Exec...   \n",
       "11  [ <p> Mr Kigaigai said in the statement that t...   \n",
       "\n",
       "                                     txtfiles_us_2017  \\\n",
       "0   [ There are some turtles on the beach , \" said...   \n",
       "1   [ \" <p> The @ @ @ @ @ @ @ @ @ @ @ said \" Roger...   \n",
       "2   [@@17173838 <p> \" If you 're inside your home ...   \n",
       "3   [ <p> Point guard Joel Berry II returned to pr...   \n",
       "4   [ It can be said that Third Culture Kids ( TCK...   \n",
       "5   [@@18747847 <p> Nance was found unresponsive a...   \n",
       "6   [ <p> \" If he is here Saturday , I will let yo...   \n",
       "7   [ \" said Nationwide Chief Economist Robert Gar...   \n",
       "8   [ <p> The sheriff 's office received a 911 cal...   \n",
       "9   [ Every day at HPD , we build on the organizat...   \n",
       "10  [ <p> The victims buried in Priceville include...   \n",
       "11  [ <p> \" With these new interesting data we hav...   \n",
       "\n",
       "                                     txtfiles_za_2017  \n",
       "0   [ <p> Speaking to Malawi24 in the aftermath of...  \n",
       "1   [ \" <p> \" At the end of the fiscal year , our ...  \n",
       "2   [ <p> Attila on Wednesday said restarting oper...  \n",
       "3   [ I gave them the facts and they said everythi...  \n",
       "4   [@@18207661 <p> Last week Zimbabwe weekly , Th...  \n",
       "5   [We 're on top of the group , \" said Brockie ....  \n",
       "6   [ She said the facility will open its doors on...  \n",
       "7   [ <p> Mashaba said he was personally getting i...  \n",
       "8   [ <p> PUST - home to the largest concentration...  \n",
       "9   [com <p> In an published interview with Drum ,...  \n",
       "10  [ <p> The same ca n't be said about your face ...  \n",
       "11  [ <p> Lambrechts said they were on call that n...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2018</th>\n",
       "      <th>txtfiles_bd_2018</th>\n",
       "      <th>txtfiles_ca_2018</th>\n",
       "      <th>txtfiles_gb_2018</th>\n",
       "      <th>txtfiles_gh_2018</th>\n",
       "      <th>txtfiles_hk_2018</th>\n",
       "      <th>txtfiles_ie_2018</th>\n",
       "      <th>txtfiles_in_2018</th>\n",
       "      <th>txtfiles_jm_2018</th>\n",
       "      <th>txtfiles_ke_2018</th>\n",
       "      <th>txtfiles_lk_2018</th>\n",
       "      <th>txtfiles_my_2018</th>\n",
       "      <th>txtfiles_ng_2018</th>\n",
       "      <th>txtfiles_nz_2018</th>\n",
       "      <th>txtfiles_ph_2018</th>\n",
       "      <th>txtfiles_pk_2018</th>\n",
       "      <th>txtfiles_sg_2018</th>\n",
       "      <th>txtfiles_tz_2018</th>\n",
       "      <th>txtfiles_us_2018</th>\n",
       "      <th>txtfiles_za_2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ &lt;p&gt; Advertisement &lt;p&gt; \" The recent spikes in...</td>\n",
       "      <td>[ &lt;p&gt; Hasina said she believes Bangladesh 's p...</td>\n",
       "      <td>[ Koo said : \" The calendar year 2017 has been...</td>\n",
       "      <td>[ &lt;p&gt; Inspectors from OfSTED said the quality ...</td>\n",
       "      <td>[ &lt;p&gt; \" Recently , President Akuffo Addo was t...</td>\n",
       "      <td>[ &lt;h&gt; Anbang Buys Its Seoul Headquarters &lt;p&gt; D...</td>\n",
       "      <td>[ This has been going on for a prolonged perio...</td>\n",
       "      <td>[ It will be an interesting tie but I am confi...</td>\n",
       "      <td>[ &lt;p&gt; CARPHA 'S Executive Director , Dr C Jame...</td>\n",
       "      <td>[ &lt;p&gt; She was placed @ @ @ @ @ @ @ @ @ @ prote...</td>\n",
       "      <td>[ They should accurately be aware that this is...</td>\n",
       "      <td>[ &lt;p&gt; In a statement today , Health director-g...</td>\n",
       "      <td>[ I thanked him and said that in the interest ...</td>\n",
       "      <td>[ &lt;p&gt; Roberts said Checkmate is one of the las...</td>\n",
       "      <td>[ &lt;p&gt; Back in 2016 , Chinese officials said th...</td>\n",
       "      <td>[ &lt;p&gt; \" The Federal Board of Revenue ( FBR ) r...</td>\n",
       "      <td>[ &lt;p&gt; \" Happy New Year ! \" Carey said , addres...</td>\n",
       "      <td>[ \" said Christian Brown , President of SNC-La...</td>\n",
       "      <td>[ Dan Dundas said .,  &lt;p&gt; The @ @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ \\n@@22515877 &lt;h&gt; Related Links &lt;p&gt; Durban - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ Picture : AAPSource:AAP &lt;p&gt; Deputy Prime Min...</td>\n",
       "      <td>[ &lt;p&gt; During a visit to an exam centre in Dhak...</td>\n",
       "      <td>[ We believe the intellectual heft and diversi...</td>\n",
       "      <td>[ \" &lt;p&gt; Mrs May \" pointed to the joint trade a...</td>\n",
       "      <td>[ &lt;p&gt; The \" lack of accelerated capital return...</td>\n",
       "      <td>[A significant portion of M&amp;amp;A loans in Sha...</td>\n",
       "      <td>[ &lt;p&gt; \" I believe that , with such a terminal ...</td>\n",
       "      <td>[ &lt;p&gt; Sindh Minister for Planning and Developm...</td>\n",
       "      <td>[ &lt;p&gt; In making the announcement , the British...</td>\n",
       "      <td>[ We wo n't stop speaking our mind due to fear...</td>\n",
       "      <td>[ Indrajit Coomaraswamy said Sri Lanka 's econ...</td>\n",
       "      <td>[ Scoring wise , it is quite easy but to keep ...</td>\n",
       "      <td>[ &lt;p&gt; They were probably too engrossed in the ...</td>\n",
       "      <td>[ Bob Menendez of New Jersey noted in a tweet ...</td>\n",
       "      <td>[ &lt;p&gt; Although they have yet to meet with its ...</td>\n",
       "      <td>[ However , the SBP deputy governor said that ...</td>\n",
       "      <td>[ Here the BJP candidate Jaswant Singh Yadav h...</td>\n",
       "      <td>[ &lt;p&gt; \" It was a problem that required immedia...</td>\n",
       "      <td>[org , which said that her performance through...</td>\n",
       "      <td>[ &lt;p&gt; The research report said that its analys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ &lt;p&gt; Liberal MP Julian Leeser , who will co-c...</td>\n",
       "      <td>[@@23440009 &lt;h&gt; Landslide kills three workers ...</td>\n",
       "      <td>[ &lt;p&gt; \" It was a very comprehensive meeting , ...</td>\n",
       "      <td>[ &lt;p&gt; Britain 's biggest flooring retailer sai...</td>\n",
       "      <td>[ &lt;p&gt; They said the current frustration was du...</td>\n",
       "      <td>[ The Government said today that a standard pl...</td>\n",
       "      <td>[30pm , Devon and Cornwall Police said .,  &lt;p&gt;...</td>\n",
       "      <td>[ &lt;p&gt; Pakistan said the Indian Army gunned dow...</td>\n",
       "      <td>[ &lt;p&gt; By way of examples , he said that , \" An...</td>\n",
       "      <td>[ &lt;p&gt; United Boda Boda Welfare Association cha...</td>\n",
       "      <td>[ &lt;p&gt; \" The Government of Maldives gives high ...</td>\n",
       "      <td>[@@23440017 &lt;p&gt; Umno information chief Annuar ...</td>\n",
       "      <td>[ Twerk it real good @theashleygraham owned th...</td>\n",
       "      <td>[ &lt;p&gt; \" At times it has been a great struggle ...</td>\n",
       "      <td>[ \" &lt;p&gt; Sony Pictures said Wednesday that the ...</td>\n",
       "      <td>[ &lt;p&gt; \" Finding this minuscule signal has open...</td>\n",
       "      <td>[ &lt;p&gt; \" There are themes about love , and a wo...</td>\n",
       "      <td>[ &lt;p&gt; Asked about their comments , he said : \"...</td>\n",
       "      <td>[ In November AD130 , Hadrian , the fidgety Ro...</td>\n",
       "      <td>[ But Google began tuning it for businesses wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ &lt;p&gt; Darmanin told the Emirates Racing @ @ @ ...</td>\n",
       "      <td>[-led Coalition said Saturday .,  &lt;p&gt; \" Coalit...</td>\n",
       "      <td>[ &lt;p&gt; The BDJS is said to be unhappy with the ...</td>\n",
       "      <td>[ &lt;p&gt; In Theresa May 's one and a half minute ...</td>\n",
       "      <td>[\\n@@23959755 &lt;h&gt; Professor Kwaku Asare , a Un...</td>\n",
       "      <td>[ That said , the SFC may recognize a voluntar...</td>\n",
       "      <td>[ &lt;p&gt; Ronan Lyons , an Economics Professor at ...</td>\n",
       "      <td>[ &lt;p&gt; The families depend on flower cultivatio...</td>\n",
       "      <td>[ , Attirella Gayle , an 83-year-old pensioner...</td>\n",
       "      <td>[ He is the biggest guest here and must be tre...</td>\n",
       "      <td>[ &lt;p&gt; He said that the UNFF would not be deter...</td>\n",
       "      <td>[ &lt;p&gt; Pontian police chief Supt Mustafa Bakri ...</td>\n",
       "      <td>[ &lt;p&gt; \" I said before I came here in Mancheste...</td>\n",
       "      <td>[ \" &lt;p&gt; STUFF &lt;p&gt; Wellington City Council chei...</td>\n",
       "      <td>[ \" &lt;p&gt; \" There seems to be no control on who ...</td>\n",
       "      <td>[ ANI/via REUTERS TV &lt;p&gt; Ten people were kille...</td>\n",
       "      <td>[ Singapore 's competition commission , which ...</td>\n",
       "      <td>[\\n@@23992902 &lt;h&gt; Zoff hails Ronaldo : ' The o...</td>\n",
       "      <td>[ \" &lt;p&gt; \" Do you know how much medicine you co...</td>\n",
       "      <td>[ &lt;p&gt; EThekwini Mayor Zandile Gumede said the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ Picture : AAP &lt;p&gt; \" We will keep close eye o...</td>\n",
       "      <td>[ &lt;p&gt; \" It 's exactly people who are the most ...</td>\n",
       "      <td>[ &lt;p&gt; \" I do n't know what the US president wi...</td>\n",
       "      <td>[ some people see it from every window of ever...</td>\n",
       "      <td>[\\n@@24463644 &lt;h&gt; Gabon 's Constitutional Cour...</td>\n",
       "      <td>[ &lt;p&gt; Song said : \" The @ @ @ @ @ @ @ @ @ @ bu...</td>\n",
       "      <td>[ &lt;p&gt; During an earnings call , Cook said the ...</td>\n",
       "      <td>[ &lt;p&gt; If the rising death trend continues , it...</td>\n",
       "      <td>[ &lt;p&gt; \" Effectively , this is what it ( the pi...</td>\n",
       "      <td>[ &lt;p&gt; Prime Minister Emmanuel Issoze Ngondet \"...</td>\n",
       "      <td>[ &lt;p&gt; Minister Sajith Premadasa said that the ...</td>\n",
       "      <td>[ ( AFP photo ) &lt;p&gt; KUALA LUMPUR : Geopolitica...</td>\n",
       "      <td>[ &lt;p&gt; State NLC Chairman , Comrade Ango Adamu ...</td>\n",
       "      <td>[ Sushi Fresh shareholder Billy Li opened his ...</td>\n",
       "      <td>[ &lt;p&gt; He said the IMC is the last resort if th...</td>\n",
       "      <td>[ And now he is standing before us , \" said 92...</td>\n",
       "      <td>[ The team is stronger than ever and it 'll co...</td>\n",
       "      <td>[ We 'll take it one match at a time and hopef...</td>\n",
       "      <td>[ &lt;p&gt; \" It is such an honor getting this award...</td>\n",
       "      <td>[ &lt;p&gt; \" We decided to publish these internal g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ &lt;p&gt; \" In tears myself , I asked my boys to b...</td>\n",
       "      <td>[ &lt;p&gt; \" New Zealand ? ? ? s nine Great Walks a...</td>\n",
       "      <td>[ &lt;p&gt; \" If you want to learn about football , ...</td>\n",
       "      <td>[ &lt;p&gt; The Met Office said people should expect...</td>\n",
       "      <td>[ &lt;p&gt; The GAF said the interdicted soldiers , ...</td>\n",
       "      <td>[ We also publish China Lexicon , a unique ful...</td>\n",
       "      <td>[ &lt;p&gt; Aviva Ireland said it has completed the ...</td>\n",
       "      <td>[\\n@@24946361 &lt;p&gt; There is rotational load she...</td>\n",
       "      <td>[ &lt;p&gt; \" No one can deny that the public passen...</td>\n",
       "      <td>[ &lt;p&gt; Police said Rose Diana Kaikai committed ...</td>\n",
       "      <td>[ Anil Dissanayake said this measure was taken...</td>\n",
       "      <td>[70 ) depending on the distance travelled and ...</td>\n",
       "      <td>[ \\n@@24956394 &lt;h&gt; Five Nigerian soldiers kill...</td>\n",
       "      <td>[ &lt;p&gt; The Australian Competition and Consumer ...</td>\n",
       "      <td>[\\n@@24956248 &lt;h&gt; Animal Hub probe team to sub...</td>\n",
       "      <td>[ \" Mark my words , the general elections will...</td>\n",
       "      <td>[ &lt;p&gt; He said the awards of contracts in the c...</td>\n",
       "      <td>[ &lt;p&gt; Article continues below &lt;p&gt; \" Maybe they...</td>\n",
       "      <td>[ &lt;p&gt; \" By the end of May , a weak run has nev...</td>\n",
       "      <td>[ &lt;p&gt; Nedbank &lt;p&gt; Nedbank has said that they d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ &lt;p&gt; \" It 's about strengthening their resili...</td>\n",
       "      <td>[ A neighbour in the Burari district found the...</td>\n",
       "      <td>[ &lt;p&gt; Her visa expires July 1 , she said .,  ,...</td>\n",
       "      <td>[ &lt;p&gt; Nina said charity support was important ...</td>\n",
       "      <td>[ &lt;p&gt; She said the former Deputy Minister pock...</td>\n",
       "      <td>[ &lt;p&gt; In Sunday 's press release , the governm...</td>\n",
       "      <td>[ Driving is a tremendous responsibility , whi...</td>\n",
       "      <td>[ This is a great opportunity for us in terms ...</td>\n",
       "      <td>[ &lt;p&gt; \" There 's dressing room conversations ,...</td>\n",
       "      <td>[ &lt;p&gt; NSW Labor 's spokesman for industrial re...</td>\n",
       "      <td>[\\n@@25420389 &lt;h&gt; China didn ? ? ? t fund my e...</td>\n",
       "      <td>[ &lt;p&gt; \" It seems like both of them are from di...</td>\n",
       "      <td>[ &lt;p&gt; Lourdes shared the screenshot from her c...</td>\n",
       "      <td>[ &lt;p&gt; \" I thought about it for one or two seco...</td>\n",
       "      <td>[ Avellaneda , DA Regional Office IX OIC infor...</td>\n",
       "      <td>[ &lt;p&gt; But three months after the first request...</td>\n",
       "      <td>[ &lt;p&gt; \" The aviation market is very large and ...</td>\n",
       "      <td>[ \" President Magufuli has directed that the m...</td>\n",
       "      <td>[ &lt;p&gt; Kate Crosbie , former director of perfor...</td>\n",
       "      <td>[ - Fela Kuti **26;36;TOOLONG &lt;p&gt; \" The Shrine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ &lt;p&gt; The electoral board said the party of Pr...</td>\n",
       "      <td>[\\n@@25925811 &lt;h&gt; just in &lt;h&gt; Son released for...</td>\n",
       "      <td>[\\n@@25925383 &lt;p&gt; Tory made the same campaign ...</td>\n",
       "      <td>[ \\n@@25925448 &lt;p&gt; SINGAPORE ( AP ) -- China '...</td>\n",
       "      <td>[ It is up to you to bring it before the house...</td>\n",
       "      <td>[ &lt;p&gt; He said that his books did not sell as w...</td>\n",
       "      <td>[ &lt;p&gt; Mr Corbyn issued the apology after it em...</td>\n",
       "      <td>[ The entire expenses will be borne by the Boa...</td>\n",
       "      <td>[ &lt;p&gt; The court further said that the material...</td>\n",
       "      <td>[ &lt;p&gt; Another visitor , Nguyen Hien Trang , sa...</td>\n",
       "      <td>[\\n@@25926159 &lt;h&gt; Trace City : Gota 's concept...</td>\n",
       "      <td>[ &lt;p&gt; \" I am not trying to defend my time as K...</td>\n",
       "      <td>[ &lt;p&gt; Mr Arnold said an AFP credit card was us...</td>\n",
       "      <td>[ In fact , 60% of gap year participants said ...</td>\n",
       "      <td>[ This strong sales performance in the first h...</td>\n",
       "      <td>[ &lt;p&gt; Sources said that the PTI 's Balochistan...</td>\n",
       "      <td>[ He further said that the West Bengal Chief M...</td>\n",
       "      <td>[ &lt;p&gt; Prof Assad said creation of strong peopl...</td>\n",
       "      <td>[ &lt;p&gt; \" Drawing on music-induced chills resear...</td>\n",
       "      <td>[ Bang on those doors until they fall , \" he s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ &lt;p&gt; CAI Director Professor David Reutens , w...</td>\n",
       "      <td>[ on the other hand , many public representati...</td>\n",
       "      <td>[ &lt;p&gt; Henein said she has repeatedly asked the...</td>\n",
       "      <td>[ Its vice chairman , the Democratic Senator M...</td>\n",
       "      <td>[ Abbas Gullet , Secretary General , KRCS , sa...</td>\n",
       "      <td>[ &lt;p&gt; Dreaver said she was held for three hour...</td>\n",
       "      <td>[ &lt;p&gt; The Wexford hurlers , who were also in P...</td>\n",
       "      <td>[ The consignment had come from Afghanistan th...</td>\n",
       "      <td>[ We do not want the children to become involv...</td>\n",
       "      <td>[ &lt;p&gt; On Monday , Equity Bank chief executive ...</td>\n",
       "      <td>[ &lt;p&gt; Contingents of police will especially be...</td>\n",
       "      <td>[ &lt;p&gt; \" So we are trying to find a way out tha...</td>\n",
       "      <td>[ &lt;p&gt; Baker has said he did not cast a vote in...</td>\n",
       "      <td>[ &lt;p&gt; \" I feel like I 'm probably being cheate...</td>\n",
       "      <td>[ &lt;p&gt; \" PCC , now operating for two and a half...</td>\n",
       "      <td>[ &lt;p&gt; However , there remains a sense of nervo...</td>\n",
       "      <td>[ REUTERS/Maxim Shemetov &lt;p&gt; 05 Sep 2018 03:30...</td>\n",
       "      <td>[ &lt;p&gt; No reason was given for the dismissal , ...</td>\n",
       "      <td>[ &lt;p&gt; Since first opening its doors in 1967 , ...</td>\n",
       "      <td>[ &lt;p&gt; \" Nevertheless , this still means that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ &lt;p&gt; \" Importantly , the nine member federati...</td>\n",
       "      <td>[\\n@@30102363 &lt;h&gt; Journos having no intention ...</td>\n",
       "      <td>[ &lt;p&gt; Chambers , who makes the handcarts on Ma...</td>\n",
       "      <td>[ Carey , said : \" Logistics remains a sought-...</td>\n",
       "      <td>[ &lt;p&gt; Danny Rose has said Tottenham need to pr...</td>\n",
       "      <td>[ He said that he had reviewed my notes and wa...</td>\n",
       "      <td>[1m investment will help Ostoform to expand it...</td>\n",
       "      <td>[ &lt;p&gt; After HBO 's blockbuster show Game of Th...</td>\n",
       "      <td>[ Particularly , the murder of this woman , a ...</td>\n",
       "      <td>[ More than 18,000 people , nearly half of the...</td>\n",
       "      <td>[ &lt;p&gt; Speaking to the media on the award , EFL...</td>\n",
       "      <td>[ &lt;p&gt; Qi Gao , FX strategist with Scotiabank ,...</td>\n",
       "      <td>[ \" &lt;p&gt; Speaking in a chat with THISDAY , Chuk...</td>\n",
       "      <td>[ &lt;p&gt; The Ministry will continue to negotiate ...</td>\n",
       "      <td>[ &lt;p&gt; Delos Santos , who is 66 years old , is ...</td>\n",
       "      <td>[ &lt;p&gt; Fire tenders of Pakistan Navy also helpe...</td>\n",
       "      <td>[ &lt;p&gt; Met after the decision , the family 's c...</td>\n",
       "      <td>[ &lt;p&gt; He said the EAC secretariat , in collabo...</td>\n",
       "      <td>[ &lt;p&gt; \" Any instrument I see I pick it up and ...</td>\n",
       "      <td>[ &lt;p&gt; \" I do n't want to carp about things ? b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ \" Today is not a day to be angry , today 's ...</td>\n",
       "      <td>[ &lt;p&gt; The decision was taken at the syndicate ...</td>\n",
       "      <td>[ &lt;p&gt; \" One of the struggles any team facesis ...</td>\n",
       "      <td>[ &lt;p&gt; She said : \" Do you think I 'd be here i...</td>\n",
       "      <td>[ \\n@@30212730 &lt;h&gt; More &lt;h&gt; We need independen...</td>\n",
       "      <td>[\\n@@50175839 &lt;p&gt; The Hong Kong Observatory sa...</td>\n",
       "      <td>[ &lt;p&gt; He said : \" Today we see what we have be...</td>\n",
       "      <td>[ &lt;p&gt; \" This sale is in line with our stated p...</td>\n",
       "      <td>[ &lt;p&gt; \" At Scotiabank , we share the passion f...</td>\n",
       "      <td>[ When you are playing and you are on the pitc...</td>\n",
       "      <td>[ &lt;p&gt; Safra Anver - Licensee and Organizer , T...</td>\n",
       "      <td>[ &lt;p&gt; \" I take this opportunity to congratulat...</td>\n",
       "      <td>[ Peter Onyekachi Nwaosusu , said that FEDPSG ...</td>\n",
       "      <td>[ &lt;p&gt; The Lyttelton singer-songwriter said of ...</td>\n",
       "      <td>[ We had to do extra training right after ever...</td>\n",
       "      <td>[ &lt;p&gt; \" Mr Zoysa has 14 days from 1 November 2...</td>\n",
       "      <td>[ &lt;p&gt; Professor Glyn Davis , vice chancellor o...</td>\n",
       "      <td>[ &lt;p&gt; The 2018 census was initially planned to...</td>\n",
       "      <td>[\\n@@70200540 &lt;p&gt; \" When the Budget 2019 is ta...</td>\n",
       "      <td>[\\n@@30212638 &lt;h&gt; Julia Roberts ' too scared '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[ Explaining the huge result , Ray White Randw...</td>\n",
       "      <td>[ &lt;p&gt; Humayun Kabir , station officer of Ashul...</td>\n",
       "      <td>[ &lt;p&gt; \" Canada , they 're just getting better ...</td>\n",
       "      <td>[ &lt;p&gt; Although Scottish Government economy min...</td>\n",
       "      <td>[ A press statement signed and issued by Mr Eu...</td>\n",
       "      <td>[ And then it becomes a race to the bottom , \"...</td>\n",
       "      <td>[ &lt;p&gt; ' I have your book in the back , and I n...</td>\n",
       "      <td>[ His friends and acquaintances also said that...</td>\n",
       "      <td>[ &lt;p&gt; Manager , Communication and Customer Ser...</td>\n",
       "      <td>[ &lt;p&gt; ADVERTISEMENT &lt;p&gt; Santa Fe Archbishop Jo...</td>\n",
       "      <td>[\\n@@40336057 &lt;h&gt; Two Indian Coast Guard Ships...</td>\n",
       "      <td>[ &lt;p&gt; Those found guilty could risk losing the...</td>\n",
       "      <td>[ &lt;p&gt; Agu said he has no preferences about whi...</td>\n",
       "      <td>[ &lt;p&gt; Posted : 8:30pm Saturday 01 Dec , 2018 &lt;...</td>\n",
       "      <td>[ I 'm hoping to get a chance to knock him out...</td>\n",
       "      <td>[\\n@@60309236 &lt;h&gt; 8,000 FIRs registered agains...</td>\n",
       "      <td>[ president who was a perpetual motion machine...</td>\n",
       "      <td>[3 tonnes of cashew nuts , \" he said when spea...</td>\n",
       "      <td>[ &lt;p&gt; Paul Manafort , President Trump 's forme...</td>\n",
       "      <td>[com/Prapan Ngawkeaw &lt;p&gt; The risk of having po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     txtfiles_au_2018  \\\n",
       "0   [ <p> Advertisement <p> \" The recent spikes in...   \n",
       "1   [ Picture : AAPSource:AAP <p> Deputy Prime Min...   \n",
       "2   [ <p> Liberal MP Julian Leeser , who will co-c...   \n",
       "3   [ <p> Darmanin told the Emirates Racing @ @ @ ...   \n",
       "4   [ Picture : AAP <p> \" We will keep close eye o...   \n",
       "5   [ <p> \" In tears myself , I asked my boys to b...   \n",
       "6   [ <p> \" It 's about strengthening their resili...   \n",
       "7   [ <p> The electoral board said the party of Pr...   \n",
       "8   [ <p> CAI Director Professor David Reutens , w...   \n",
       "9   [ <p> \" Importantly , the nine member federati...   \n",
       "10  [ \" Today is not a day to be angry , today 's ...   \n",
       "11  [ Explaining the huge result , Ray White Randw...   \n",
       "\n",
       "                                     txtfiles_bd_2018  \\\n",
       "0   [ <p> Hasina said she believes Bangladesh 's p...   \n",
       "1   [ <p> During a visit to an exam centre in Dhak...   \n",
       "2   [@@23440009 <h> Landslide kills three workers ...   \n",
       "3   [-led Coalition said Saturday .,  <p> \" Coalit...   \n",
       "4   [ <p> \" It 's exactly people who are the most ...   \n",
       "5   [ <p> \" New Zealand ? ? ? s nine Great Walks a...   \n",
       "6   [ A neighbour in the Burari district found the...   \n",
       "7   [\\n@@25925811 <h> just in <h> Son released for...   \n",
       "8   [ on the other hand , many public representati...   \n",
       "9   [\\n@@30102363 <h> Journos having no intention ...   \n",
       "10  [ <p> The decision was taken at the syndicate ...   \n",
       "11  [ <p> Humayun Kabir , station officer of Ashul...   \n",
       "\n",
       "                                     txtfiles_ca_2018  \\\n",
       "0   [ Koo said : \" The calendar year 2017 has been...   \n",
       "1   [ We believe the intellectual heft and diversi...   \n",
       "2   [ <p> \" It was a very comprehensive meeting , ...   \n",
       "3   [ <p> The BDJS is said to be unhappy with the ...   \n",
       "4   [ <p> \" I do n't know what the US president wi...   \n",
       "5   [ <p> \" If you want to learn about football , ...   \n",
       "6   [ <p> Her visa expires July 1 , she said .,  ,...   \n",
       "7   [\\n@@25925383 <p> Tory made the same campaign ...   \n",
       "8   [ <p> Henein said she has repeatedly asked the...   \n",
       "9   [ <p> Chambers , who makes the handcarts on Ma...   \n",
       "10  [ <p> \" One of the struggles any team facesis ...   \n",
       "11  [ <p> \" Canada , they 're just getting better ...   \n",
       "\n",
       "                                     txtfiles_gb_2018  \\\n",
       "0   [ <p> Inspectors from OfSTED said the quality ...   \n",
       "1   [ \" <p> Mrs May \" pointed to the joint trade a...   \n",
       "2   [ <p> Britain 's biggest flooring retailer sai...   \n",
       "3   [ <p> In Theresa May 's one and a half minute ...   \n",
       "4   [ some people see it from every window of ever...   \n",
       "5   [ <p> The Met Office said people should expect...   \n",
       "6   [ <p> Nina said charity support was important ...   \n",
       "7   [ \\n@@25925448 <p> SINGAPORE ( AP ) -- China '...   \n",
       "8   [ Its vice chairman , the Democratic Senator M...   \n",
       "9   [ Carey , said : \" Logistics remains a sought-...   \n",
       "10  [ <p> She said : \" Do you think I 'd be here i...   \n",
       "11  [ <p> Although Scottish Government economy min...   \n",
       "\n",
       "                                     txtfiles_gh_2018  \\\n",
       "0   [ <p> \" Recently , President Akuffo Addo was t...   \n",
       "1   [ <p> The \" lack of accelerated capital return...   \n",
       "2   [ <p> They said the current frustration was du...   \n",
       "3   [\\n@@23959755 <h> Professor Kwaku Asare , a Un...   \n",
       "4   [\\n@@24463644 <h> Gabon 's Constitutional Cour...   \n",
       "5   [ <p> The GAF said the interdicted soldiers , ...   \n",
       "6   [ <p> She said the former Deputy Minister pock...   \n",
       "7   [ It is up to you to bring it before the house...   \n",
       "8   [ Abbas Gullet , Secretary General , KRCS , sa...   \n",
       "9   [ <p> Danny Rose has said Tottenham need to pr...   \n",
       "10  [ \\n@@30212730 <h> More <h> We need independen...   \n",
       "11  [ A press statement signed and issued by Mr Eu...   \n",
       "\n",
       "                                     txtfiles_hk_2018  \\\n",
       "0   [ <h> Anbang Buys Its Seoul Headquarters <p> D...   \n",
       "1   [A significant portion of M&amp;A loans in Sha...   \n",
       "2   [ The Government said today that a standard pl...   \n",
       "3   [ That said , the SFC may recognize a voluntar...   \n",
       "4   [ <p> Song said : \" The @ @ @ @ @ @ @ @ @ @ bu...   \n",
       "5   [ We also publish China Lexicon , a unique ful...   \n",
       "6   [ <p> In Sunday 's press release , the governm...   \n",
       "7   [ <p> He said that his books did not sell as w...   \n",
       "8   [ <p> Dreaver said she was held for three hour...   \n",
       "9   [ He said that he had reviewed my notes and wa...   \n",
       "10  [\\n@@50175839 <p> The Hong Kong Observatory sa...   \n",
       "11  [ And then it becomes a race to the bottom , \"...   \n",
       "\n",
       "                                     txtfiles_ie_2018  \\\n",
       "0   [ This has been going on for a prolonged perio...   \n",
       "1   [ <p> \" I believe that , with such a terminal ...   \n",
       "2   [30pm , Devon and Cornwall Police said .,  <p>...   \n",
       "3   [ <p> Ronan Lyons , an Economics Professor at ...   \n",
       "4   [ <p> During an earnings call , Cook said the ...   \n",
       "5   [ <p> Aviva Ireland said it has completed the ...   \n",
       "6   [ Driving is a tremendous responsibility , whi...   \n",
       "7   [ <p> Mr Corbyn issued the apology after it em...   \n",
       "8   [ <p> The Wexford hurlers , who were also in P...   \n",
       "9   [1m investment will help Ostoform to expand it...   \n",
       "10  [ <p> He said : \" Today we see what we have be...   \n",
       "11  [ <p> ' I have your book in the back , and I n...   \n",
       "\n",
       "                                     txtfiles_in_2018  \\\n",
       "0   [ It will be an interesting tie but I am confi...   \n",
       "1   [ <p> Sindh Minister for Planning and Developm...   \n",
       "2   [ <p> Pakistan said the Indian Army gunned dow...   \n",
       "3   [ <p> The families depend on flower cultivatio...   \n",
       "4   [ <p> If the rising death trend continues , it...   \n",
       "5   [\\n@@24946361 <p> There is rotational load she...   \n",
       "6   [ This is a great opportunity for us in terms ...   \n",
       "7   [ The entire expenses will be borne by the Boa...   \n",
       "8   [ The consignment had come from Afghanistan th...   \n",
       "9   [ <p> After HBO 's blockbuster show Game of Th...   \n",
       "10  [ <p> \" This sale is in line with our stated p...   \n",
       "11  [ His friends and acquaintances also said that...   \n",
       "\n",
       "                                     txtfiles_jm_2018  \\\n",
       "0   [ <p> CARPHA 'S Executive Director , Dr C Jame...   \n",
       "1   [ <p> In making the announcement , the British...   \n",
       "2   [ <p> By way of examples , he said that , \" An...   \n",
       "3   [ , Attirella Gayle , an 83-year-old pensioner...   \n",
       "4   [ <p> \" Effectively , this is what it ( the pi...   \n",
       "5   [ <p> \" No one can deny that the public passen...   \n",
       "6   [ <p> \" There 's dressing room conversations ,...   \n",
       "7   [ <p> The court further said that the material...   \n",
       "8   [ We do not want the children to become involv...   \n",
       "9   [ Particularly , the murder of this woman , a ...   \n",
       "10  [ <p> \" At Scotiabank , we share the passion f...   \n",
       "11  [ <p> Manager , Communication and Customer Ser...   \n",
       "\n",
       "                                     txtfiles_ke_2018  \\\n",
       "0   [ <p> She was placed @ @ @ @ @ @ @ @ @ @ prote...   \n",
       "1   [ We wo n't stop speaking our mind due to fear...   \n",
       "2   [ <p> United Boda Boda Welfare Association cha...   \n",
       "3   [ He is the biggest guest here and must be tre...   \n",
       "4   [ <p> Prime Minister Emmanuel Issoze Ngondet \"...   \n",
       "5   [ <p> Police said Rose Diana Kaikai committed ...   \n",
       "6   [ <p> NSW Labor 's spokesman for industrial re...   \n",
       "7   [ <p> Another visitor , Nguyen Hien Trang , sa...   \n",
       "8   [ <p> On Monday , Equity Bank chief executive ...   \n",
       "9   [ More than 18,000 people , nearly half of the...   \n",
       "10  [ When you are playing and you are on the pitc...   \n",
       "11  [ <p> ADVERTISEMENT <p> Santa Fe Archbishop Jo...   \n",
       "\n",
       "                                     txtfiles_lk_2018  \\\n",
       "0   [ They should accurately be aware that this is...   \n",
       "1   [ Indrajit Coomaraswamy said Sri Lanka 's econ...   \n",
       "2   [ <p> \" The Government of Maldives gives high ...   \n",
       "3   [ <p> He said that the UNFF would not be deter...   \n",
       "4   [ <p> Minister Sajith Premadasa said that the ...   \n",
       "5   [ Anil Dissanayake said this measure was taken...   \n",
       "6   [\\n@@25420389 <h> China didn ? ? ? t fund my e...   \n",
       "7   [\\n@@25926159 <h> Trace City : Gota 's concept...   \n",
       "8   [ <p> Contingents of police will especially be...   \n",
       "9   [ <p> Speaking to the media on the award , EFL...   \n",
       "10  [ <p> Safra Anver - Licensee and Organizer , T...   \n",
       "11  [\\n@@40336057 <h> Two Indian Coast Guard Ships...   \n",
       "\n",
       "                                     txtfiles_my_2018  \\\n",
       "0   [ <p> In a statement today , Health director-g...   \n",
       "1   [ Scoring wise , it is quite easy but to keep ...   \n",
       "2   [@@23440017 <p> Umno information chief Annuar ...   \n",
       "3   [ <p> Pontian police chief Supt Mustafa Bakri ...   \n",
       "4   [ ( AFP photo ) <p> KUALA LUMPUR : Geopolitica...   \n",
       "5   [70 ) depending on the distance travelled and ...   \n",
       "6   [ <p> \" It seems like both of them are from di...   \n",
       "7   [ <p> \" I am not trying to defend my time as K...   \n",
       "8   [ <p> \" So we are trying to find a way out tha...   \n",
       "9   [ <p> Qi Gao , FX strategist with Scotiabank ,...   \n",
       "10  [ <p> \" I take this opportunity to congratulat...   \n",
       "11  [ <p> Those found guilty could risk losing the...   \n",
       "\n",
       "                                     txtfiles_ng_2018  \\\n",
       "0   [ I thanked him and said that in the interest ...   \n",
       "1   [ <p> They were probably too engrossed in the ...   \n",
       "2   [ Twerk it real good @theashleygraham owned th...   \n",
       "3   [ <p> \" I said before I came here in Mancheste...   \n",
       "4   [ <p> State NLC Chairman , Comrade Ango Adamu ...   \n",
       "5   [ \\n@@24956394 <h> Five Nigerian soldiers kill...   \n",
       "6   [ <p> Lourdes shared the screenshot from her c...   \n",
       "7   [ <p> Mr Arnold said an AFP credit card was us...   \n",
       "8   [ <p> Baker has said he did not cast a vote in...   \n",
       "9   [ \" <p> Speaking in a chat with THISDAY , Chuk...   \n",
       "10  [ Peter Onyekachi Nwaosusu , said that FEDPSG ...   \n",
       "11  [ <p> Agu said he has no preferences about whi...   \n",
       "\n",
       "                                     txtfiles_nz_2018  \\\n",
       "0   [ <p> Roberts said Checkmate is one of the las...   \n",
       "1   [ Bob Menendez of New Jersey noted in a tweet ...   \n",
       "2   [ <p> \" At times it has been a great struggle ...   \n",
       "3   [ \" <p> STUFF <p> Wellington City Council chei...   \n",
       "4   [ Sushi Fresh shareholder Billy Li opened his ...   \n",
       "5   [ <p> The Australian Competition and Consumer ...   \n",
       "6   [ <p> \" I thought about it for one or two seco...   \n",
       "7   [ In fact , 60% of gap year participants said ...   \n",
       "8   [ <p> \" I feel like I 'm probably being cheate...   \n",
       "9   [ <p> The Ministry will continue to negotiate ...   \n",
       "10  [ <p> The Lyttelton singer-songwriter said of ...   \n",
       "11  [ <p> Posted : 8:30pm Saturday 01 Dec , 2018 <...   \n",
       "\n",
       "                                     txtfiles_ph_2018  \\\n",
       "0   [ <p> Back in 2016 , Chinese officials said th...   \n",
       "1   [ <p> Although they have yet to meet with its ...   \n",
       "2   [ \" <p> Sony Pictures said Wednesday that the ...   \n",
       "3   [ \" <p> \" There seems to be no control on who ...   \n",
       "4   [ <p> He said the IMC is the last resort if th...   \n",
       "5   [\\n@@24956248 <h> Animal Hub probe team to sub...   \n",
       "6   [ Avellaneda , DA Regional Office IX OIC infor...   \n",
       "7   [ This strong sales performance in the first h...   \n",
       "8   [ <p> \" PCC , now operating for two and a half...   \n",
       "9   [ <p> Delos Santos , who is 66 years old , is ...   \n",
       "10  [ We had to do extra training right after ever...   \n",
       "11  [ I 'm hoping to get a chance to knock him out...   \n",
       "\n",
       "                                     txtfiles_pk_2018  \\\n",
       "0   [ <p> \" The Federal Board of Revenue ( FBR ) r...   \n",
       "1   [ However , the SBP deputy governor said that ...   \n",
       "2   [ <p> \" Finding this minuscule signal has open...   \n",
       "3   [ ANI/via REUTERS TV <p> Ten people were kille...   \n",
       "4   [ And now he is standing before us , \" said 92...   \n",
       "5   [ \" Mark my words , the general elections will...   \n",
       "6   [ <p> But three months after the first request...   \n",
       "7   [ <p> Sources said that the PTI 's Balochistan...   \n",
       "8   [ <p> However , there remains a sense of nervo...   \n",
       "9   [ <p> Fire tenders of Pakistan Navy also helpe...   \n",
       "10  [ <p> \" Mr Zoysa has 14 days from 1 November 2...   \n",
       "11  [\\n@@60309236 <h> 8,000 FIRs registered agains...   \n",
       "\n",
       "                                     txtfiles_sg_2018  \\\n",
       "0   [ <p> \" Happy New Year ! \" Carey said , addres...   \n",
       "1   [ Here the BJP candidate Jaswant Singh Yadav h...   \n",
       "2   [ <p> \" There are themes about love , and a wo...   \n",
       "3   [ Singapore 's competition commission , which ...   \n",
       "4   [ The team is stronger than ever and it 'll co...   \n",
       "5   [ <p> He said the awards of contracts in the c...   \n",
       "6   [ <p> \" The aviation market is very large and ...   \n",
       "7   [ He further said that the West Bengal Chief M...   \n",
       "8   [ REUTERS/Maxim Shemetov <p> 05 Sep 2018 03:30...   \n",
       "9   [ <p> Met after the decision , the family 's c...   \n",
       "10  [ <p> Professor Glyn Davis , vice chancellor o...   \n",
       "11  [ president who was a perpetual motion machine...   \n",
       "\n",
       "                                     txtfiles_tz_2018  \\\n",
       "0   [ \" said Christian Brown , President of SNC-La...   \n",
       "1   [ <p> \" It was a problem that required immedia...   \n",
       "2   [ <p> Asked about their comments , he said : \"...   \n",
       "3   [\\n@@23992902 <h> Zoff hails Ronaldo : ' The o...   \n",
       "4   [ We 'll take it one match at a time and hopef...   \n",
       "5   [ <p> Article continues below <p> \" Maybe they...   \n",
       "6   [ \" President Magufuli has directed that the m...   \n",
       "7   [ <p> Prof Assad said creation of strong peopl...   \n",
       "8   [ <p> No reason was given for the dismissal , ...   \n",
       "9   [ <p> He said the EAC secretariat , in collabo...   \n",
       "10  [ <p> The 2018 census was initially planned to...   \n",
       "11  [3 tonnes of cashew nuts , \" he said when spea...   \n",
       "\n",
       "                                     txtfiles_us_2018  \\\n",
       "0   [ Dan Dundas said .,  <p> The @ @ @ @ @ @ @ @ ...   \n",
       "1   [org , which said that her performance through...   \n",
       "2   [ In November AD130 , Hadrian , the fidgety Ro...   \n",
       "3   [ \" <p> \" Do you know how much medicine you co...   \n",
       "4   [ <p> \" It is such an honor getting this award...   \n",
       "5   [ <p> \" By the end of May , a weak run has nev...   \n",
       "6   [ <p> Kate Crosbie , former director of perfor...   \n",
       "7   [ <p> \" Drawing on music-induced chills resear...   \n",
       "8   [ <p> Since first opening its doors in 1967 , ...   \n",
       "9   [ <p> \" Any instrument I see I pick it up and ...   \n",
       "10  [\\n@@70200540 <p> \" When the Budget 2019 is ta...   \n",
       "11  [ <p> Paul Manafort , President Trump 's forme...   \n",
       "\n",
       "                                     txtfiles_za_2018  \n",
       "0   [ \\n@@22515877 <h> Related Links <p> Durban - ...  \n",
       "1   [ <p> The research report said that its analys...  \n",
       "2   [ But Google began tuning it for businesses wh...  \n",
       "3   [ <p> EThekwini Mayor Zandile Gumede said the ...  \n",
       "4   [ <p> \" We decided to publish these internal g...  \n",
       "5   [ <p> Nedbank <p> Nedbank has said that they d...  \n",
       "6   [ - Fela Kuti **26;36;TOOLONG <p> \" The Shrine...  \n",
       "7   [ Bang on those doors until they fall , \" he s...  \n",
       "8   [ <p> \" Nevertheless , this still means that t...  \n",
       "9   [ <p> \" I do n't want to carp about things ? b...  \n",
       "10  [\\n@@30212638 <h> Julia Roberts ' too scared '...  \n",
       "11  [com/Prapan Ngawkeaw <p> The risk of having po...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2019</th>\n",
       "      <th>txtfiles_bd_2019</th>\n",
       "      <th>txtfiles_ca_2019</th>\n",
       "      <th>txtfiles_gb_2019</th>\n",
       "      <th>txtfiles_gh_2019</th>\n",
       "      <th>txtfiles_hk_2019</th>\n",
       "      <th>txtfiles_ie_2019</th>\n",
       "      <th>txtfiles_in_2019</th>\n",
       "      <th>txtfiles_jm_2019</th>\n",
       "      <th>txtfiles_ke_2019</th>\n",
       "      <th>txtfiles_lk_2019</th>\n",
       "      <th>txtfiles_my_2019</th>\n",
       "      <th>txtfiles_ng_2019</th>\n",
       "      <th>txtfiles_nz_2019</th>\n",
       "      <th>txtfiles_ph_2019</th>\n",
       "      <th>txtfiles_pk_2019</th>\n",
       "      <th>txtfiles_sg_2019</th>\n",
       "      <th>txtfiles_tz_2019</th>\n",
       "      <th>txtfiles_us_2019</th>\n",
       "      <th>txtfiles_za_2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ A SAPOL spokesperson said they had no inform...</td>\n",
       "      <td>[ &lt;p&gt; The two parts will be voted on separatel...</td>\n",
       "      <td>[ They can be reached at email protected ) \\n@...</td>\n",
       "      <td>[ &lt;p&gt; He attended the festival with a group of...</td>\n",
       "      <td>[ &lt;p&gt; Banks that require more than the limit o...</td>\n",
       "      <td>[ &lt;p&gt; Senior Inspector Jacky Chan , temporaril...</td>\n",
       "      <td>[ \\n@@60396449 &lt;p&gt; Mr Harris said , as 80% of ...</td>\n",
       "      <td>[ The Congress party will also go to Supreme C...</td>\n",
       "      <td>[ &lt;p&gt; Noting that the security measures - fuel...</td>\n",
       "      <td>[ &lt;p&gt; Send ' NEWS ' to 40227 to receive all th...</td>\n",
       "      <td>[ &lt;p&gt; Sri Lanka 's fastest-growing credit card...</td>\n",
       "      <td>[\\n@@40446179 &lt;h&gt; Trending Now &lt;h&gt; No agreemen...</td>\n",
       "      <td>[ &lt;p&gt; Mr Ajimobi , who listed other priority s...</td>\n",
       "      <td>[ \" &lt;p&gt; Guptill said Sri Lanka had a strong bo...</td>\n",
       "      <td>[ \" These are my dream fights , \" Nietes said ...</td>\n",
       "      <td>[ &lt;p&gt; That said , the man has displayed an unu...</td>\n",
       "      <td>[ &lt;p&gt; A police spokesman said the driver had b...</td>\n",
       "      <td>[ PHOTO FILE NMG &lt;h&gt; In Summary &lt;p&gt; The United...</td>\n",
       "      <td>[ &lt;p&gt; \" We are confident that you can trust Ta...</td>\n",
       "      <td>[ &lt;p&gt; SANRAL Eastern Region Regional Manager D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ \" Ms Power said there were exciting things h...</td>\n",
       "      <td>[ \" He was very good , but he got the role lar...</td>\n",
       "      <td>[ The BC Nurses ' Union said it is reviewing t...</td>\n",
       "      <td>[ It said the MOD has no \" coherent and credib...</td>\n",
       "      <td>[ &lt;p&gt; Speaking to some NDC supporters and dele...</td>\n",
       "      <td>[ &lt;p&gt; Frontier Services Group , which speciali...</td>\n",
       "      <td>[ &lt;p&gt; They said : \" Frost and ice will be fair...</td>\n",
       "      <td>[\\n@@30517243 &lt;p&gt; Srinagar : A bullet riddled ...</td>\n",
       "      <td>[ &lt;p&gt; \" I hold you responsible for anything th...</td>\n",
       "      <td>[ &lt;p&gt; Ringtone said that the two Gospel artist...</td>\n",
       "      <td>[ q! q! q! q! q! q! \\n@@40569948 &lt;h&gt; Attempts ...</td>\n",
       "      <td>[ &lt;p&gt; The sources who declined to be named als...</td>\n",
       "      <td>[ Abubakar Rasheed , has said universities in ...</td>\n",
       "      <td>[ &lt;p&gt; Mr Earle said while the Asian man appear...</td>\n",
       "      <td>[ Baseless statements like these cause serious...</td>\n",
       "      <td>[ &lt;p&gt; Brian Olsavsky , Amazon 's chief financi...</td>\n",
       "      <td>[ This , the FM said , will benefit nearly 3 c...</td>\n",
       "      <td>[ \" The kind of environment we are living toda...</td>\n",
       "      <td>[ If nothing else , a deal should prevent the ...</td>\n",
       "      <td>[ &lt;p&gt; \" I would say when it comes to dating , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ &lt;p&gt; Education Department CEO , Vicki Baylis ...</td>\n",
       "      <td>[ The retailer said Thursday that it 's creati...</td>\n",
       "      <td>[ &lt;p&gt; \" All their goals , we talked about it ,...</td>\n",
       "      <td>[ &lt;p&gt; She said : \" The news has come like a bo...</td>\n",
       "      <td>[\\n@@30620462 &lt;p&gt; The Bank of Ghana said it wa...</td>\n",
       "      <td>[ In the future they will invite their friends...</td>\n",
       "      <td>[ &lt;p&gt; \" It felt great to make my debut , \" he ...</td>\n",
       "      <td>[ &lt;p&gt; BJP president Amit Shah said Friday that...</td>\n",
       "      <td>[ Hutchinson said .,  The juice can be sent to...</td>\n",
       "      <td>[ The DP said that Kenya Airways supports the ...</td>\n",
       "      <td>[ &lt;p&gt; Those of you who know our guest of honor...</td>\n",
       "      <td>[ &lt;p&gt; In the application , the AG said Muhamma...</td>\n",
       "      <td>[ &lt;p&gt; The council said , \" We expect Abia INEC...</td>\n",
       "      <td>[ &lt;p&gt; \" It was just nice to bounce back after ...</td>\n",
       "      <td>[ &lt;p&gt; Yes to better health care for the people...</td>\n",
       "      <td>[in &lt;h&gt; The hilarious host said that if India ...</td>\n",
       "      <td>[ &lt;p&gt; \" Investors hope that Gill , working wit...</td>\n",
       "      <td>[ &lt;p&gt; At the Extraordinary Summit of the ( AU ...</td>\n",
       "      <td>[ \" Your cause of action may require a higher ...</td>\n",
       "      <td>[ &lt;p&gt; \" The system takes a lot of friction out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ &lt;p&gt; Australian Academy of Science Fellow and...</td>\n",
       "      <td>[ &lt;p&gt; College Principle Mobakkharul Islam said...</td>\n",
       "      <td>[ Korean embassy raid group preparing for even...</td>\n",
       "      <td>[ &lt;p&gt; In an update provided earlier this year ...</td>\n",
       "      <td>[\\n@@30726093 &lt;h&gt; Vietnam stepmom of accused K...</td>\n",
       "      <td>[ Billions are being given away as grants to s...</td>\n",
       "      <td>[ &lt;p&gt; \" He 's a good player , is n't he ? \" sa...</td>\n",
       "      <td>[ &lt;p&gt; South Korean soldiers will remove mines ...</td>\n",
       "      <td>[ &lt;p&gt; The mayor said the St James Municipal Co...</td>\n",
       "      <td>[ &lt;p&gt; \" We wanted to make education fun , \" sa...</td>\n",
       "      <td>[ &lt;p&gt; The Additional Magistrate has accordingl...</td>\n",
       "      <td>[ &lt;p&gt; The Ministry of Finance ( MoF ) said thi...</td>\n",
       "      <td>[ &lt;p&gt; \" What does the North Central has in the...</td>\n",
       "      <td>[ &lt;p&gt; \" As we reported previously , monitoring...</td>\n",
       "      <td>[757-trillion national budget for 2019 would b...</td>\n",
       "      <td>[ &lt;p&gt; \" It was grueling and the winds were tou...</td>\n",
       "      <td>[ &lt;p&gt; The biker , surnamed Qin , claimed he ha...</td>\n",
       "      <td>[ &lt;p&gt; \" This training is critically important ...</td>\n",
       "      <td>[ Saturday , causing 13 buildings on Notre Dam...</td>\n",
       "      <td>[\\n@@30726021 &lt;h&gt; WATCH : Meghan could shun ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ &lt;p&gt; \" They 've destroyed and re-done part of...</td>\n",
       "      <td>[ &lt;p&gt; \" We shall , to the extent that is compa...</td>\n",
       "      <td>[ &lt;p&gt; \" I think that 's more what we 're accus...</td>\n",
       "      <td>[ &lt;p&gt; That said , once you hit a cruising spee...</td>\n",
       "      <td>[ &lt;p&gt; While Chairman of the Committee , Ben Ab...</td>\n",
       "      <td>[ &lt;p&gt; Local police chief Candra Sukma Kumara s...</td>\n",
       "      <td>[ \\n@@60791967 &lt;h&gt; Get live updates from RT ? ...</td>\n",
       "      <td>[ ( Photo : ANI ) &lt;p&gt; As many as 27 vehicles b...</td>\n",
       "      <td>[ &lt;p&gt; The Opposition leader said the country s...</td>\n",
       "      <td>[ With that said , here 's what I learnt from ...</td>\n",
       "      <td>[ &lt;p&gt; Issuing a statement the organisation sai...</td>\n",
       "      <td>[\\n@@40902874 &lt;h&gt; Plunging Australia house pri...</td>\n",
       "      <td>[President Muhammadu Buhari and Sports Ministe...</td>\n",
       "      <td>[\\n@@70736331 &lt;p&gt; Citing a report it was hande...</td>\n",
       "      <td>[ raised the alert level upon the recommendati...</td>\n",
       "      <td>[ &lt;p&gt; It said the men have been tortured , hel...</td>\n",
       "      <td>[ &lt;p&gt; Introduced for the first time outside of...</td>\n",
       "      <td>[ &lt;p&gt; Mr Kalidushi said stakeholders have also...</td>\n",
       "      <td>[ &lt;p&gt; \" It 's been a fun time these past coupl...</td>\n",
       "      <td>[ \" &lt;p&gt; \" We 're also people and we 're fed up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ CA chief Kevin Roberts said earlier this yea...</td>\n",
       "      <td>[ &lt;p&gt; \" It 's a long story , but I hope you 'r...</td>\n",
       "      <td>[ One even said that it was okay if India did ...</td>\n",
       "      <td>[ &lt;p&gt; He said : \" I doubt that New York City s...</td>\n",
       "      <td>[ The future lies in the web \" he said .,  &lt;p&gt;...</td>\n",
       "      <td>[ &lt;p&gt; Four years later , the Chinese company c...</td>\n",
       "      <td>[ &lt;p&gt; She now feels better than ever after she...</td>\n",
       "      <td>[ \" I personally witnessed the results of the ...</td>\n",
       "      <td>[ &lt;p&gt; Chief Executive Officer of the NHF , Eve...</td>\n",
       "      <td>[ &lt;p&gt; \" Moving forward , I am directing the Mi...</td>\n",
       "      <td>[\\n@@41014696 &lt;p&gt; The Thero has said that he i...</td>\n",
       "      <td>[ &lt;p&gt; Deputy Transport Minister Datuk Kamarudd...</td>\n",
       "      <td>[ Like he said in one of his off -the- cuff sp...</td>\n",
       "      <td>[ Collecting data to be able to tell them is i...</td>\n",
       "      <td>[ &lt;p&gt; The startup said in an FAQ post on its F...</td>\n",
       "      <td>[ &lt;p&gt; The complainant Asif Shah said that land...</td>\n",
       "      <td>[ &lt;p&gt; Arcadia Group said in a statement on Gre...</td>\n",
       "      <td>[ &lt;p&gt; She said skilled labor force is importan...</td>\n",
       "      <td>[ They 'd pass me coming this way and then oth...</td>\n",
       "      <td>[9 billion ) out of emerging markets in May , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1 music festival victim was taken to hospital...</td>\n",
       "      <td>[ &lt;p&gt; Swift said she suffered years of \" manip...</td>\n",
       "      <td>[ &lt;p&gt; Shortly after learning that it would be ...</td>\n",
       "      <td>[ &lt;p&gt; She has performed at Cardiff 's Coyote U...</td>\n",
       "      <td>[ &lt;p&gt; The personnel said the quality of the T-...</td>\n",
       "      <td>[ &lt;p&gt; Organisers said that , as of 5.,  &lt;p&gt; Mr...</td>\n",
       "      <td>[ That 's how it is because for that they need...</td>\n",
       "      <td>[ &lt;p&gt; Her predecessor as number one Naomi Osak...</td>\n",
       "      <td>[ Shahine Robinson , said these include the op...</td>\n",
       "      <td>[ Kate said she put most of the earnings into ...</td>\n",
       "      <td>[ &lt;p&gt; Navy Spokesman , Lieutenant Commander Is...</td>\n",
       "      <td>[ &lt;p&gt; \" That means that we can explain the chu...</td>\n",
       "      <td>[ Dr Marvin Dekil , Project Coordinator of HYP...</td>\n",
       "      <td>[ &lt;p&gt; Under cross examination , Young said the...</td>\n",
       "      <td>[ The people who know me well can attest to th...</td>\n",
       "      <td>[ &lt;p&gt; The sources further said the prime minis...</td>\n",
       "      <td>[ &lt;p&gt; Senior Superintendent Kong Wing Cheung s...</td>\n",
       "      <td>[ He is a great player that would be really go...</td>\n",
       "      <td>[ He said they agreed that the military and ci...</td>\n",
       "      <td>[ &lt;p&gt; Most tweeps said when they tune in to a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ &lt;p&gt; \" Buddy could n't do it because he was i...</td>\n",
       "      <td>[ &lt;p&gt; DU Professor Emeritus Serajul Islam Chou...</td>\n",
       "      <td>[ &lt;p&gt; Barr said the Department of Justice is l...</td>\n",
       "      <td>[ ( Pictured : Sally Lane and John Letts ) &lt;p&gt;...</td>\n",
       "      <td>[ &lt;p&gt; Speaking at a press conference in Accra ...</td>\n",
       "      <td>[The authority said it noticed a widely circul...</td>\n",
       "      <td>[ \" &lt;p&gt; Relatives said Ford 's mental health d...</td>\n",
       "      <td>[ Seeing me , my friend said , \" Tumse ab na h...</td>\n",
       "      <td>[ Chang said the Ministry has prioritised inve...</td>\n",
       "      <td>[ &lt;p&gt; He said he realised it was time to chang...</td>\n",
       "      <td>[ According to an AFP report of May 9 , 2018 \"...</td>\n",
       "      <td>[ &lt;p&gt; Now , Prime Minister Tun Dr Mahathir Moh...</td>\n",
       "      <td>[\\n@@50965859 &lt;p&gt; The Edo State Government has...</td>\n",
       "      <td>[ However , council strategic policy manager S...</td>\n",
       "      <td>[ Guillermo Eleazar , director of the National...</td>\n",
       "      <td>[ &lt;p&gt; Khalid Shaikh , the PML-N 's provincial ...</td>\n",
       "      <td>[ Flores said .,  &lt;p&gt; Meanwhile , the Laguna P...</td>\n",
       "      <td>[ including the President of United States , I...</td>\n",
       "      <td>[ &lt;p&gt; The OPW said it intended to repair the g...</td>\n",
       "      <td>[ &lt;p&gt; \" The process of debt counselling has be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ &lt;p&gt; Back in Australia , Bowen said : ' ' We ...</td>\n",
       "      <td>[ &lt;p&gt; @ @ @ @ @ @ @ @ @ @ secure their connect...</td>\n",
       "      <td>[ \" &lt;p&gt; \" She felt that that she should receiv...</td>\n",
       "      <td>[ Just like I 've said in recent weeks , Leeds...</td>\n",
       "      <td>[ &lt;p&gt; Both suspects are considered to be dange...</td>\n",
       "      <td>[ &lt;p&gt; At the age of 18 , he graduated as the s...</td>\n",
       "      <td>[ &lt;p&gt; The Ministry of Agriculture is said to b...</td>\n",
       "      <td>[ Statistics Canada said the economy expanded ...</td>\n",
       "      <td>[ The study said myrcene , a terpene with an e...</td>\n",
       "      <td>[ NCA chief executive Maurice Aketch said the ...</td>\n",
       "      <td>[ &lt;p&gt; \" Families come first with our services ...</td>\n",
       "      <td>[ &lt;p&gt; \" This is a new government with a very e...</td>\n",
       "      <td>[ Now I am thinking of a project that will fit...</td>\n",
       "      <td>[ \" &lt;p&gt; It said despite many good examples of ...</td>\n",
       "      <td>[ She 's an apprentice to a prominent fashion ...</td>\n",
       "      <td>[ &lt;p&gt; Addressing the newsmen in Sialkot on Sun...</td>\n",
       "      <td>[ The developer said that 48 per cent of the u...</td>\n",
       "      <td>[ &lt;p&gt; He said the plan is due to government 's...</td>\n",
       "      <td>[ He 'll likely see a lot of single coverage t...</td>\n",
       "      <td>[ He is going to be part of that , \" said Luka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ &lt;p&gt; But if their hunch was right , then it c...</td>\n",
       "      <td>[ &lt;p&gt; \" It 's obvious that there 's a problem ...</td>\n",
       "      <td>[ They knocked on the door , got the tenant ou...</td>\n",
       "      <td>[L ) chairman Archie Norman has said he is unp...</td>\n",
       "      <td>[ &lt;p&gt; \" We are not going to break any more rec...</td>\n",
       "      <td>[ General Abdelaziz al-Fagham , who died in ho...</td>\n",
       "      <td>[ Meryl Halls said : \" We are delighted as eve...</td>\n",
       "      <td>[ He said he was allowed to circumnavigate the...</td>\n",
       "      <td>[ &lt;p&gt; \" This is eight years younger than the g...</td>\n",
       "      <td>[ &lt;p&gt; Through a video on the Daily Nation YouT...</td>\n",
       "      <td>[ Song said that distinguishing between the tw...</td>\n",
       "      <td>[\\n@@41375726 &lt;h&gt; Busy year ahead for builders...</td>\n",
       "      <td>[According to a statement by Ibrahim Aliyu , t...</td>\n",
       "      <td>[ &lt;p&gt; \" I think Kiwis respect someone who does...</td>\n",
       "      <td>[ NBA program involves boys and girls , parent...</td>\n",
       "      <td>[ \" &lt;p&gt; In a statement PBC said that the propo...</td>\n",
       "      <td>[ \" &lt;p&gt; On the reason behind the @ @ @ @ @ @ @...</td>\n",
       "      <td>[ &lt;p&gt; Speaking in Dar es Salaam on Saturday Ju...</td>\n",
       "      <td>[ He called the actor amazing and said that he...</td>\n",
       "      <td>[ It 's time for action , \" he said .,  &lt;p&gt; ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ &lt;p&gt; He said any White House chief of staff s...</td>\n",
       "      <td>[ File photo &lt;p&gt; UNB , Dhaka &lt;p&gt; BNP senior le...</td>\n",
       "      <td>[ It 's a quirky little show with a lot of hea...</td>\n",
       "      <td>[ &lt;p&gt; The letter , which starts of addressed t...</td>\n",
       "      <td>[ We have to fight back against this violence ...</td>\n",
       "      <td>[ &lt;p&gt; The owner of a bar in D'Aguilar Street s...</td>\n",
       "      <td>[ &lt;p&gt; Ladbrokes ' Nicola McGeady said : \" Ever...</td>\n",
       "      <td>[ &lt;p&gt; The police said that he has committed of...</td>\n",
       "      <td>[ &lt;p&gt; The minister said he expects the new agr...</td>\n",
       "      <td>[ &lt;p&gt; \" It is very important ( for them to tak...</td>\n",
       "      <td>[ &lt;p&gt; The guidelines were issued after the Sup...</td>\n",
       "      <td>[ The worst part is that there is currently no...</td>\n",
       "      <td>[ Nonsense ! FA : You said before that Moslems...</td>\n",
       "      <td>[ &lt;p&gt; The scientists were collectively \" impre...</td>\n",
       "      <td>[\\n@@71344668 &lt;h&gt; Kyrie Irving pushes back on ...</td>\n",
       "      <td>[ &lt;p&gt; On this exciting development , Muhammad ...</td>\n",
       "      <td>[\\n@@41453969 &lt;h&gt; AGE confirms exhibitor tally...</td>\n",
       "      <td>[ &lt;p&gt; Sebastian said she feels lucky to meet t...</td>\n",
       "      <td>[ &lt;p&gt; I 'm 64 and I 'd say most people in the ...</td>\n",
       "      <td>[\\n@@61342748 &lt;h&gt; South Africa braces for blea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[ &lt;p&gt; \" He said loads of stuff to me , Wlad wa...</td>\n",
       "      <td>[ Those who have entered Bangladesh in the pas...</td>\n",
       "      <td>[ &lt;p&gt; \" Obviously , they 're a good team , \" B...</td>\n",
       "      <td>[ Netflix said the show had more than 40 milli...</td>\n",
       "      <td>[ First , from my partner , who after three ye...</td>\n",
       "      <td>[ He said overly loose policy can harm long-te...</td>\n",
       "      <td>[ &lt;p&gt; Noel Moran said : \" We are very happy to...</td>\n",
       "      <td>[ Rawal said \" If we go back to the fundamenta...</td>\n",
       "      <td>[ They are coming to talk about their programm...</td>\n",
       "      <td>[ Last year , the European Union passed the Ge...</td>\n",
       "      <td>[ &lt;p&gt; The Central Bank had to commission a for...</td>\n",
       "      <td>[ &lt;p&gt; \" It 's not realistic to expect a miracu...</td>\n",
       "      <td>[ &lt;p&gt; Responding , the Publicity Secretary of ...</td>\n",
       "      <td>[ We give three advocate phone numbers to the ...</td>\n",
       "      <td>[ &lt;p&gt; PHOTO : AP &lt;p&gt; Continue reading below ? ...</td>\n",
       "      <td>[ \\n@@51235118 &lt;h&gt; JI chief blasts govt for po...</td>\n",
       "      <td>[ &lt;p&gt; Based in the university 's Institute of ...</td>\n",
       "      <td>[ &lt;p&gt; The WHO Director General Dr Tedros Ghebr...</td>\n",
       "      <td>[\\n@@31487700 &lt;h&gt; Sitting on the doorstep of t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     txtfiles_au_2019  \\\n",
       "0   [ A SAPOL spokesperson said they had no inform...   \n",
       "1   [ \" Ms Power said there were exciting things h...   \n",
       "2   [ <p> Education Department CEO , Vicki Baylis ...   \n",
       "3   [ <p> Australian Academy of Science Fellow and...   \n",
       "4   [ <p> \" They 've destroyed and re-done part of...   \n",
       "5   [ CA chief Kevin Roberts said earlier this yea...   \n",
       "6   [1 music festival victim was taken to hospital...   \n",
       "7   [ <p> \" Buddy could n't do it because he was i...   \n",
       "8   [ <p> Back in Australia , Bowen said : ' ' We ...   \n",
       "9   [ <p> But if their hunch was right , then it c...   \n",
       "10  [ <p> He said any White House chief of staff s...   \n",
       "11  [ <p> \" He said loads of stuff to me , Wlad wa...   \n",
       "\n",
       "                                     txtfiles_bd_2019  \\\n",
       "0   [ <p> The two parts will be voted on separatel...   \n",
       "1   [ \" He was very good , but he got the role lar...   \n",
       "2   [ The retailer said Thursday that it 's creati...   \n",
       "3   [ <p> College Principle Mobakkharul Islam said...   \n",
       "4   [ <p> \" We shall , to the extent that is compa...   \n",
       "5   [ <p> \" It 's a long story , but I hope you 'r...   \n",
       "6   [ <p> Swift said she suffered years of \" manip...   \n",
       "7   [ <p> DU Professor Emeritus Serajul Islam Chou...   \n",
       "8   [ <p> @ @ @ @ @ @ @ @ @ @ secure their connect...   \n",
       "9   [ <p> \" It 's obvious that there 's a problem ...   \n",
       "10  [ File photo <p> UNB , Dhaka <p> BNP senior le...   \n",
       "11  [ Those who have entered Bangladesh in the pas...   \n",
       "\n",
       "                                     txtfiles_ca_2019  \\\n",
       "0   [ They can be reached at email protected ) \\n@...   \n",
       "1   [ The BC Nurses ' Union said it is reviewing t...   \n",
       "2   [ <p> \" All their goals , we talked about it ,...   \n",
       "3   [ Korean embassy raid group preparing for even...   \n",
       "4   [ <p> \" I think that 's more what we 're accus...   \n",
       "5   [ One even said that it was okay if India did ...   \n",
       "6   [ <p> Shortly after learning that it would be ...   \n",
       "7   [ <p> Barr said the Department of Justice is l...   \n",
       "8   [ \" <p> \" She felt that that she should receiv...   \n",
       "9   [ They knocked on the door , got the tenant ou...   \n",
       "10  [ It 's a quirky little show with a lot of hea...   \n",
       "11  [ <p> \" Obviously , they 're a good team , \" B...   \n",
       "\n",
       "                                     txtfiles_gb_2019  \\\n",
       "0   [ <p> He attended the festival with a group of...   \n",
       "1   [ It said the MOD has no \" coherent and credib...   \n",
       "2   [ <p> She said : \" The news has come like a bo...   \n",
       "3   [ <p> In an update provided earlier this year ...   \n",
       "4   [ <p> That said , once you hit a cruising spee...   \n",
       "5   [ <p> He said : \" I doubt that New York City s...   \n",
       "6   [ <p> She has performed at Cardiff 's Coyote U...   \n",
       "7   [ ( Pictured : Sally Lane and John Letts ) <p>...   \n",
       "8   [ Just like I 've said in recent weeks , Leeds...   \n",
       "9   [L ) chairman Archie Norman has said he is unp...   \n",
       "10  [ <p> The letter , which starts of addressed t...   \n",
       "11  [ Netflix said the show had more than 40 milli...   \n",
       "\n",
       "                                     txtfiles_gh_2019  \\\n",
       "0   [ <p> Banks that require more than the limit o...   \n",
       "1   [ <p> Speaking to some NDC supporters and dele...   \n",
       "2   [\\n@@30620462 <p> The Bank of Ghana said it wa...   \n",
       "3   [\\n@@30726093 <h> Vietnam stepmom of accused K...   \n",
       "4   [ <p> While Chairman of the Committee , Ben Ab...   \n",
       "5   [ The future lies in the web \" he said .,  <p>...   \n",
       "6   [ <p> The personnel said the quality of the T-...   \n",
       "7   [ <p> Speaking at a press conference in Accra ...   \n",
       "8   [ <p> Both suspects are considered to be dange...   \n",
       "9   [ <p> \" We are not going to break any more rec...   \n",
       "10  [ We have to fight back against this violence ...   \n",
       "11  [ First , from my partner , who after three ye...   \n",
       "\n",
       "                                     txtfiles_hk_2019  \\\n",
       "0   [ <p> Senior Inspector Jacky Chan , temporaril...   \n",
       "1   [ <p> Frontier Services Group , which speciali...   \n",
       "2   [ In the future they will invite their friends...   \n",
       "3   [ Billions are being given away as grants to s...   \n",
       "4   [ <p> Local police chief Candra Sukma Kumara s...   \n",
       "5   [ <p> Four years later , the Chinese company c...   \n",
       "6   [ <p> Organisers said that , as of 5.,  <p> Mr...   \n",
       "7   [The authority said it noticed a widely circul...   \n",
       "8   [ <p> At the age of 18 , he graduated as the s...   \n",
       "9   [ General Abdelaziz al-Fagham , who died in ho...   \n",
       "10  [ <p> The owner of a bar in D'Aguilar Street s...   \n",
       "11  [ He said overly loose policy can harm long-te...   \n",
       "\n",
       "                                     txtfiles_ie_2019  \\\n",
       "0   [ \\n@@60396449 <p> Mr Harris said , as 80% of ...   \n",
       "1   [ <p> They said : \" Frost and ice will be fair...   \n",
       "2   [ <p> \" It felt great to make my debut , \" he ...   \n",
       "3   [ <p> \" He 's a good player , is n't he ? \" sa...   \n",
       "4   [ \\n@@60791967 <h> Get live updates from RT ? ...   \n",
       "5   [ <p> She now feels better than ever after she...   \n",
       "6   [ That 's how it is because for that they need...   \n",
       "7   [ \" <p> Relatives said Ford 's mental health d...   \n",
       "8   [ <p> The Ministry of Agriculture is said to b...   \n",
       "9   [ Meryl Halls said : \" We are delighted as eve...   \n",
       "10  [ <p> Ladbrokes ' Nicola McGeady said : \" Ever...   \n",
       "11  [ <p> Noel Moran said : \" We are very happy to...   \n",
       "\n",
       "                                     txtfiles_in_2019  \\\n",
       "0   [ The Congress party will also go to Supreme C...   \n",
       "1   [\\n@@30517243 <p> Srinagar : A bullet riddled ...   \n",
       "2   [ <p> BJP president Amit Shah said Friday that...   \n",
       "3   [ <p> South Korean soldiers will remove mines ...   \n",
       "4   [ ( Photo : ANI ) <p> As many as 27 vehicles b...   \n",
       "5   [ \" I personally witnessed the results of the ...   \n",
       "6   [ <p> Her predecessor as number one Naomi Osak...   \n",
       "7   [ Seeing me , my friend said , \" Tumse ab na h...   \n",
       "8   [ Statistics Canada said the economy expanded ...   \n",
       "9   [ He said he was allowed to circumnavigate the...   \n",
       "10  [ <p> The police said that he has committed of...   \n",
       "11  [ Rawal said \" If we go back to the fundamenta...   \n",
       "\n",
       "                                     txtfiles_jm_2019  \\\n",
       "0   [ <p> Noting that the security measures - fuel...   \n",
       "1   [ <p> \" I hold you responsible for anything th...   \n",
       "2   [ Hutchinson said .,  The juice can be sent to...   \n",
       "3   [ <p> The mayor said the St James Municipal Co...   \n",
       "4   [ <p> The Opposition leader said the country s...   \n",
       "5   [ <p> Chief Executive Officer of the NHF , Eve...   \n",
       "6   [ Shahine Robinson , said these include the op...   \n",
       "7   [ Chang said the Ministry has prioritised inve...   \n",
       "8   [ The study said myrcene , a terpene with an e...   \n",
       "9   [ <p> \" This is eight years younger than the g...   \n",
       "10  [ <p> The minister said he expects the new agr...   \n",
       "11  [ They are coming to talk about their programm...   \n",
       "\n",
       "                                     txtfiles_ke_2019  \\\n",
       "0   [ <p> Send ' NEWS ' to 40227 to receive all th...   \n",
       "1   [ <p> Ringtone said that the two Gospel artist...   \n",
       "2   [ The DP said that Kenya Airways supports the ...   \n",
       "3   [ <p> \" We wanted to make education fun , \" sa...   \n",
       "4   [ With that said , here 's what I learnt from ...   \n",
       "5   [ <p> \" Moving forward , I am directing the Mi...   \n",
       "6   [ Kate said she put most of the earnings into ...   \n",
       "7   [ <p> He said he realised it was time to chang...   \n",
       "8   [ NCA chief executive Maurice Aketch said the ...   \n",
       "9   [ <p> Through a video on the Daily Nation YouT...   \n",
       "10  [ <p> \" It is very important ( for them to tak...   \n",
       "11  [ Last year , the European Union passed the Ge...   \n",
       "\n",
       "                                     txtfiles_lk_2019  \\\n",
       "0   [ <p> Sri Lanka 's fastest-growing credit card...   \n",
       "1   [ q! q! q! q! q! q! \\n@@40569948 <h> Attempts ...   \n",
       "2   [ <p> Those of you who know our guest of honor...   \n",
       "3   [ <p> The Additional Magistrate has accordingl...   \n",
       "4   [ <p> Issuing a statement the organisation sai...   \n",
       "5   [\\n@@41014696 <p> The Thero has said that he i...   \n",
       "6   [ <p> Navy Spokesman , Lieutenant Commander Is...   \n",
       "7   [ According to an AFP report of May 9 , 2018 \"...   \n",
       "8   [ <p> \" Families come first with our services ...   \n",
       "9   [ Song said that distinguishing between the tw...   \n",
       "10  [ <p> The guidelines were issued after the Sup...   \n",
       "11  [ <p> The Central Bank had to commission a for...   \n",
       "\n",
       "                                     txtfiles_my_2019  \\\n",
       "0   [\\n@@40446179 <h> Trending Now <h> No agreemen...   \n",
       "1   [ <p> The sources who declined to be named als...   \n",
       "2   [ <p> In the application , the AG said Muhamma...   \n",
       "3   [ <p> The Ministry of Finance ( MoF ) said thi...   \n",
       "4   [\\n@@40902874 <h> Plunging Australia house pri...   \n",
       "5   [ <p> Deputy Transport Minister Datuk Kamarudd...   \n",
       "6   [ <p> \" That means that we can explain the chu...   \n",
       "7   [ <p> Now , Prime Minister Tun Dr Mahathir Moh...   \n",
       "8   [ <p> \" This is a new government with a very e...   \n",
       "9   [\\n@@41375726 <h> Busy year ahead for builders...   \n",
       "10  [ The worst part is that there is currently no...   \n",
       "11  [ <p> \" It 's not realistic to expect a miracu...   \n",
       "\n",
       "                                     txtfiles_ng_2019  \\\n",
       "0   [ <p> Mr Ajimobi , who listed other priority s...   \n",
       "1   [ Abubakar Rasheed , has said universities in ...   \n",
       "2   [ <p> The council said , \" We expect Abia INEC...   \n",
       "3   [ <p> \" What does the North Central has in the...   \n",
       "4   [President Muhammadu Buhari and Sports Ministe...   \n",
       "5   [ Like he said in one of his off -the- cuff sp...   \n",
       "6   [ Dr Marvin Dekil , Project Coordinator of HYP...   \n",
       "7   [\\n@@50965859 <p> The Edo State Government has...   \n",
       "8   [ Now I am thinking of a project that will fit...   \n",
       "9   [According to a statement by Ibrahim Aliyu , t...   \n",
       "10  [ Nonsense ! FA : You said before that Moslems...   \n",
       "11  [ <p> Responding , the Publicity Secretary of ...   \n",
       "\n",
       "                                     txtfiles_nz_2019  \\\n",
       "0   [ \" <p> Guptill said Sri Lanka had a strong bo...   \n",
       "1   [ <p> Mr Earle said while the Asian man appear...   \n",
       "2   [ <p> \" It was just nice to bounce back after ...   \n",
       "3   [ <p> \" As we reported previously , monitoring...   \n",
       "4   [\\n@@70736331 <p> Citing a report it was hande...   \n",
       "5   [ Collecting data to be able to tell them is i...   \n",
       "6   [ <p> Under cross examination , Young said the...   \n",
       "7   [ However , council strategic policy manager S...   \n",
       "8   [ \" <p> It said despite many good examples of ...   \n",
       "9   [ <p> \" I think Kiwis respect someone who does...   \n",
       "10  [ <p> The scientists were collectively \" impre...   \n",
       "11  [ We give three advocate phone numbers to the ...   \n",
       "\n",
       "                                     txtfiles_ph_2019  \\\n",
       "0   [ \" These are my dream fights , \" Nietes said ...   \n",
       "1   [ Baseless statements like these cause serious...   \n",
       "2   [ <p> Yes to better health care for the people...   \n",
       "3   [757-trillion national budget for 2019 would b...   \n",
       "4   [ raised the alert level upon the recommendati...   \n",
       "5   [ <p> The startup said in an FAQ post on its F...   \n",
       "6   [ The people who know me well can attest to th...   \n",
       "7   [ Guillermo Eleazar , director of the National...   \n",
       "8   [ She 's an apprentice to a prominent fashion ...   \n",
       "9   [ NBA program involves boys and girls , parent...   \n",
       "10  [\\n@@71344668 <h> Kyrie Irving pushes back on ...   \n",
       "11  [ <p> PHOTO : AP <p> Continue reading below ? ...   \n",
       "\n",
       "                                     txtfiles_pk_2019  \\\n",
       "0   [ <p> That said , the man has displayed an unu...   \n",
       "1   [ <p> Brian Olsavsky , Amazon 's chief financi...   \n",
       "2   [in <h> The hilarious host said that if India ...   \n",
       "3   [ <p> \" It was grueling and the winds were tou...   \n",
       "4   [ <p> It said the men have been tortured , hel...   \n",
       "5   [ <p> The complainant Asif Shah said that land...   \n",
       "6   [ <p> The sources further said the prime minis...   \n",
       "7   [ <p> Khalid Shaikh , the PML-N 's provincial ...   \n",
       "8   [ <p> Addressing the newsmen in Sialkot on Sun...   \n",
       "9   [ \" <p> In a statement PBC said that the propo...   \n",
       "10  [ <p> On this exciting development , Muhammad ...   \n",
       "11  [ \\n@@51235118 <h> JI chief blasts govt for po...   \n",
       "\n",
       "                                     txtfiles_sg_2019  \\\n",
       "0   [ <p> A police spokesman said the driver had b...   \n",
       "1   [ This , the FM said , will benefit nearly 3 c...   \n",
       "2   [ <p> \" Investors hope that Gill , working wit...   \n",
       "3   [ <p> The biker , surnamed Qin , claimed he ha...   \n",
       "4   [ <p> Introduced for the first time outside of...   \n",
       "5   [ <p> Arcadia Group said in a statement on Gre...   \n",
       "6   [ <p> Senior Superintendent Kong Wing Cheung s...   \n",
       "7   [ Flores said .,  <p> Meanwhile , the Laguna P...   \n",
       "8   [ The developer said that 48 per cent of the u...   \n",
       "9   [ \" <p> On the reason behind the @ @ @ @ @ @ @...   \n",
       "10  [\\n@@41453969 <h> AGE confirms exhibitor tally...   \n",
       "11  [ <p> Based in the university 's Institute of ...   \n",
       "\n",
       "                                     txtfiles_tz_2019  \\\n",
       "0   [ PHOTO FILE NMG <h> In Summary <p> The United...   \n",
       "1   [ \" The kind of environment we are living toda...   \n",
       "2   [ <p> At the Extraordinary Summit of the ( AU ...   \n",
       "3   [ <p> \" This training is critically important ...   \n",
       "4   [ <p> Mr Kalidushi said stakeholders have also...   \n",
       "5   [ <p> She said skilled labor force is importan...   \n",
       "6   [ He is a great player that would be really go...   \n",
       "7   [ including the President of United States , I...   \n",
       "8   [ <p> He said the plan is due to government 's...   \n",
       "9   [ <p> Speaking in Dar es Salaam on Saturday Ju...   \n",
       "10  [ <p> Sebastian said she feels lucky to meet t...   \n",
       "11  [ <p> The WHO Director General Dr Tedros Ghebr...   \n",
       "\n",
       "                                     txtfiles_us_2019  \\\n",
       "0   [ <p> \" We are confident that you can trust Ta...   \n",
       "1   [ If nothing else , a deal should prevent the ...   \n",
       "2   [ \" Your cause of action may require a higher ...   \n",
       "3   [ Saturday , causing 13 buildings on Notre Dam...   \n",
       "4   [ <p> \" It 's been a fun time these past coupl...   \n",
       "5   [ They 'd pass me coming this way and then oth...   \n",
       "6   [ He said they agreed that the military and ci...   \n",
       "7   [ <p> The OPW said it intended to repair the g...   \n",
       "8   [ He 'll likely see a lot of single coverage t...   \n",
       "9   [ He called the actor amazing and said that he...   \n",
       "10  [ <p> I 'm 64 and I 'd say most people in the ...   \n",
       "11  [\\n@@31487700 <h> Sitting on the doorstep of t...   \n",
       "\n",
       "                                     txtfiles_za_2019  \n",
       "0   [ <p> SANRAL Eastern Region Regional Manager D...  \n",
       "1   [ <p> \" I would say when it comes to dating , ...  \n",
       "2   [ <p> \" The system takes a lot of friction out...  \n",
       "3   [\\n@@30726021 <h> WATCH : Meghan could shun ro...  \n",
       "4   [ \" <p> \" We 're also people and we 're fed up...  \n",
       "5   [9 billion ) out of emerging markets in May , ...  \n",
       "6   [ <p> Most tweeps said when they tune in to a ...  \n",
       "7   [ <p> \" The process of debt counselling has be...  \n",
       "8   [ He is going to be part of that , \" said Luka...  \n",
       "9   [ It 's time for action , \" he said .,  <p> ' ...  \n",
       "10  [\\n@@61342748 <h> South Africa braces for blea...  \n",
       "11                                                NaN  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2020</th>\n",
       "      <th>txtfiles_bd_2020</th>\n",
       "      <th>txtfiles_ca_2020</th>\n",
       "      <th>txtfiles_gb_2020</th>\n",
       "      <th>txtfiles_gh_2020</th>\n",
       "      <th>txtfiles_hk_2020</th>\n",
       "      <th>txtfiles_ie_2020</th>\n",
       "      <th>txtfiles_in_2020</th>\n",
       "      <th>txtfiles_jm_2020</th>\n",
       "      <th>txtfiles_ke_2020</th>\n",
       "      <th>txtfiles_lk_2020</th>\n",
       "      <th>txtfiles_my_2020</th>\n",
       "      <th>txtfiles_ng_2020</th>\n",
       "      <th>txtfiles_nz_2020</th>\n",
       "      <th>txtfiles_ph_2020</th>\n",
       "      <th>txtfiles_pk_2020</th>\n",
       "      <th>txtfiles_sg_2020</th>\n",
       "      <th>txtfiles_tz_2020</th>\n",
       "      <th>txtfiles_us_2020</th>\n",
       "      <th>txtfiles_za_2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ &lt;p&gt; However , the 19-year-old , who has not ...</td>\n",
       "      <td>[ &lt;p&gt; Addressing a briefing at DNC headquarter...</td>\n",
       "      <td>[ \" It 's a scam , \" Brown said .,  Here 's wh...</td>\n",
       "      <td>[ &lt;p&gt; A New Year memorial service was held for...</td>\n",
       "      <td>[ &lt;p&gt; \" You have to see how the standoff over ...</td>\n",
       "      <td>[ &lt;p&gt; \" There is every potential that the @ @ ...</td>\n",
       "      <td>[ The US State Department put the figure at 10...</td>\n",
       "      <td>[ Among the top 7 cities , MMR saw maximum new...</td>\n",
       "      <td>[ &lt;p&gt; \" Papine has been an area that has been ...</td>\n",
       "      <td>[ &lt;p&gt; In an advisory directed to civilian airl...</td>\n",
       "      <td>[ He He said said that that at at present pres...</td>\n",
       "      <td>[\\n@@41612099 &lt;p&gt; \" I think this club , in the...</td>\n",
       "      <td>[ Taking our services to Kenya represents the ...</td>\n",
       "      <td>[ I I just just said said no no .,  &lt;p&gt; \" They...</td>\n",
       "      <td>[ &lt;p&gt; \" She 's always smiling and very profess...</td>\n",
       "      <td>[ &lt;p&gt; Wajid looked at me and said , \" Governor...</td>\n",
       "      <td>[ &lt;p&gt; Von der Leyen said both sides needed to ...</td>\n",
       "      <td>[\\n@@61494772 &lt;p&gt; A wildlife veterinary expert...</td>\n",
       "      <td>[ And it was Justice Lewis Powell who said app...</td>\n",
       "      <td>[ &lt;p&gt; He said he would be taking a break from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ ( YouTube ) &lt;p&gt; In the video he said Marano ...</td>\n",
       "      <td>[ People in Bangladesh continued to be harasse...</td>\n",
       "      <td>[ &lt;p&gt; The commander of Canada 's Pacific fleet...</td>\n",
       "      <td>[ \" &lt;p&gt; The anti-hate group said it met with T...</td>\n",
       "      <td>[ If you print money you are going to cause in...</td>\n",
       "      <td>[ Li was said to be responsible for investigat...</td>\n",
       "      <td>[ &lt;p&gt; Patrick Kanner also said that the French...</td>\n",
       "      <td>[ &lt;p&gt; When you go back to the day this inciden...</td>\n",
       "      <td>[ &lt;p&gt; Business Development and Communication S...</td>\n",
       "      <td>[ &lt;p&gt; The area MP Ndindi Nyoro said area Natio...</td>\n",
       "      <td>[ &lt;p&gt; Some of the Ministers present had then s...</td>\n",
       "      <td>[ He said he wants to bring the traditional in...</td>\n",
       "      <td>[ &lt;p&gt; Italy had said Thursday it was stopping ...</td>\n",
       "      <td>[ &lt;p&gt; The Report said that India is progressin...</td>\n",
       "      <td>[ But having said that , you ca n't really go ...</td>\n",
       "      <td>[ &lt;p&gt; He said that the modernisation of rail l...</td>\n",
       "      <td>[ &lt;p&gt; Senior members of the royal family were ...</td>\n",
       "      <td>[ When they get damaged , we spend more time t...</td>\n",
       "      <td>[ &lt;p&gt; \" Inbreeding has always been high in tho...</td>\n",
       "      <td>[ You must remember that he has parted ways wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ &lt;p&gt; Outside of China , the department said t...</td>\n",
       "      <td>[ &lt;p&gt; \" These are the skills you will learn wh...</td>\n",
       "      <td>[ climate summit in Glasgow in November , camp...</td>\n",
       "      <td>[ Read more : \" Burned by broken promises agai...</td>\n",
       "      <td>[ \" &lt;p&gt; In January 2019 , Steyer said he did n...</td>\n",
       "      <td>[ &lt;p&gt; Jesus said , \" I am the Way , the Truth ...</td>\n",
       "      <td>[ &lt;p&gt; According to The Metro , John Owens , a ...</td>\n",
       "      <td>[ \" On the artificial intelligence front , I h...</td>\n",
       "      <td>[ &lt;p&gt; \" It prevents undervaluation of vehicle ...</td>\n",
       "      <td>[ &lt;p&gt; Sodapoppin said , \" I actually think I '...</td>\n",
       "      <td>[ &lt;p&gt; He said all members @ @ @ @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ On Saturday morning , he said he had the num...</td>\n",
       "      <td>[ \" &lt;p&gt; Jaja said the BPE had gone to town wit...</td>\n",
       "      <td>[ \\n@@31678143 &lt;h&gt; Moments of grief , solidari...</td>\n",
       "      <td>[ But a day later , his ego said something dif...</td>\n",
       "      <td>[ The accord would have positive effects for P...</td>\n",
       "      <td>[ &lt;p&gt; He testified that he could have \" possib...</td>\n",
       "      <td>[ \" Defectors will play no significant role in...</td>\n",
       "      <td>[ You get rest when you can , \" he said .,  &lt;p...</td>\n",
       "      <td>[ &lt;p&gt; \" The NHI will ensure that we have suffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ Yup , that 's what they said .,  They said e...</td>\n",
       "      <td>[ Stay home , maintain cleanliness and social ...</td>\n",
       "      <td>[ Decades from now , how will they look back o...</td>\n",
       "      <td>[ &lt;p&gt; Lucy said : \" The thing for us is , it '...</td>\n",
       "      <td>[ Kojo Oppong Nkrumah said : \" the platform wi...</td>\n",
       "      <td>[ &lt;p&gt; A home health nurse who is part of the O...</td>\n",
       "      <td>[ &lt;p&gt; \" I would like to congratulate Shane Kin...</td>\n",
       "      <td>[ &lt;p&gt; S&amp;amp;P Global Ratings said the coronoav...</td>\n",
       "      <td>[ Tufton said .,  Hutchinson , said there are ...</td>\n",
       "      <td>[ Can you imagine waiting 30 years and then ha...</td>\n",
       "      <td>[ Deepika Udagama said in a letter sent to Pre...</td>\n",
       "      <td>[ &lt;p&gt; CFM chairman Mohana Mohariff said this i...</td>\n",
       "      <td>[\\n@@51583650 &lt;h&gt; Bayelsa Govt resolves to com...</td>\n",
       "      <td>[ RBNZ Governor Adrian Orr said that the new T...</td>\n",
       "      <td>[\\n@@71836290 &lt;p&gt; \" At this point , what is mo...</td>\n",
       "      <td>[ \\n@@84460757 &lt;h&gt; PTI govt launches ' COVID-1...</td>\n",
       "      <td>[ Being largely an online tournament , we have...</td>\n",
       "      <td>[ Can you imagine waiting 30 years and then ha...</td>\n",
       "      <td>[ Contemplating his hard-gained success , Hoga...</td>\n",
       "      <td>[ &lt;p&gt; He said while they will attempt to keep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ all we need is a small window to travel back...</td>\n",
       "      <td>[ &lt;p&gt; Acting on a tip off , a team of Rab-15 c...</td>\n",
       "      <td>[ The panel 's work will likely last through J...</td>\n",
       "      <td>[ &lt;p&gt; She said : \" I am pleading to anyone who...</td>\n",
       "      <td>[ &lt;p&gt; Delivering the ruling after a petition f...</td>\n",
       "      <td>[ &lt;p&gt; \" It 's good to be a storytelling sister...</td>\n",
       "      <td>[ &lt;p&gt; Garda Ryan Hill said groceries worth ? 4...</td>\n",
       "      <td>[ \" So all are requested to stay a bit more al...</td>\n",
       "      <td>[ &lt;p&gt; Meanwhile 141 fewer people were being tr...</td>\n",
       "      <td>[ &lt;p&gt; \" Kieran Trippier has been charged with ...</td>\n",
       "      <td>[ The Police Media Unit said that the former i...</td>\n",
       "      <td>[ &lt;p&gt; \" It never left the flag , \" Spieth said...</td>\n",
       "      <td>[\\n@@51653813 &lt;h&gt; Federal government to engage...</td>\n",
       "      <td>[ &lt;p&gt; Environment Southland air quality scient...</td>\n",
       "      <td>[ The CHP 's investigation is ongoing , \" a sp...</td>\n",
       "      <td>[ &lt;p&gt; Talking to journalists , APTTFA chairman...</td>\n",
       "      <td>[ &lt;p&gt; The Times reported that the owner , Andr...</td>\n",
       "      <td>[Mr Mbowe , whose speech was broadcast through...</td>\n",
       "      <td>[ &lt;h&gt; \" It 's like waking up Christmas morning...</td>\n",
       "      <td>[ &lt;p&gt; Responding to questions during a joint v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ Picture : SuppliedSource:Supplied &lt;p&gt; Coles ...</td>\n",
       "      <td>[ &lt;p&gt; In a condolence message , the Libyan Min...</td>\n",
       "      <td>[ But an equally strong fear of fitna , or pub...</td>\n",
       "      <td>[ \" I had fallen out with the manager at Birmi...</td>\n",
       "      <td>[ &lt;p&gt; \" I 'm thinking about how to keep the te...</td>\n",
       "      <td>[ &lt;p&gt; The statement said the university chiefs...</td>\n",
       "      <td>[ &lt;p&gt; \" They are two quality additions , \" sai...</td>\n",
       "      <td>[ &lt;p&gt; Sundar Pichai , the Indian-American CEO ...</td>\n",
       "      <td>[ The official report said Bogle was killed du...</td>\n",
       "      <td>[ &lt;p&gt; \" Mr Yego has been chasing us like child...</td>\n",
       "      <td>[hits &lt;p&gt; NEW DELHI ( Reuters ) : India is in ...</td>\n",
       "      <td>[\\n@@42021034 &lt;h&gt; Deportation of illegal immig...</td>\n",
       "      <td>[4 million ) of its loans to individuals and b...</td>\n",
       "      <td>[ &lt;p&gt; \" The protocol has been broken , but it ...</td>\n",
       "      <td>[ In mid-April , Riot said it would n't immedi...</td>\n",
       "      <td>[ The circular said the officers should call t...</td>\n",
       "      <td>[ \" When quiet diplomacy proves futile , the U...</td>\n",
       "      <td>[ \\n@@61900942 &lt;p&gt; Ezekiel Mbilinyi , Mbogwe D...</td>\n",
       "      <td>[ &lt;p&gt; Police Superintendent David Brown said t...</td>\n",
       "      <td>[ \\n@@61900952 &lt;h&gt; Newsdeck &lt;h&gt; New York 's Cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ &lt;p&gt; \" How was this a battle because it felt ...</td>\n",
       "      <td>[ Photo : Collected &lt;p&gt; Talking to reporters ,...</td>\n",
       "      <td>[ &lt;p&gt; \" We are pleased to have arrived at an a...</td>\n",
       "      <td>[ ' &lt;p&gt; Ruby revealed that her son was allowed...</td>\n",
       "      <td>[ &lt;p&gt; \" I 'm the CEO on that team and that is ...</td>\n",
       "      <td>[ &lt;p&gt; They said some opinion leaders had gone ...</td>\n",
       "      <td>[ &lt;p&gt; In a statement issued yesterday afternoo...</td>\n",
       "      <td>[ &lt;p&gt; Flipkart group CEO , Kalyan Krishnamurth...</td>\n",
       "      <td>[ &lt;p&gt; Minister Williams said the ST&amp;amp;I Poli...</td>\n",
       "      <td>[ &lt;p&gt; Since the first case of the coronavirus ...</td>\n",
       "      <td>[ &lt;p&gt; The Commercial Bank said the proceeds of...</td>\n",
       "      <td>[ &lt;p&gt; The yen firmed slightly on safe-haven fl...</td>\n",
       "      <td>[ &lt;p&gt; \" There is the need to review a lot of t...</td>\n",
       "      <td>[ Salespeople Mark Slade , Mark Walton and Bre...</td>\n",
       "      <td>[ &lt;p&gt; \" Given the nature and profile of our pl...</td>\n",
       "      <td>[ &lt;p&gt; A large number of people from Tehkal as ...</td>\n",
       "      <td>[ \" &lt;p&gt; As part of a series of statements issu...</td>\n",
       "      <td>[ &lt;p&gt; Commenting on this year 's shortlist , M...</td>\n",
       "      <td>[ &lt;p&gt; \" Part of my ideas come more from my Jew...</td>\n",
       "      <td>[ Picture : Adel Hana/AP &lt;h&gt; Palestinians decl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ \\n@@72131068 &lt;p&gt; \" In 2018 the NSW governmen...</td>\n",
       "      <td>[\\n@@51858471 &lt;h&gt; Share true history of 1971 a...</td>\n",
       "      <td>[ She was out there for me as a support to lea...</td>\n",
       "      <td>[\\n@@85555746 &lt;h&gt; ' He nailed himself to the p...</td>\n",
       "      <td>[ \\n@@31989297 &lt;h&gt; Cairo : Torino are not for ...</td>\n",
       "      <td>[ And we 're making some progress on certain i...</td>\n",
       "      <td>[ &lt;p&gt; Substitute Conor Coleman was one of two ...</td>\n",
       "      <td>[ ( Express Photo by Shashi Ghosh ) &lt;p&gt; 16 / 1...</td>\n",
       "      <td>[ &lt;p&gt; \" It is also critical at this time that ...</td>\n",
       "      <td>[ &lt;p&gt; Daisy said her chickens were gaping .,  ...</td>\n",
       "      <td>[ &lt;p&gt; \" It is with pleasure that I offer my fe...</td>\n",
       "      <td>[\\n@@42162298 &lt;p&gt; However , he said in some ar...</td>\n",
       "      <td>[\\n@@51859327 &lt;p&gt; President Muhammadu Buhari h...</td>\n",
       "      <td>[\\n@@31989266 &lt;p&gt; She said she had complied wi...</td>\n",
       "      <td>[ &lt;p&gt; \" The CPL games are quite different , th...</td>\n",
       "      <td>[ Arif Alvi on Thursday said Pakistan 's succe...</td>\n",
       "      <td>[ ) That said , there 's no gore -- and overal...</td>\n",
       "      <td>[ Elephant poaching has gone down by 80 percen...</td>\n",
       "      <td>[ &lt;p&gt; \" This term is n't going to end , \" said...</td>\n",
       "      <td>[ &lt;p&gt; Lee died Thursday evening at Taipei Vete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ as I said the last couple of weeks I have en...</td>\n",
       "      <td>[ &lt;p&gt; But with England white-ball captain Eoin...</td>\n",
       "      <td>[ &lt;p&gt; @ @ @ @ @ @ @ @ @ @ leadership on the Ri...</td>\n",
       "      <td>[\\n@@42239141 &lt;h&gt; Donald Trump says the White ...</td>\n",
       "      <td>[ \\n@@32053391 &lt;h&gt; Some private schools likely...</td>\n",
       "      <td>[ He also said \" this is a good place \" when r...</td>\n",
       "      <td>[ &lt;p&gt; Detective Garda Eoin Kane said that Ms D...</td>\n",
       "      <td>[ &lt;p&gt; \" The details are required to be furnish...</td>\n",
       "      <td>[ That said.,  Congratulations , and may you e...</td>\n",
       "      <td>[ AFP &lt;p&gt; Donald Trump has no plans to meet wi...</td>\n",
       "      <td>[ &lt;p&gt; When the meeting concluded and the EPDP ...</td>\n",
       "      <td>[ &lt;p&gt; AMMB 's group chief financial officer Ja...</td>\n",
       "      <td>[ In such cases , rape would be said to have o...</td>\n",
       "      <td>[ \" &lt;p&gt; Craig Knight , Co-Founder of HYZON Mot...</td>\n",
       "      <td>[ &lt;p&gt; Miasco , in an interview with Superbalit...</td>\n",
       "      <td>[ &lt;p&gt; It said that loss in transport by hiring...</td>\n",
       "      <td>[ &lt;p&gt; The company said that the move is aimed ...</td>\n",
       "      <td>[ \\n@@62098492 &lt;h&gt; Tanzanian travellers scared...</td>\n",
       "      <td>[ The tweet said the remaining 94% had \" other...</td>\n",
       "      <td>[\\n@@62098458 &lt;h&gt; The perpetrators of GBV are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    txtfiles_au_2020  \\\n",
       "0  [ <p> However , the 19-year-old , who has not ...   \n",
       "1  [ ( YouTube ) <p> In the video he said Marano ...   \n",
       "2  [ <p> Outside of China , the department said t...   \n",
       "3  [ Yup , that 's what they said .,  They said e...   \n",
       "4  [ all we need is a small window to travel back...   \n",
       "5  [ Picture : SuppliedSource:Supplied <p> Coles ...   \n",
       "6  [ <p> \" How was this a battle because it felt ...   \n",
       "7  [ \\n@@72131068 <p> \" In 2018 the NSW governmen...   \n",
       "8  [ as I said the last couple of weeks I have en...   \n",
       "\n",
       "                                    txtfiles_bd_2020  \\\n",
       "0  [ <p> Addressing a briefing at DNC headquarter...   \n",
       "1  [ People in Bangladesh continued to be harasse...   \n",
       "2  [ <p> \" These are the skills you will learn wh...   \n",
       "3  [ Stay home , maintain cleanliness and social ...   \n",
       "4  [ <p> Acting on a tip off , a team of Rab-15 c...   \n",
       "5  [ <p> In a condolence message , the Libyan Min...   \n",
       "6  [ Photo : Collected <p> Talking to reporters ,...   \n",
       "7  [\\n@@51858471 <h> Share true history of 1971 a...   \n",
       "8  [ <p> But with England white-ball captain Eoin...   \n",
       "\n",
       "                                    txtfiles_ca_2020  \\\n",
       "0  [ \" It 's a scam , \" Brown said .,  Here 's wh...   \n",
       "1  [ <p> The commander of Canada 's Pacific fleet...   \n",
       "2  [ climate summit in Glasgow in November , camp...   \n",
       "3  [ Decades from now , how will they look back o...   \n",
       "4  [ The panel 's work will likely last through J...   \n",
       "5  [ But an equally strong fear of fitna , or pub...   \n",
       "6  [ <p> \" We are pleased to have arrived at an a...   \n",
       "7  [ She was out there for me as a support to lea...   \n",
       "8  [ <p> @ @ @ @ @ @ @ @ @ @ leadership on the Ri...   \n",
       "\n",
       "                                    txtfiles_gb_2020  \\\n",
       "0  [ <p> A New Year memorial service was held for...   \n",
       "1  [ \" <p> The anti-hate group said it met with T...   \n",
       "2  [ Read more : \" Burned by broken promises agai...   \n",
       "3  [ <p> Lucy said : \" The thing for us is , it '...   \n",
       "4  [ <p> She said : \" I am pleading to anyone who...   \n",
       "5  [ \" I had fallen out with the manager at Birmi...   \n",
       "6  [ ' <p> Ruby revealed that her son was allowed...   \n",
       "7  [\\n@@85555746 <h> ' He nailed himself to the p...   \n",
       "8  [\\n@@42239141 <h> Donald Trump says the White ...   \n",
       "\n",
       "                                    txtfiles_gh_2020  \\\n",
       "0  [ <p> \" You have to see how the standoff over ...   \n",
       "1  [ If you print money you are going to cause in...   \n",
       "2  [ \" <p> In January 2019 , Steyer said he did n...   \n",
       "3  [ Kojo Oppong Nkrumah said : \" the platform wi...   \n",
       "4  [ <p> Delivering the ruling after a petition f...   \n",
       "5  [ <p> \" I 'm thinking about how to keep the te...   \n",
       "6  [ <p> \" I 'm the CEO on that team and that is ...   \n",
       "7  [ \\n@@31989297 <h> Cairo : Torino are not for ...   \n",
       "8  [ \\n@@32053391 <h> Some private schools likely...   \n",
       "\n",
       "                                    txtfiles_hk_2020  \\\n",
       "0  [ <p> \" There is every potential that the @ @ ...   \n",
       "1  [ Li was said to be responsible for investigat...   \n",
       "2  [ <p> Jesus said , \" I am the Way , the Truth ...   \n",
       "3  [ <p> A home health nurse who is part of the O...   \n",
       "4  [ <p> \" It 's good to be a storytelling sister...   \n",
       "5  [ <p> The statement said the university chiefs...   \n",
       "6  [ <p> They said some opinion leaders had gone ...   \n",
       "7  [ And we 're making some progress on certain i...   \n",
       "8  [ He also said \" this is a good place \" when r...   \n",
       "\n",
       "                                    txtfiles_ie_2020  \\\n",
       "0  [ The US State Department put the figure at 10...   \n",
       "1  [ <p> Patrick Kanner also said that the French...   \n",
       "2  [ <p> According to The Metro , John Owens , a ...   \n",
       "3  [ <p> \" I would like to congratulate Shane Kin...   \n",
       "4  [ <p> Garda Ryan Hill said groceries worth ? 4...   \n",
       "5  [ <p> \" They are two quality additions , \" sai...   \n",
       "6  [ <p> In a statement issued yesterday afternoo...   \n",
       "7  [ <p> Substitute Conor Coleman was one of two ...   \n",
       "8  [ <p> Detective Garda Eoin Kane said that Ms D...   \n",
       "\n",
       "                                    txtfiles_in_2020  \\\n",
       "0  [ Among the top 7 cities , MMR saw maximum new...   \n",
       "1  [ <p> When you go back to the day this inciden...   \n",
       "2  [ \" On the artificial intelligence front , I h...   \n",
       "3  [ <p> S&amp;P Global Ratings said the coronoav...   \n",
       "4  [ \" So all are requested to stay a bit more al...   \n",
       "5  [ <p> Sundar Pichai , the Indian-American CEO ...   \n",
       "6  [ <p> Flipkart group CEO , Kalyan Krishnamurth...   \n",
       "7  [ ( Express Photo by Shashi Ghosh ) <p> 16 / 1...   \n",
       "8  [ <p> \" The details are required to be furnish...   \n",
       "\n",
       "                                    txtfiles_jm_2020  \\\n",
       "0  [ <p> \" Papine has been an area that has been ...   \n",
       "1  [ <p> Business Development and Communication S...   \n",
       "2  [ <p> \" It prevents undervaluation of vehicle ...   \n",
       "3  [ Tufton said .,  Hutchinson , said there are ...   \n",
       "4  [ <p> Meanwhile 141 fewer people were being tr...   \n",
       "5  [ The official report said Bogle was killed du...   \n",
       "6  [ <p> Minister Williams said the ST&amp;I Poli...   \n",
       "7  [ <p> \" It is also critical at this time that ...   \n",
       "8  [ That said.,  Congratulations , and may you e...   \n",
       "\n",
       "                                    txtfiles_ke_2020  \\\n",
       "0  [ <p> In an advisory directed to civilian airl...   \n",
       "1  [ <p> The area MP Ndindi Nyoro said area Natio...   \n",
       "2  [ <p> Sodapoppin said , \" I actually think I '...   \n",
       "3  [ Can you imagine waiting 30 years and then ha...   \n",
       "4  [ <p> \" Kieran Trippier has been charged with ...   \n",
       "5  [ <p> \" Mr Yego has been chasing us like child...   \n",
       "6  [ <p> Since the first case of the coronavirus ...   \n",
       "7  [ <p> Daisy said her chickens were gaping .,  ...   \n",
       "8  [ AFP <p> Donald Trump has no plans to meet wi...   \n",
       "\n",
       "                                    txtfiles_lk_2020  \\\n",
       "0  [ He He said said that that at at present pres...   \n",
       "1  [ <p> Some of the Ministers present had then s...   \n",
       "2  [ <p> He said all members @ @ @ @ @ @ @ @ @ @ ...   \n",
       "3  [ Deepika Udagama said in a letter sent to Pre...   \n",
       "4  [ The Police Media Unit said that the former i...   \n",
       "5  [hits <p> NEW DELHI ( Reuters ) : India is in ...   \n",
       "6  [ <p> The Commercial Bank said the proceeds of...   \n",
       "7  [ <p> \" It is with pleasure that I offer my fe...   \n",
       "8  [ <p> When the meeting concluded and the EPDP ...   \n",
       "\n",
       "                                    txtfiles_my_2020  \\\n",
       "0  [\\n@@41612099 <p> \" I think this club , in the...   \n",
       "1  [ He said he wants to bring the traditional in...   \n",
       "2  [ On Saturday morning , he said he had the num...   \n",
       "3  [ <p> CFM chairman Mohana Mohariff said this i...   \n",
       "4  [ <p> \" It never left the flag , \" Spieth said...   \n",
       "5  [\\n@@42021034 <h> Deportation of illegal immig...   \n",
       "6  [ <p> The yen firmed slightly on safe-haven fl...   \n",
       "7  [\\n@@42162298 <p> However , he said in some ar...   \n",
       "8  [ <p> AMMB 's group chief financial officer Ja...   \n",
       "\n",
       "                                    txtfiles_ng_2020  \\\n",
       "0  [ Taking our services to Kenya represents the ...   \n",
       "1  [ <p> Italy had said Thursday it was stopping ...   \n",
       "2  [ \" <p> Jaja said the BPE had gone to town wit...   \n",
       "3  [\\n@@51583650 <h> Bayelsa Govt resolves to com...   \n",
       "4  [\\n@@51653813 <h> Federal government to engage...   \n",
       "5  [4 million ) of its loans to individuals and b...   \n",
       "6  [ <p> \" There is the need to review a lot of t...   \n",
       "7  [\\n@@51859327 <p> President Muhammadu Buhari h...   \n",
       "8  [ In such cases , rape would be said to have o...   \n",
       "\n",
       "                                    txtfiles_nz_2020  \\\n",
       "0  [ I I just just said said no no .,  <p> \" They...   \n",
       "1  [ <p> The Report said that India is progressin...   \n",
       "2  [ \\n@@31678143 <h> Moments of grief , solidari...   \n",
       "3  [ RBNZ Governor Adrian Orr said that the new T...   \n",
       "4  [ <p> Environment Southland air quality scient...   \n",
       "5  [ <p> \" The protocol has been broken , but it ...   \n",
       "6  [ Salespeople Mark Slade , Mark Walton and Bre...   \n",
       "7  [\\n@@31989266 <p> She said she had complied wi...   \n",
       "8  [ \" <p> Craig Knight , Co-Founder of HYZON Mot...   \n",
       "\n",
       "                                    txtfiles_ph_2020  \\\n",
       "0  [ <p> \" She 's always smiling and very profess...   \n",
       "1  [ But having said that , you ca n't really go ...   \n",
       "2  [ But a day later , his ego said something dif...   \n",
       "3  [\\n@@71836290 <p> \" At this point , what is mo...   \n",
       "4  [ The CHP 's investigation is ongoing , \" a sp...   \n",
       "5  [ In mid-April , Riot said it would n't immedi...   \n",
       "6  [ <p> \" Given the nature and profile of our pl...   \n",
       "7  [ <p> \" The CPL games are quite different , th...   \n",
       "8  [ <p> Miasco , in an interview with Superbalit...   \n",
       "\n",
       "                                    txtfiles_pk_2020  \\\n",
       "0  [ <p> Wajid looked at me and said , \" Governor...   \n",
       "1  [ <p> He said that the modernisation of rail l...   \n",
       "2  [ The accord would have positive effects for P...   \n",
       "3  [ \\n@@84460757 <h> PTI govt launches ' COVID-1...   \n",
       "4  [ <p> Talking to journalists , APTTFA chairman...   \n",
       "5  [ The circular said the officers should call t...   \n",
       "6  [ <p> A large number of people from Tehkal as ...   \n",
       "7  [ Arif Alvi on Thursday said Pakistan 's succe...   \n",
       "8  [ <p> It said that loss in transport by hiring...   \n",
       "\n",
       "                                    txtfiles_sg_2020  \\\n",
       "0  [ <p> Von der Leyen said both sides needed to ...   \n",
       "1  [ <p> Senior members of the royal family were ...   \n",
       "2  [ <p> He testified that he could have \" possib...   \n",
       "3  [ Being largely an online tournament , we have...   \n",
       "4  [ <p> The Times reported that the owner , Andr...   \n",
       "5  [ \" When quiet diplomacy proves futile , the U...   \n",
       "6  [ \" <p> As part of a series of statements issu...   \n",
       "7  [ ) That said , there 's no gore -- and overal...   \n",
       "8  [ <p> The company said that the move is aimed ...   \n",
       "\n",
       "                                    txtfiles_tz_2020  \\\n",
       "0  [\\n@@61494772 <p> A wildlife veterinary expert...   \n",
       "1  [ When they get damaged , we spend more time t...   \n",
       "2  [ \" Defectors will play no significant role in...   \n",
       "3  [ Can you imagine waiting 30 years and then ha...   \n",
       "4  [Mr Mbowe , whose speech was broadcast through...   \n",
       "5  [ \\n@@61900942 <p> Ezekiel Mbilinyi , Mbogwe D...   \n",
       "6  [ <p> Commenting on this year 's shortlist , M...   \n",
       "7  [ Elephant poaching has gone down by 80 percen...   \n",
       "8  [ \\n@@62098492 <h> Tanzanian travellers scared...   \n",
       "\n",
       "                                    txtfiles_us_2020  \\\n",
       "0  [ And it was Justice Lewis Powell who said app...   \n",
       "1  [ <p> \" Inbreeding has always been high in tho...   \n",
       "2  [ You get rest when you can , \" he said .,  <p...   \n",
       "3  [ Contemplating his hard-gained success , Hoga...   \n",
       "4  [ <h> \" It 's like waking up Christmas morning...   \n",
       "5  [ <p> Police Superintendent David Brown said t...   \n",
       "6  [ <p> \" Part of my ideas come more from my Jew...   \n",
       "7  [ <p> \" This term is n't going to end , \" said...   \n",
       "8  [ The tweet said the remaining 94% had \" other...   \n",
       "\n",
       "                                    txtfiles_za_2020  \n",
       "0  [ <p> He said he would be taking a break from ...  \n",
       "1  [ You must remember that he has parted ways wi...  \n",
       "2  [ <p> \" The NHI will ensure that we have suffi...  \n",
       "3  [ <p> He said while they will attempt to keep ...  \n",
       "4  [ <p> Responding to questions during a joint v...  \n",
       "5  [ \\n@@61900952 <h> Newsdeck <h> New York 's Cu...  \n",
       "6  [ Picture : Adel Hana/AP <h> Palestinians decl...  \n",
       "7  [ <p> Lee died Thursday evening at Taipei Vete...  \n",
       "8  [\\n@@62098458 <h> The perpetrators of GBV are ...  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1381651),\n",
       " ('@', 1133827),\n",
       " ('the', 1119350),\n",
       " ('\"', 986572),\n",
       " ('said', 891340),\n",
       " ('.', 833170),\n",
       " ('<p>', 600283),\n",
       " ('to', 557093),\n",
       " ('of', 473406),\n",
       " ('and', 471020),\n",
       " ('a', 461222),\n",
       " ('in', 366837),\n",
       " ('that', 287801),\n",
       " ('he', 263174),\n",
       " (\"'s\", 223949),\n",
       " ('it', 196431),\n",
       " ('was', 181949),\n",
       " ('for', 176020),\n",
       " ('is', 168455),\n",
       " ('i', 157567),\n",
       " ('on', 147951),\n",
       " ('we', 127498),\n",
       " ('with', 127242),\n",
       " ('she', 120825),\n",
       " ('at', 105064),\n",
       " ('be', 104018),\n",
       " ('they', 102319),\n",
       " ('have', 101427),\n",
       " ('as', 94931),\n",
       " ('not', 88122),\n",
       " ('are', 85892),\n",
       " ('this', 84433),\n",
       " ('his', 84420),\n",
       " ('who', 78630),\n",
       " ('but', 77191),\n",
       " (\"n't\", 75186),\n",
       " ('an', 73787),\n",
       " ('from', 73209),\n",
       " ('had', 72717),\n",
       " ('has', 71137),\n",
       " (\"'\", 66562),\n",
       " ('you', 63467),\n",
       " ('by', 62167),\n",
       " ('were', 59742),\n",
       " ('her', 58920),\n",
       " ('would', 58579),\n",
       " ('about', 57751),\n",
       " ('will', 56988),\n",
       " ('there', 53877),\n",
       " ('been', 51631),\n",
       " ('do', 51351),\n",
       " ('people', 49572),\n",
       " ('when', 47939),\n",
       " ('their', 47928),\n",
       " ('what', 45479),\n",
       " ('or', 44297),\n",
       " ('one', 44133),\n",
       " ('more', 43010),\n",
       " ('our', 41066),\n",
       " ('all', 40982),\n",
       " ('?', 40424),\n",
       " ('out', 40287),\n",
       " ('--', 39868),\n",
       " ('if', 37410),\n",
       " ('so', 35710),\n",
       " ('police', 35565),\n",
       " ('after', 35468),\n",
       " ('up', 35321),\n",
       " ('just', 35165),\n",
       " ('can', 35015),\n",
       " ('did', 32875),\n",
       " (':', 32599),\n",
       " ('like', 32289),\n",
       " ('which', 31846),\n",
       " ('because', 31813),\n",
       " ('time', 31682),\n",
       " ('my', 30998),\n",
       " ('also', 30624),\n",
       " ('could', 30400),\n",
       " ('him', 30309),\n",
       " ('no', 29835),\n",
       " ('them', 29723),\n",
       " (')', 29121),\n",
       " (\"'re\", 28018),\n",
       " ('some', 27951),\n",
       " ('new', 27949),\n",
       " ('(', 27483),\n",
       " ('going', 27219),\n",
       " ('get', 26887),\n",
       " ('other', 26858),\n",
       " ('state', 26228),\n",
       " ('its', 25520),\n",
       " ('into', 24460),\n",
       " ('president', 24213),\n",
       " ('than', 23911),\n",
       " ('statement', 23420),\n",
       " ('years', 23089),\n",
       " ('two', 22904),\n",
       " ('year', 22773),\n",
       " ('trump', 22746),\n",
       " ('think', 22621),\n",
       " ('me', 22367),\n",
       " ('how', 21591),\n",
       " ('over', 21255),\n",
       " ('very', 20915),\n",
       " ('know', 20536),\n",
       " ('being', 20070),\n",
       " ('first', 20032),\n",
       " ('any', 19955),\n",
       " ('last', 19865),\n",
       " ('school', 19836),\n",
       " ('us', 19753),\n",
       " ('now', 19725),\n",
       " ('those', 19320),\n",
       " ('really', 18913),\n",
       " ('before', 18841),\n",
       " (\"'ve\", 18617),\n",
       " ('make', 18561),\n",
       " ('where', 18523),\n",
       " ('back', 18190),\n",
       " ('city', 18154),\n",
       " ('work', 18038),\n",
       " ('while', 18013),\n",
       " ('county', 17963),\n",
       " ('want', 17908),\n",
       " ('should', 17753),\n",
       " ('many', 17032),\n",
       " ('see', 16994),\n",
       " ('during', 16979),\n",
       " ('these', 16817),\n",
       " ('officials', 16672),\n",
       " ('good', 16569),\n",
       " ('lot', 16381),\n",
       " ('only', 16349),\n",
       " ('way', 16207),\n",
       " ('family', 16156),\n",
       " ('day', 15849),\n",
       " ('made', 15412),\n",
       " (\"'m\", 15394),\n",
       " ('go', 15264),\n",
       " ('department', 15247),\n",
       " ('through', 15163),\n",
       " ('<h>', 15065),\n",
       " ('still', 14691),\n",
       " ('director', 14681),\n",
       " ('take', 14630),\n",
       " ('then', 14229),\n",
       " ('most', 14179),\n",
       " ('here', 14013),\n",
       " ('home', 13971),\n",
       " ('office', 13862),\n",
       " ('may', 13837),\n",
       " ('even', 13784),\n",
       " ('need', 13714),\n",
       " ('does', 13641),\n",
       " ('public', 13617),\n",
       " ('house', 13537),\n",
       " ('right', 13489),\n",
       " ('much', 13347),\n",
       " ('community', 13330),\n",
       " ('government', 13286),\n",
       " ('down', 13202),\n",
       " ('team', 13144),\n",
       " ('come', 12879),\n",
       " ('against', 12549),\n",
       " ('well', 12516),\n",
       " ('got', 12490),\n",
       " ('company', 12470),\n",
       " ('help', 12466),\n",
       " ('never', 12407),\n",
       " ('part', 12043),\n",
       " ('something', 11887),\n",
       " ('chief', 11850),\n",
       " ('news', 11764),\n",
       " ('week', 11689),\n",
       " ('around', 11606),\n",
       " ('three', 11522),\n",
       " ('told', 11521),\n",
       " ('things', 11410),\n",
       " ('your', 11288),\n",
       " ('man', 11274),\n",
       " ('students', 11270),\n",
       " ('children', 11126),\n",
       " ('another', 11037),\n",
       " ('friday', 11032),\n",
       " ('off', 10963),\n",
       " ('every', 10930),\n",
       " ('such', 10771),\n",
       " ('asked', 10723),\n",
       " ('life', 10628),\n",
       " ('found', 10604),\n",
       " ('group', 10426),\n",
       " ('since', 10420),\n",
       " ('law', 10412),\n",
       " ('court', 10407),\n",
       " ('former', 10343),\n",
       " ('great', 10329),\n",
       " ('report', 10298),\n",
       " ('university', 10239),\n",
       " ('high', 9981),\n",
       " ('country', 9969),\n",
       " ('say', 9967),\n",
       " ('case', 9949),\n",
       " ('both', 9884),\n",
       " ('always', 9831),\n",
       " ('game', 9814),\n",
       " ('thursday', 9781),\n",
       " ('health', 9737),\n",
       " ('attorney', 9633),\n",
       " ('coach', 9564),\n",
       " ('next', 9538),\n",
       " ('national', 9405),\n",
       " ('tuesday', 9331),\n",
       " ('working', 9323),\n",
       " ('women', 9313),\n",
       " ('monday', 9263),\n",
       " ('area', 9118),\n",
       " ('investigation', 9107),\n",
       " ('world', 9062),\n",
       " ('including', 9050),\n",
       " ('long', 9027),\n",
       " ('percent', 8968),\n",
       " ('able', 8955),\n",
       " ('wednesday', 8897),\n",
       " ('same', 8893),\n",
       " ('put', 8868),\n",
       " ('between', 8825),\n",
       " ('business', 8762),\n",
       " ('support', 8760),\n",
       " ('use', 8746),\n",
       " ('little', 8714),\n",
       " ('place', 8691),\n",
       " ('wanted', 8673),\n",
       " ('white', 8629),\n",
       " ('fire', 8606),\n",
       " ('better', 8574),\n",
       " ('under', 8519),\n",
       " ('district', 8507),\n",
       " ('best', 8493),\n",
       " ('used', 8489),\n",
       " ('times', 8473),\n",
       " ('came', 8383),\n",
       " ('board', 8345),\n",
       " ('doing', 8260),\n",
       " ('too', 8236),\n",
       " ('members', 8217),\n",
       " ('interview', 8174),\n",
       " ('end', 8168),\n",
       " ('today', 8161),\n",
       " ('money', 8103),\n",
       " ('look', 8080),\n",
       " ('information', 8049),\n",
       " ('thing', 8020),\n",
       " ('important', 8015),\n",
       " ('center', 7898),\n",
       " ('done', 7879),\n",
       " ('authorities', 7867),\n",
       " ('officer', 7804),\n",
       " ('own', 7743),\n",
       " ('woman', 7695),\n",
       " ('called', 7684),\n",
       " ('night', 7636),\n",
       " ('service', 7619),\n",
       " ('person', 7611),\n",
       " ('why', 7588),\n",
       " ('different', 7555),\n",
       " ('without', 7508),\n",
       " ('feel', 7502),\n",
       " ('north', 7491),\n",
       " ('big', 7477),\n",
       " ('kind', 7433),\n",
       " ('days', 7422),\n",
       " ('head', 7414),\n",
       " ('play', 7405),\n",
       " ('several', 7390),\n",
       " ('executive', 7362),\n",
       " ('away', 7329),\n",
       " ('took', 7325),\n",
       " ('later', 7297),\n",
       " ('trying', 7250),\n",
       " ('whether', 7181),\n",
       " ('sunday', 7181),\n",
       " ('security', 7141),\n",
       " ('care', 7099),\n",
       " ('local', 7083),\n",
       " ('system', 7047),\n",
       " ('program', 7044),\n",
       " (\"'ll\", 7041),\n",
       " (\"'d\", 7032),\n",
       " ('went', 7017),\n",
       " ('adding', 7013),\n",
       " ('change', 7011),\n",
       " ('general', 7008),\n",
       " ('getting', 6993),\n",
       " (';', 6985),\n",
       " ('show', 6974),\n",
       " ('states', 6967),\n",
       " ('left', 6924),\n",
       " ('believe', 6859),\n",
       " ('job', 6856),\n",
       " ('number', 6846),\n",
       " ('thought', 6809),\n",
       " ('decision', 6803),\n",
       " ('officers', 6802),\n",
       " ('continue', 6771),\n",
       " ('ca', 6769),\n",
       " ('having', 6763),\n",
       " ('staff', 6761),\n",
       " ('process', 6750),\n",
       " ('saturday', 6749),\n",
       " ('party', 6728),\n",
       " ('minister', 6708),\n",
       " ('each', 6704),\n",
       " ('meeting', 6669),\n",
       " ('four', 6666),\n",
       " ('give', 6659),\n",
       " ('seen', 6637),\n",
       " ('spokesman', 6604),\n",
       " ('might', 6565),\n",
       " ('past', 6564),\n",
       " ('call', 6521),\n",
       " ('until', 6517),\n",
       " ('united', 6509),\n",
       " ('point', 6504),\n",
       " ('release', 6489),\n",
       " ('young', 6469),\n",
       " ('sure', 6449),\n",
       " ('keep', 6427),\n",
       " ('looking', 6417),\n",
       " ('making', 6395),\n",
       " ('few', 6390),\n",
       " ('hard', 6346),\n",
       " ('someone', 6335),\n",
       " ('deal', 6321),\n",
       " ('find', 6315),\n",
       " ('coming', 6257),\n",
       " ('media', 6234),\n",
       " ('water', 6223),\n",
       " ('senior', 6191),\n",
       " ('death', 6136),\n",
       " ('together', 6114),\n",
       " ('early', 6089),\n",
       " ('according', 6070),\n",
       " ('committee', 6069),\n",
       " ('taken', 6059),\n",
       " ('federal', 6049),\n",
       " ('anything', 6040),\n",
       " ('mother', 5980),\n",
       " ('already', 5974),\n",
       " ('u.', 5949),\n",
       " ('building', 5939),\n",
       " ('started', 5927),\n",
       " ('ago', 5915),\n",
       " ('issue', 5882),\n",
       " ('manager', 5881),\n",
       " ('once', 5873),\n",
       " ('though', 5847),\n",
       " ('political', 5824),\n",
       " ('again', 5803),\n",
       " ('american', 5795),\n",
       " ('car', 5783),\n",
       " ('press', 5781),\n",
       " ('future', 5777),\n",
       " ('million', 5753),\n",
       " ('sheriff', 5749),\n",
       " ('open', 5738),\n",
       " ('forward', 5711),\n",
       " ('kids', 5707),\n",
       " ('plan', 5694),\n",
       " ('administration', 5689),\n",
       " ('official', 5688),\n",
       " ('saw', 5682),\n",
       " ('everything', 5655),\n",
       " ('morning', 5651),\n",
       " ('five', 5612),\n",
       " ('campaign', 5581),\n",
       " ('services', 5579),\n",
       " ('justice', 5572),\n",
       " ('season', 5535),\n",
       " ('agency', 5524),\n",
       " ('issues', 5523),\n",
       " ('earlier', 5520),\n",
       " ('student', 5518),\n",
       " ('john', 5515),\n",
       " ('project', 5508),\n",
       " ('months', 5499),\n",
       " ('love', 5474),\n",
       " ('10', 5467),\n",
       " ('move', 5458),\n",
       " ('am', 5457),\n",
       " ('south', 5436),\n",
       " ('others', 5432),\n",
       " ('conference', 5418),\n",
       " ('ever', 5400),\n",
       " ('safety', 5391),\n",
       " ('knew', 5381),\n",
       " ('across', 5376),\n",
       " ('policy', 5366),\n",
       " ('possible', 5362),\n",
       " ('least', 5353),\n",
       " ('enough', 5338),\n",
       " ('likely', 5314),\n",
       " ('happened', 5297),\n",
       " ('child', 5280),\n",
       " ('member', 5267),\n",
       " ('!', 5260),\n",
       " ('far', 5229),\n",
       " ('father', 5215),\n",
       " ('start', 5212),\n",
       " ('parents', 5177),\n",
       " ('month', 5177),\n",
       " ('involved', 5170),\n",
       " ('event', 5169),\n",
       " ('bill', 5137),\n",
       " ('york', 5117),\n",
       " ('taking', 5113),\n",
       " ('felt', 5100),\n",
       " ('set', 5057),\n",
       " ('second', 5054),\n",
       " ('everyone', 5043),\n",
       " ('hospital', 5019),\n",
       " ('evidence', 5016),\n",
       " ('son', 5009),\n",
       " ('2018', 4978),\n",
       " ('let', 4965),\n",
       " ('experience', 4958),\n",
       " ('needed', 4956),\n",
       " ('medical', 4953),\n",
       " ('outside', 4949),\n",
       " ('nothing', 4940),\n",
       " ('education', 4891),\n",
       " ('history', 4882),\n",
       " ('close', 4882),\n",
       " ('power', 4865),\n",
       " ('recent', 4853),\n",
       " ('families', 4845),\n",
       " ('street', 4840),\n",
       " ('given', 4832),\n",
       " ('market', 4830),\n",
       " ('worked', 4814),\n",
       " ('social', 4780),\n",
       " ('research', 4777),\n",
       " ('run', 4775),\n",
       " ('needs', 4757),\n",
       " ('received', 4749),\n",
       " ('top', 4734),\n",
       " ('college', 4730),\n",
       " ('men', 4729),\n",
       " ('along', 4728),\n",
       " ('special', 4711),\n",
       " ('lives', 4693),\n",
       " ('heard', 4692),\n",
       " ('council', 4688),\n",
       " ('reported', 4682),\n",
       " ('incident', 4680),\n",
       " ('development', 4672),\n",
       " ('story', 4659),\n",
       " ('shooting', 4658),\n",
       " ('judge', 4645),\n",
       " ('real', 4634),\n",
       " ('video', 4609),\n",
       " ('become', 4594),\n",
       " ('bring', 4587),\n",
       " ('must', 4576),\n",
       " ('secretary', 4539),\n",
       " ('leader', 4538),\n",
       " ('sexual', 4537),\n",
       " ('pay', 4536),\n",
       " ('-', 4536),\n",
       " ('situation', 4524),\n",
       " ('road', 4514),\n",
       " ('data', 4513),\n",
       " ('killed', 4499),\n",
       " ('expected', 4495),\n",
       " ('matter', 4483),\n",
       " ('try', 4476),\n",
       " ('friends', 4474),\n",
       " ('washington', 4472),\n",
       " ('provide', 4464),\n",
       " ('clear', 4463),\n",
       " ('tell', 4462),\n",
       " ('rights', 4454),\n",
       " ('election', 4454),\n",
       " ('near', 4453),\n",
       " ('plans', 4443),\n",
       " ('however', 4436),\n",
       " ('within', 4419),\n",
       " ('shot', 4411),\n",
       " ('schools', 4411),\n",
       " ('yet', 4409),\n",
       " ('actually', 4403),\n",
       " ('hours', 4366),\n",
       " ('defense', 4351),\n",
       " ('west', 4337),\n",
       " ('hope', 4319),\n",
       " ('order', 4313),\n",
       " ('live', 4292),\n",
       " ('mr.', 4290),\n",
       " ('often', 4285),\n",
       " ('smith', 4284),\n",
       " ('opportunity', 4280),\n",
       " ('park', 4278),\n",
       " ('2016', 4271),\n",
       " ('vote', 4259),\n",
       " ('line', 4256),\n",
       " ('probably', 4252),\n",
       " ('facebook', 4237),\n",
       " ('room', 4219),\n",
       " ('victim', 4215),\n",
       " ('less', 4199),\n",
       " ('town', 4199),\n",
       " ('church', 4197),\n",
       " ('stop', 4194),\n",
       " ('small', 4191),\n",
       " ('problem', 4187),\n",
       " ('cases', 4181),\n",
       " ('residents', 4162),\n",
       " ('talk', 4157),\n",
       " ('pretty', 4150),\n",
       " ('professor', 4146),\n",
       " ('leave', 4135),\n",
       " ('name', 4127),\n",
       " ('black', 4125),\n",
       " ('employees', 4118),\n",
       " ('hit', 4116),\n",
       " ('side', 4114),\n",
       " ('food', 4107),\n",
       " ('johnson', 4096),\n",
       " ('human', 4090),\n",
       " ('body', 4077),\n",
       " ('international', 4075),\n",
       " ('saying', 4068),\n",
       " ('further', 4056),\n",
       " ('among', 4055),\n",
       " ('investigators', 4054),\n",
       " ('using', 4053),\n",
       " ('legal', 4046),\n",
       " ('weeks', 4043),\n",
       " ('win', 4037),\n",
       " ('lead', 4014),\n",
       " ('bank', 4012),\n",
       " ('mayor', 4011),\n",
       " ('response', 4010),\n",
       " ('died', 4002),\n",
       " ('victims', 4002),\n",
       " ('spokeswoman', 4001),\n",
       " ('due', 3993),\n",
       " ('gun', 3989),\n",
       " ('behind', 3967),\n",
       " ('enforcement', 3966),\n",
       " ('companies', 3966),\n",
       " ('strong', 3963),\n",
       " ('david', 3963),\n",
       " ('ceo', 3953),\n",
       " ('couple', 3951),\n",
       " ('level', 3946),\n",
       " ('old', 3942),\n",
       " ('face', 3937),\n",
       " ('current', 3931),\n",
       " ('action', 3904),\n",
       " ('inside', 3886),\n",
       " ('players', 3880),\n",
       " ('late', 3878),\n",
       " ('daughter', 3876),\n",
       " ('whole', 3870),\n",
       " ('six', 3868),\n",
       " ('2017', 3867),\n",
       " ('military', 3867),\n",
       " ('michael', 3865),\n",
       " ('impact', 3864),\n",
       " ('especially', 3855),\n",
       " ('phone', 3849),\n",
       " ('himself', 3844),\n",
       " ('anyone', 3835),\n",
       " ('happen', 3824),\n",
       " ('post', 3823),\n",
       " ('potential', 3823),\n",
       " ('organization', 3822),\n",
       " ('friend', 3820),\n",
       " ('following', 3810),\n",
       " ('wants', 3794),\n",
       " ('full', 3790),\n",
       " ('lost', 3785),\n",
       " ('china', 3784),\n",
       " ('prosecutors', 3778),\n",
       " ('idea', 3769),\n",
       " ('guys', 3765),\n",
       " ('bad', 3762),\n",
       " ('based', 3762),\n",
       " ('running', 3761),\n",
       " ('republican', 3758),\n",
       " ('playing', 3739),\n",
       " ('20', 3738),\n",
       " ('major', 3730),\n",
       " ('financial', 3729),\n",
       " ('tax', 3728),\n",
       " ('fact', 3716),\n",
       " ('released', 3695),\n",
       " ('wife', 3678),\n",
       " ('allegations', 3674),\n",
       " ('understand', 3673),\n",
       " ('property', 3667),\n",
       " ('vehicle', 3658),\n",
       " ('arrested', 3654),\n",
       " ('control', 3636),\n",
       " ('role', 3632),\n",
       " ('known', 3631),\n",
       " ('trade', 3628),\n",
       " ('attack', 3627),\n",
       " ('reports', 3626),\n",
       " ('held', 3615),\n",
       " ('force', 3615),\n",
       " ('says', 3609),\n",
       " ('email', 3604),\n",
       " ('america', 3595),\n",
       " ('areas', 3581),\n",
       " ('speaking', 3580),\n",
       " ('large', 3572),\n",
       " ('difficult', 3562),\n",
       " ('kavanaugh', 3559),\n",
       " ('senate', 3555),\n",
       " ('industry', 3532),\n",
       " ('abuse', 3529),\n",
       " ('tried', 3524),\n",
       " ('weather', 3516),\n",
       " ('leaders', 3514),\n",
       " ('bit', 3506),\n",
       " ('war', 3496),\n",
       " ('whose', 3490),\n",
       " ('question', 3472),\n",
       " ('comes', 3472),\n",
       " ('study', 3462),\n",
       " ('air', 3447),\n",
       " ('access', 3444),\n",
       " ('risk', 3444),\n",
       " ('position', 3443),\n",
       " ('chairman', 3441),\n",
       " ('brown', 3423),\n",
       " ('israel', 3416),\n",
       " ('charges', 3415),\n",
       " ('makes', 3408),\n",
       " ('meet', 3385),\n",
       " ('stay', 3383),\n",
       " ('reason', 3380),\n",
       " ('began', 3376),\n",
       " ('economic', 3369),\n",
       " ('letter', 3368),\n",
       " ('march', 3366),\n",
       " ('california', 3365),\n",
       " ('although', 3363),\n",
       " ('safe', 3359),\n",
       " ('played', 3356),\n",
       " ('deputy', 3352),\n",
       " ('recently', 3341),\n",
       " ('almost', 3326),\n",
       " ('fbi', 3322),\n",
       " ('criminal', 3321),\n",
       " ('gave', 3308),\n",
       " ('foreign', 3308),\n",
       " ('vice', 3304),\n",
       " ('read', 3280),\n",
       " ('front', 3271),\n",
       " ('helped', 3264),\n",
       " ('russia', 3255),\n",
       " ('cost', 3254),\n",
       " ('association', 3253),\n",
       " ('hearing', 3246),\n",
       " ('mike', 3245),\n",
       " ('interest', 3243),\n",
       " ('efforts', 3234),\n",
       " ('congress', 3228),\n",
       " ('field', 3222),\n",
       " ('free', 3221),\n",
       " ('goal', 3217),\n",
       " ('mark', 3217),\n",
       " ('violence', 3199),\n",
       " ('workers', 3194),\n",
       " ('30', 3192),\n",
       " ('commission', 3189),\n",
       " ('half', 3185),\n",
       " ('border', 3179),\n",
       " ('cause', 3175),\n",
       " ('union', 3172),\n",
       " ('james', 3171),\n",
       " ('increase', 3165),\n",
       " ('living', 3163),\n",
       " ('games', 3151),\n",
       " ('governor', 3137),\n",
       " ('talking', 3126),\n",
       " ('despite', 3108),\n",
       " ('focus', 3107),\n",
       " ('led', 3096),\n",
       " ('act', 3086),\n",
       " ('instead', 3081),\n",
       " ('democrats', 3080),\n",
       " ('scott', 3079),\n",
       " ('photo', 3078),\n",
       " ('sent', 3069),\n",
       " ('crime', 3066),\n",
       " ('guy', 3064),\n",
       " ('technology', 3053),\n",
       " ('management', 3044),\n",
       " ('significant', 3042),\n",
       " ('works', 3039),\n",
       " ('changes', 3037),\n",
       " ('met', 3030),\n",
       " ('else', 3026),\n",
       " ('chance', 3021),\n",
       " ('brought', 3020),\n",
       " ('space', 3019),\n",
       " ('emergency', 3019),\n",
       " ('believed', 3016),\n",
       " ('means', 3014),\n",
       " ('course', 3004),\n",
       " ('drug', 3003),\n",
       " ('suspect', 3002),\n",
       " ('patients', 2999),\n",
       " ('questions', 2996),\n",
       " ('happy', 2995),\n",
       " ('soon', 2990),\n",
       " ('training', 2986),\n",
       " ('personal', 2986),\n",
       " ('concerns', 2983),\n",
       " ('per', 2979),\n",
       " ('everybody', 2970),\n",
       " ('heart', 2968),\n",
       " ('wo', 2965),\n",
       " ('williams', 2964),\n",
       " ('democratic', 2957),\n",
       " ('ready', 2955),\n",
       " ('girl', 2954),\n",
       " ('problems', 2948),\n",
       " ('energy', 2946),\n",
       " ('mr', 2944),\n",
       " ('longer', 2939),\n",
       " ('lawyer', 2938),\n",
       " ('result', 2929),\n",
       " ('prime', 2929),\n",
       " ('growth', 2927),\n",
       " ('review', 2912),\n",
       " ('return', 2911),\n",
       " ('available', 2908),\n",
       " ('seeing', 2901),\n",
       " ('east', 2899),\n",
       " ('address', 2899),\n",
       " ('1', 2896),\n",
       " ('san', 2875),\n",
       " ('decided', 2871),\n",
       " ('step', 2866),\n",
       " ('serious', 2854),\n",
       " ('private', 2844),\n",
       " ('scene', 2842),\n",
       " ('assistant', 2841),\n",
       " ('jones', 2836),\n",
       " ('toward', 2834),\n",
       " ('moving', 2831),\n",
       " ('ensure', 2829),\n",
       " ('added', 2828),\n",
       " ('fight', 2827),\n",
       " ('customers', 2826),\n",
       " ('donald', 2826),\n",
       " ('spokesperson', 2818),\n",
       " ('showed', 2818),\n",
       " ('florida', 2815),\n",
       " ('agreement', 2815),\n",
       " ('allow', 2814),\n",
       " ('share', 2813),\n",
       " ('additional', 2812),\n",
       " ('message', 2810),\n",
       " ('effort', 2805),\n",
       " ('russian', 2805),\n",
       " ('nearly', 2802),\n",
       " ('multiple', 2798),\n",
       " ('sense', 2797),\n",
       " ('resources', 2784),\n",
       " ('remain', 2784),\n",
       " ('football', 2779),\n",
       " ('higher', 2772),\n",
       " ('accused', 2767),\n",
       " ('store', 2766),\n",
       " ('career', 2762),\n",
       " ('sometimes', 2760),\n",
       " ('third', 2759),\n",
       " ('looked', 2756),\n",
       " ('amount', 2750),\n",
       " ('countries', 2745),\n",
       " ('concerned', 2742),\n",
       " ('protect', 2740),\n",
       " ('hall', 2731),\n",
       " ('ministry', 2727),\n",
       " ('relationship', 2727),\n",
       " ('similar', 2722),\n",
       " ('site', 2715),\n",
       " ('events', 2715),\n",
       " ('jobs', 2713),\n",
       " ('driver', 2713),\n",
       " ('rather', 2713),\n",
       " ('aware', 2711),\n",
       " ('central', 2710),\n",
       " ('create', 2709),\n",
       " ('wrong', 2698),\n",
       " ('husband', 2698),\n",
       " ('moment', 2697),\n",
       " ('afternoon', 2697),\n",
       " ('became', 2696),\n",
       " ('leadership', 2673),\n",
       " ('storm', 2671),\n",
       " ('owner', 2668),\n",
       " ('ask', 2652),\n",
       " ('minutes', 2652),\n",
       " ('related', 2643),\n",
       " ('ways', 2642),\n",
       " ('nation', 2635),\n",
       " ('source', 2634),\n",
       " ('hand', 2625),\n",
       " ('15', 2622),\n",
       " ('comment', 2620),\n",
       " ('record', 2618),\n",
       " ('maybe', 2617),\n",
       " ('hear', 2604),\n",
       " ('ahead', 2600),\n",
       " ('assault', 2598),\n",
       " ('key', 2597),\n",
       " ('immediately', 2593),\n",
       " ('remains', 2592),\n",
       " ('2', 2578),\n",
       " ('themselves', 2576),\n",
       " ('age', 2576),\n",
       " ('excited', 2574),\n",
       " ('housing', 2573),\n",
       " ('middle', 2566),\n",
       " ('twitter', 2566),\n",
       " ('paul', 2564),\n",
       " ('land', 2562),\n",
       " ('player', 2557),\n",
       " ('practice', 2554),\n",
       " ('groups', 2551),\n",
       " ('korea', 2551),\n",
       " ('final', 2545),\n",
       " ('build', 2540),\n",
       " ('region', 2540),\n",
       " ('voters', 2540),\n",
       " ('offer', 2539),\n",
       " ('12', 2538),\n",
       " ('june', 2536),\n",
       " ('definitely', 2529),\n",
       " ('remember', 2528),\n",
       " ('believes', 2527),\n",
       " ('entire', 2525),\n",
       " ('hold', 2523),\n",
       " ('treatment', 2516),\n",
       " ('boy', 2515),\n",
       " ('either', 2514),\n",
       " ('positive', 2514),\n",
       " ('sources', 2512),\n",
       " ('turned', 2510),\n",
       " ('homes', 2508),\n",
       " ('spoke', 2505),\n",
       " ('businesses', 2503),\n",
       " ('expect', 2503),\n",
       " ('search', 2501),\n",
       " ('race', 2498),\n",
       " ('funding', 2494),\n",
       " ('firm', 2489),\n",
       " ('throughout', 2488),\n",
       " ('certain', 2482),\n",
       " ('conditions', 2482),\n",
       " ('class', 2481),\n",
       " ('list', 2471),\n",
       " ('mean', 2470),\n",
       " ('april', 2466),\n",
       " ('texas', 2464),\n",
       " ('learned', 2456),\n",
       " ('identified', 2455),\n",
       " ('communities', 2449),\n",
       " ('dead', 2446),\n",
       " ('charged', 2444),\n",
       " ('proud', 2444),\n",
       " ('allowed', 2442),\n",
       " ('growing', 2439),\n",
       " ('summer', 2439),\n",
       " ('economy', 2436),\n",
       " ('loss', 2435),\n",
       " ('door', 2433),\n",
       " ('gone', 2432),\n",
       " ('turn', 2430),\n",
       " ('committed', 2426),\n",
       " ('global', 2426),\n",
       " ('ground', 2424),\n",
       " ('learn', 2424),\n",
       " ('resident', 2423),\n",
       " ('drive', 2421),\n",
       " ('ford', 2413),\n",
       " ('5', 2412),\n",
       " ('results', 2411),\n",
       " ('speak', 2410),\n",
       " ('alleged', 2397),\n",
       " ('actions', 2393),\n",
       " ('arrest', 2393),\n",
       " ('&amp;', 2391),\n",
       " ('campus', 2389),\n",
       " ('society', 2387),\n",
       " ('spent', 2375),\n",
       " ('price', 2365),\n",
       " ('driving', 2364),\n",
       " ('calls', 2364),\n",
       " ('ball', 2364),\n",
       " ('operations', 2359),\n",
       " ('sign', 2350),\n",
       " ('quickly', 2347),\n",
       " ('red', 2347),\n",
       " ('credit', 2338),\n",
       " ('giving', 2338),\n",
       " ('huge', 2337),\n",
       " ('fall', 2332),\n",
       " ('advertisement', 2332),\n",
       " ('robert', 2327),\n",
       " ('brother', 2325),\n",
       " ('paid', 2324),\n",
       " ('teachers', 2323),\n",
       " ('budget', 2320),\n",
       " ('speech', 2317),\n",
       " ('true', 2317),\n",
       " ('main', 2315),\n",
       " ('provided', 2314),\n",
       " ('july', 2314),\n",
       " ('teacher', 2312),\n",
       " ('stand', 2311),\n",
       " ('commissioner', 2310),\n",
       " ('online', 2303),\n",
       " ('previously', 2302),\n",
       " ('biggest', 2302),\n",
       " ('seven', 2302),\n",
       " ('shows', 2302),\n",
       " ('cut', 2294),\n",
       " ('league', 2292),\n",
       " ('serve', 2291),\n",
       " ('terms', 2290),\n",
       " ('100', 2286),\n",
       " ('thomas', 2281),\n",
       " ('goes', 2280),\n",
       " ('kim', 2280),\n",
       " ('planned', 2279),\n",
       " ('murder', 2274),\n",
       " ('attention', 2271),\n",
       " ('light', 2270),\n",
       " ('include', 2266),\n",
       " ('single', 2265),\n",
       " ('fun', 2265),\n",
       " ('republicans', 2263),\n",
       " ('book', 2255),\n",
       " ('ability', 2254),\n",
       " ('changed', 2254),\n",
       " ('currently', 2252),\n",
       " ('music', 2250),\n",
       " ('club', 2250),\n",
       " ('immigration', 2249),\n",
       " ('short', 2248),\n",
       " ('period', 2247),\n",
       " ('takes', 2240),\n",
       " ('threat', 2238),\n",
       " ('referring', 2233),\n",
       " ('traffic', 2232),\n",
       " ('mind', 2229),\n",
       " ('dr.', 2227),\n",
       " ('capital', 2226),\n",
       " ('environment', 2223),\n",
       " ('god', 2221),\n",
       " ('planning', 2214),\n",
       " ('trust', 2212),\n",
       " ('caused', 2210),\n",
       " ('sales', 2208),\n",
       " ('fired', 2207),\n",
       " ('loved', 2207),\n",
       " ('3', 2202),\n",
       " ('moved', 2199),\n",
       " ('rest', 2195),\n",
       " ('gas', 2192),\n",
       " ('hill', 2183),\n",
       " ('majority', 2183),\n",
       " ('hopes', 2182),\n",
       " ('details', 2180),\n",
       " ('chris', 2169),\n",
       " ('2015', 2160),\n",
       " ('weekend', 2159),\n",
       " ('trial', 2159),\n",
       " ('thinking', 2156),\n",
       " ('programs', 2156),\n",
       " ('series', 2154),\n",
       " ('quality', 2147),\n",
       " ('davis', 2141),\n",
       " ('documents', 2139),\n",
       " ('lack', 2138),\n",
       " ('authority', 2132),\n",
       " ('previous', 2131),\n",
       " ('damage', 2127),\n",
       " ('green', 2119),\n",
       " ('elections', 2118),\n",
       " ('gets', 2118),\n",
       " ('army', 2114),\n",
       " ...]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_us_2018.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2010.to_pickle('df_2010.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2011.to_pickle('df_2011.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012.to_pickle('df_2012.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.to_pickle('df_2013.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014.to_pickle('df_2014.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015.to_pickle('df_2015.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016.to_pickle('df_2016.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017.to_pickle('df_2017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.to_pickle('df_2018.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019.to_pickle('df_2019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020.to_pickle('df_2020.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_pickle('df_2018.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.txtfiles_au_2018[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
