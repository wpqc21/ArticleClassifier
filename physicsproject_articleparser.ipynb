{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import sys\n",
    "# import glob\n",
    "# import codecs\n",
    "# import pathlib\n",
    "\n",
    "# # from tqdm import tqdm\n",
    "# # import slugify\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "# !pip install utils\n",
    "\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW_PATH = r\"../Physics Project GITHUB/\" #None\n",
    "# sources_files = utils.get_sources_files(path=RAW_PATH)\n",
    "# text_folders = utils.get_text_folders(path=RAW_PATH)\n",
    "\n",
    "# print(\"Number of Sources Files: \", len(sources_files))\n",
    "# print(\"Number of Text Folders: \", len(text_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print(glob.glob(\"/Physics Project GITHUB/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# All files ending with .txt\n",
    "print(glob.glob(\"/Physics Project GITHUB/*.txt\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "txtfiles_all = []\n",
    "for file in glob.glob('**/10*'):\n",
    "    txtfiles_all.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_10-01\\\\10-01-au.txt',\n",
       " 'text_10-01\\\\10-01-bd.txt',\n",
       " 'text_10-01\\\\10-01-ca.txt',\n",
       " 'text_10-01\\\\10-01-gb.txt',\n",
       " 'text_10-01\\\\10-01-gh.txt',\n",
       " 'text_10-01\\\\10-01-hk.txt',\n",
       " 'text_10-01\\\\10-01-ie.txt',\n",
       " 'text_10-01\\\\10-01-in.txt',\n",
       " 'text_10-01\\\\10-01-jm.txt',\n",
       " 'text_10-01\\\\10-01-ke.txt',\n",
       " 'text_10-01\\\\10-01-lk.txt',\n",
       " 'text_10-01\\\\10-01-my.txt',\n",
       " 'text_10-01\\\\10-01-ng.txt',\n",
       " 'text_10-01\\\\10-01-nz.txt',\n",
       " 'text_10-01\\\\10-01-ph.txt',\n",
       " 'text_10-01\\\\10-01-pk.txt',\n",
       " 'text_10-01\\\\10-01-sg.txt',\n",
       " 'text_10-01\\\\10-01-tz.txt',\n",
       " 'text_10-01\\\\10-01-us.txt',\n",
       " 'text_10-01\\\\10-01-za.txt',\n",
       " 'text_10-02\\\\10-02-au.txt',\n",
       " 'text_10-02\\\\10-02-bd.txt',\n",
       " 'text_10-02\\\\10-02-ca.txt',\n",
       " 'text_10-02\\\\10-02-gb.txt',\n",
       " 'text_10-02\\\\10-02-gh.txt',\n",
       " 'text_10-02\\\\10-02-hk.txt',\n",
       " 'text_10-02\\\\10-02-ie.txt',\n",
       " 'text_10-02\\\\10-02-in.txt',\n",
       " 'text_10-02\\\\10-02-jm.txt',\n",
       " 'text_10-02\\\\10-02-ke.txt',\n",
       " 'text_10-02\\\\10-02-lk.txt',\n",
       " 'text_10-02\\\\10-02-my.txt',\n",
       " 'text_10-02\\\\10-02-ng.txt',\n",
       " 'text_10-02\\\\10-02-nz.txt',\n",
       " 'text_10-02\\\\10-02-ph.txt',\n",
       " 'text_10-02\\\\10-02-pk.txt',\n",
       " 'text_10-02\\\\10-02-sg.txt',\n",
       " 'text_10-02\\\\10-02-tz.txt',\n",
       " 'text_10-02\\\\10-02-us.txt',\n",
       " 'text_10-02\\\\10-02-za.txt',\n",
       " 'text_10-03\\\\10-03-au.txt',\n",
       " 'text_10-03\\\\10-03-bd.txt',\n",
       " 'text_10-03\\\\10-03-ca.txt',\n",
       " 'text_10-03\\\\10-03-gb.txt',\n",
       " 'text_10-03\\\\10-03-gh.txt',\n",
       " 'text_10-03\\\\10-03-hk.txt',\n",
       " 'text_10-03\\\\10-03-ie.txt',\n",
       " 'text_10-03\\\\10-03-in.txt',\n",
       " 'text_10-03\\\\10-03-jm.txt',\n",
       " 'text_10-03\\\\10-03-ke.txt',\n",
       " 'text_10-03\\\\10-03-lk.txt',\n",
       " 'text_10-03\\\\10-03-my.txt',\n",
       " 'text_10-03\\\\10-03-ng.txt',\n",
       " 'text_10-03\\\\10-03-nz.txt',\n",
       " 'text_10-03\\\\10-03-ph.txt',\n",
       " 'text_10-03\\\\10-03-pk.txt',\n",
       " 'text_10-03\\\\10-03-sg.txt',\n",
       " 'text_10-03\\\\10-03-tz.txt',\n",
       " 'text_10-03\\\\10-03-us.txt',\n",
       " 'text_10-03\\\\10-03-za.txt',\n",
       " 'text_10-04\\\\10-04-au.txt',\n",
       " 'text_10-04\\\\10-04-bd.txt',\n",
       " 'text_10-04\\\\10-04-ca.txt',\n",
       " 'text_10-04\\\\10-04-gb.txt',\n",
       " 'text_10-04\\\\10-04-gh.txt',\n",
       " 'text_10-04\\\\10-04-hk.txt',\n",
       " 'text_10-04\\\\10-04-ie.txt',\n",
       " 'text_10-04\\\\10-04-in.txt',\n",
       " 'text_10-04\\\\10-04-jm.txt',\n",
       " 'text_10-04\\\\10-04-ke.txt',\n",
       " 'text_10-04\\\\10-04-lk.txt',\n",
       " 'text_10-04\\\\10-04-my.txt',\n",
       " 'text_10-04\\\\10-04-ng.txt',\n",
       " 'text_10-04\\\\10-04-nz.txt',\n",
       " 'text_10-04\\\\10-04-ph.txt',\n",
       " 'text_10-04\\\\10-04-pk.txt',\n",
       " 'text_10-04\\\\10-04-sg.txt',\n",
       " 'text_10-04\\\\10-04-us.txt',\n",
       " 'text_10-04\\\\10-04-za.txt',\n",
       " 'text_10-05\\\\10-05-au.txt',\n",
       " 'text_10-05\\\\10-05-bd.txt',\n",
       " 'text_10-05\\\\10-05-ca.txt',\n",
       " 'text_10-05\\\\10-05-gb.txt',\n",
       " 'text_10-05\\\\10-05-gh.txt',\n",
       " 'text_10-05\\\\10-05-hk.txt',\n",
       " 'text_10-05\\\\10-05-ie.txt',\n",
       " 'text_10-05\\\\10-05-in.txt',\n",
       " 'text_10-05\\\\10-05-jm.txt',\n",
       " 'text_10-05\\\\10-05-ke.txt',\n",
       " 'text_10-05\\\\10-05-lk.txt',\n",
       " 'text_10-05\\\\10-05-my.txt',\n",
       " 'text_10-05\\\\10-05-ng.txt',\n",
       " 'text_10-05\\\\10-05-nz.txt',\n",
       " 'text_10-05\\\\10-05-ph.txt',\n",
       " 'text_10-05\\\\10-05-pk.txt',\n",
       " 'text_10-05\\\\10-05-sg.txt',\n",
       " 'text_10-05\\\\10-05-tz.txt',\n",
       " 'text_10-05\\\\10-05-us.txt',\n",
       " 'text_10-05\\\\10-05-za.txt',\n",
       " 'text_10-06\\\\10-06-au.txt',\n",
       " 'text_10-06\\\\10-06-bd.txt',\n",
       " 'text_10-06\\\\10-06-ca.txt',\n",
       " 'text_10-06\\\\10-06-gb.txt',\n",
       " 'text_10-06\\\\10-06-gh.txt',\n",
       " 'text_10-06\\\\10-06-hk.txt',\n",
       " 'text_10-06\\\\10-06-ie.txt',\n",
       " 'text_10-06\\\\10-06-in.txt',\n",
       " 'text_10-06\\\\10-06-jm.txt',\n",
       " 'text_10-06\\\\10-06-ke.txt',\n",
       " 'text_10-06\\\\10-06-lk.txt',\n",
       " 'text_10-06\\\\10-06-my.txt',\n",
       " 'text_10-06\\\\10-06-ng.txt',\n",
       " 'text_10-06\\\\10-06-nz.txt',\n",
       " 'text_10-06\\\\10-06-ph.txt',\n",
       " 'text_10-06\\\\10-06-pk.txt',\n",
       " 'text_10-06\\\\10-06-sg.txt',\n",
       " 'text_10-06\\\\10-06-tz.txt',\n",
       " 'text_10-06\\\\10-06-us.txt',\n",
       " 'text_10-06\\\\10-06-za.txt',\n",
       " 'text_10-07\\\\10-07-au.txt',\n",
       " 'text_10-07\\\\10-07-bd.txt',\n",
       " 'text_10-07\\\\10-07-ca.txt',\n",
       " 'text_10-07\\\\10-07-gb.txt',\n",
       " 'text_10-07\\\\10-07-gh.txt',\n",
       " 'text_10-07\\\\10-07-hk.txt',\n",
       " 'text_10-07\\\\10-07-ie.txt',\n",
       " 'text_10-07\\\\10-07-in.txt',\n",
       " 'text_10-07\\\\10-07-jm.txt',\n",
       " 'text_10-07\\\\10-07-ke.txt',\n",
       " 'text_10-07\\\\10-07-lk.txt',\n",
       " 'text_10-07\\\\10-07-my.txt',\n",
       " 'text_10-07\\\\10-07-ng.txt',\n",
       " 'text_10-07\\\\10-07-nz.txt',\n",
       " 'text_10-07\\\\10-07-ph.txt',\n",
       " 'text_10-07\\\\10-07-pk.txt',\n",
       " 'text_10-07\\\\10-07-sg.txt',\n",
       " 'text_10-07\\\\10-07-us.txt',\n",
       " 'text_10-07\\\\10-07-za.txt',\n",
       " 'text_10-08\\\\10-08-au.txt',\n",
       " 'text_10-08\\\\10-08-bd.txt',\n",
       " 'text_10-08\\\\10-08-ca.txt',\n",
       " 'text_10-08\\\\10-08-gb.txt',\n",
       " 'text_10-08\\\\10-08-gh.txt',\n",
       " 'text_10-08\\\\10-08-hk.txt',\n",
       " 'text_10-08\\\\10-08-ie.txt',\n",
       " 'text_10-08\\\\10-08-in.txt',\n",
       " 'text_10-08\\\\10-08-jm.txt',\n",
       " 'text_10-08\\\\10-08-ke.txt',\n",
       " 'text_10-08\\\\10-08-lk.txt',\n",
       " 'text_10-08\\\\10-08-my.txt',\n",
       " 'text_10-08\\\\10-08-ng.txt',\n",
       " 'text_10-08\\\\10-08-nz.txt',\n",
       " 'text_10-08\\\\10-08-ph.txt',\n",
       " 'text_10-08\\\\10-08-pk.txt',\n",
       " 'text_10-08\\\\10-08-sg.txt',\n",
       " 'text_10-08\\\\10-08-tz.txt',\n",
       " 'text_10-08\\\\10-08-us.txt',\n",
       " 'text_10-08\\\\10-08-za.txt',\n",
       " 'text_10-09\\\\10-09-au.txt',\n",
       " 'text_10-09\\\\10-09-bd.txt',\n",
       " 'text_10-09\\\\10-09-ca.txt',\n",
       " 'text_10-09\\\\10-09-gb.txt',\n",
       " 'text_10-09\\\\10-09-gh.txt',\n",
       " 'text_10-09\\\\10-09-hk.txt',\n",
       " 'text_10-09\\\\10-09-ie.txt',\n",
       " 'text_10-09\\\\10-09-in.txt',\n",
       " 'text_10-09\\\\10-09-jm.txt',\n",
       " 'text_10-09\\\\10-09-ke.txt',\n",
       " 'text_10-09\\\\10-09-lk.txt',\n",
       " 'text_10-09\\\\10-09-my.txt',\n",
       " 'text_10-09\\\\10-09-ng.txt',\n",
       " 'text_10-09\\\\10-09-nz.txt',\n",
       " 'text_10-09\\\\10-09-ph.txt',\n",
       " 'text_10-09\\\\10-09-pk.txt',\n",
       " 'text_10-09\\\\10-09-sg.txt',\n",
       " 'text_10-09\\\\10-09-us.txt',\n",
       " 'text_10-09\\\\10-09-za.txt',\n",
       " 'text_10-10\\\\10-10-au.txt',\n",
       " 'text_10-10\\\\10-10-bd.txt',\n",
       " 'text_10-10\\\\10-10-ca.txt',\n",
       " 'text_10-10\\\\10-10-gb.txt',\n",
       " 'text_10-10\\\\10-10-gh.txt',\n",
       " 'text_10-10\\\\10-10-hk.txt',\n",
       " 'text_10-10\\\\10-10-ie.txt',\n",
       " 'text_10-10\\\\10-10-in.txt',\n",
       " 'text_10-10\\\\10-10-jm.txt',\n",
       " 'text_10-10\\\\10-10-ke.txt',\n",
       " 'text_10-10\\\\10-10-lk.txt',\n",
       " 'text_10-10\\\\10-10-my.txt',\n",
       " 'text_10-10\\\\10-10-ng.txt',\n",
       " 'text_10-10\\\\10-10-nz.txt',\n",
       " 'text_10-10\\\\10-10-ph.txt',\n",
       " 'text_10-10\\\\10-10-pk.txt',\n",
       " 'text_10-10\\\\10-10-sg.txt',\n",
       " 'text_10-10\\\\10-10-tz.txt',\n",
       " 'text_10-10\\\\10-10-us.txt',\n",
       " 'text_10-10\\\\10-10-za.txt',\n",
       " 'text_10-11\\\\10-11-au.txt',\n",
       " 'text_10-11\\\\10-11-bd.txt',\n",
       " 'text_10-11\\\\10-11-ca.txt',\n",
       " 'text_10-11\\\\10-11-gb.txt',\n",
       " 'text_10-11\\\\10-11-gh.txt',\n",
       " 'text_10-11\\\\10-11-hk.txt',\n",
       " 'text_10-11\\\\10-11-ie.txt',\n",
       " 'text_10-11\\\\10-11-in.txt',\n",
       " 'text_10-11\\\\10-11-jm.txt',\n",
       " 'text_10-11\\\\10-11-ke.txt',\n",
       " 'text_10-11\\\\10-11-lk.txt',\n",
       " 'text_10-11\\\\10-11-my.txt',\n",
       " 'text_10-11\\\\10-11-ng.txt',\n",
       " 'text_10-11\\\\10-11-nz.txt',\n",
       " 'text_10-11\\\\10-11-ph.txt',\n",
       " 'text_10-11\\\\10-11-pk.txt',\n",
       " 'text_10-11\\\\10-11-sg.txt',\n",
       " 'text_10-11\\\\10-11-tz.txt',\n",
       " 'text_10-11\\\\10-11-us.txt',\n",
       " 'text_10-11\\\\10-11-za.txt',\n",
       " 'text_10-12\\\\10-12-au.txt',\n",
       " 'text_10-12\\\\10-12-bd.txt',\n",
       " 'text_10-12\\\\10-12-ca.txt',\n",
       " 'text_10-12\\\\10-12-gb.txt',\n",
       " 'text_10-12\\\\10-12-gh.txt',\n",
       " 'text_10-12\\\\10-12-hk.txt',\n",
       " 'text_10-12\\\\10-12-ie.txt',\n",
       " 'text_10-12\\\\10-12-in.txt',\n",
       " 'text_10-12\\\\10-12-jm.txt',\n",
       " 'text_10-12\\\\10-12-ke.txt',\n",
       " 'text_10-12\\\\10-12-lk.txt',\n",
       " 'text_10-12\\\\10-12-my.txt',\n",
       " 'text_10-12\\\\10-12-ng.txt',\n",
       " 'text_10-12\\\\10-12-nz.txt',\n",
       " 'text_10-12\\\\10-12-ph.txt',\n",
       " 'text_10-12\\\\10-12-pk.txt',\n",
       " 'text_10-12\\\\10-12-sg.txt',\n",
       " 'text_10-12\\\\10-12-tz.txt',\n",
       " 'text_10-12\\\\10-12-us.txt',\n",
       " 'text_10-12\\\\10-12-za.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2010 = []\n",
    "for file in glob.glob('**/10*au.txt'):\n",
    "    txtfiles_au_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2011 = []\n",
    "for file in glob.glob('**/11*au.txt'):\n",
    "    txtfiles_au_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2012 = []\n",
    "for file in glob.glob('**/12*au.txt'):\n",
    "    txtfiles_au_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2013 = []\n",
    "for file in glob.glob('**/13*au.txt'):\n",
    "    txtfiles_au_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2014 = []\n",
    "for file in glob.glob('**/14*au.txt'):\n",
    "    txtfiles_au_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2015 = []\n",
    "for file in glob.glob('**/15*au.txt'):\n",
    "    txtfiles_au_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2016 = []\n",
    "for file in glob.glob('**/16*au.txt'):\n",
    "    txtfiles_au_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2017 = []\n",
    "for file in glob.glob('**/17*au.txt'):\n",
    "    txtfiles_au_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2018 = []\n",
    "for file in glob.glob('**/18*au.txt'):\n",
    "    txtfiles_au_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2019 = []\n",
    "for file in glob.glob('**/19*au.txt'):\n",
    "    txtfiles_au_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_au_2020 = []\n",
    "for file in glob.glob('**/20*au.txt'):\n",
    "    txtfiles_au_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2010 = []\n",
    "for file in glob.glob('**/10*bd.txt'):\n",
    "    txtfiles_bd_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2011 = []\n",
    "for file in glob.glob('**/11*bd.txt'):\n",
    "    txtfiles_bd_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2012 = []\n",
    "for file in glob.glob('**/12*bd.txt'):\n",
    "    txtfiles_bd_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2013 = []\n",
    "for file in glob.glob('**/13*bd.txt'):\n",
    "    txtfiles_bd_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2014 = []\n",
    "for file in glob.glob('**/14*bd.txt'):\n",
    "    txtfiles_bd_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2015 = []\n",
    "for file in glob.glob('**/15*bd.txt'):\n",
    "    txtfiles_bd_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2016 = []\n",
    "for file in glob.glob('**/16*bd.txt'):\n",
    "    txtfiles_bd_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2017 = []\n",
    "for file in glob.glob('**/17*bd.txt'):\n",
    "    txtfiles_bd_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2018 = []\n",
    "for file in glob.glob('**/18*bd.txt'):\n",
    "    txtfiles_bd_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2019 = []\n",
    "for file in glob.glob('**/19*bd.txt'):\n",
    "    txtfiles_bd_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_bd_2020 = []\n",
    "for file in glob.glob('**/20*bd.txt'):\n",
    "    txtfiles_bd_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2010 = []\n",
    "for file in glob.glob('**/10*ca.txt'):\n",
    "    txtfiles_ca_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2011 = []\n",
    "for file in glob.glob('**/11*ca.txt'):\n",
    "    txtfiles_ca_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2012 = []\n",
    "for file in glob.glob('**/12*ca.txt'):\n",
    "    txtfiles_ca_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2013 = []\n",
    "for file in glob.glob('**/13*ca.txt'):\n",
    "    txtfiles_ca_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2014 = []\n",
    "for file in glob.glob('**/14*ca.txt'):\n",
    "    txtfiles_ca_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2015 = []\n",
    "for file in glob.glob('**/15*ca.txt'):\n",
    "    txtfiles_ca_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2016 = []\n",
    "for file in glob.glob('**/16*ca.txt'):\n",
    "    txtfiles_ca_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2017 = []\n",
    "for file in glob.glob('**/17*ca.txt'):\n",
    "    txtfiles_ca_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2018 = []\n",
    "for file in glob.glob('**/18*ca.txt'):\n",
    "    txtfiles_ca_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2019 = []\n",
    "for file in glob.glob('**/19*ca.txt'):\n",
    "    txtfiles_ca_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ca_2020 = []\n",
    "for file in glob.glob('**/20*ca.txt'):\n",
    "    txtfiles_ca_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2010 = []\n",
    "for file in glob.glob('**/10*gb.txt'):\n",
    "    txtfiles_gb_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2011 = []\n",
    "for file in glob.glob('**/11*gb.txt'):\n",
    "    txtfiles_gb_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2012 = []\n",
    "for file in glob.glob('**/12*gb.txt'):\n",
    "    txtfiles_gb_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2013 = []\n",
    "for file in glob.glob('**/13*gb.txt'):\n",
    "    txtfiles_gb_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2014 = []\n",
    "for file in glob.glob('**/14*gb.txt'):\n",
    "    txtfiles_gb_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2015 = []\n",
    "for file in glob.glob('**/15*gb.txt'):\n",
    "    txtfiles_gb_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2016 = []\n",
    "for file in glob.glob('**/16*gb.txt'):\n",
    "    txtfiles_gb_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2017 = []\n",
    "for file in glob.glob('**/17*gb.txt'):\n",
    "    txtfiles_gb_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2018 = []\n",
    "for file in glob.glob('**/18*gb.txt'):\n",
    "    txtfiles_gb_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2019 = []\n",
    "for file in glob.glob('**/19*gb.txt'):\n",
    "    txtfiles_gb_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gb_2020 = []\n",
    "for file in glob.glob('**/20*gb.txt'):\n",
    "    txtfiles_gb_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2010 = []\n",
    "for file in glob.glob('**/10*gh.txt'):\n",
    "    txtfiles_gh_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2011 = []\n",
    "for file in glob.glob('**/11*gh.txt'):\n",
    "    txtfiles_gh_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2012 = []\n",
    "for file in glob.glob('**/12*gh.txt'):\n",
    "    txtfiles_gh_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2013 = []\n",
    "for file in glob.glob('**/13*gh.txt'):\n",
    "    txtfiles_gh_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2014 = []\n",
    "for file in glob.glob('**/14*gh.txt'):\n",
    "    txtfiles_gh_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2015 = []\n",
    "for file in glob.glob('**/15*gh.txt'):\n",
    "    txtfiles_gh_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2016 = []\n",
    "for file in glob.glob('**/16*gh.txt'):\n",
    "    txtfiles_gh_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2017 = []\n",
    "for file in glob.glob('**/17*gh.txt'):\n",
    "    txtfiles_gh_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2018 = []\n",
    "for file in glob.glob('**/18*gh.txt'):\n",
    "    txtfiles_gh_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2019 = []\n",
    "for file in glob.glob('**/19*gh.txt'):\n",
    "    txtfiles_gh_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_gh_2020 = []\n",
    "for file in glob.glob('**/20*gh.txt'):\n",
    "    txtfiles_gh_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2010 = []\n",
    "for file in glob.glob('**/10*hk.txt'):\n",
    "    txtfiles_hk_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2011 = []\n",
    "for file in glob.glob('**/11*hk.txt'):\n",
    "    txtfiles_hk_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2012 = []\n",
    "for file in glob.glob('**/12*hk.txt'):\n",
    "    txtfiles_hk_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2013 = []\n",
    "for file in glob.glob('**/13*hk.txt'):\n",
    "    txtfiles_hk_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2014 = []\n",
    "for file in glob.glob('**/14*hk.txt'):\n",
    "    txtfiles_hk_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2015 = []\n",
    "for file in glob.glob('**/15*hk.txt'):\n",
    "    txtfiles_hk_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2016 = []\n",
    "for file in glob.glob('**/16*hk.txt'):\n",
    "    txtfiles_hk_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2017 = []\n",
    "for file in glob.glob('**/17*hk.txt'):\n",
    "    txtfiles_hk_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2018 = []\n",
    "for file in glob.glob('**/18*hk.txt'):\n",
    "    txtfiles_hk_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2019 = []\n",
    "for file in glob.glob('**/19*hk.txt'):\n",
    "    txtfiles_hk_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_hk_2020 = []\n",
    "for file in glob.glob('**/20*hk.txt'):\n",
    "    txtfiles_hk_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2010 = []\n",
    "for file in glob.glob('**/10*ie.txt'):\n",
    "    txtfiles_ie_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2011 = []\n",
    "for file in glob.glob('**/11*ie.txt'):\n",
    "    txtfiles_ie_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2012 = []\n",
    "for file in glob.glob('**/12*ie.txt'):\n",
    "    txtfiles_ie_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2013 = []\n",
    "for file in glob.glob('**/13*ie.txt'):\n",
    "    txtfiles_ie_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2014 = []\n",
    "for file in glob.glob('**/14*ie.txt'):\n",
    "    txtfiles_ie_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2015 = []\n",
    "for file in glob.glob('**/15*ie.txt'):\n",
    "    txtfiles_ie_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2016 = []\n",
    "for file in glob.glob('**/16*ie.txt'):\n",
    "    txtfiles_ie_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2017 = []\n",
    "for file in glob.glob('**/17*ie.txt'):\n",
    "    txtfiles_ie_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2018 = []\n",
    "for file in glob.glob('**/18*ie.txt'):\n",
    "    txtfiles_ie_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2019 = []\n",
    "for file in glob.glob('**/19*ie.txt'):\n",
    "    txtfiles_ie_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ie_2020 = []\n",
    "for file in glob.glob('**/20*ie.txt'):\n",
    "    txtfiles_ie_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2010 = []\n",
    "for file in glob.glob('**/10*in.txt'):\n",
    "    txtfiles_in_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2011 = []\n",
    "for file in glob.glob('**/11*in.txt'):\n",
    "    txtfiles_in_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2012 = []\n",
    "for file in glob.glob('**/12*in.txt'):\n",
    "    txtfiles_in_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2013 = []\n",
    "for file in glob.glob('**/13*in.txt'):\n",
    "    txtfiles_in_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2014 = []\n",
    "for file in glob.glob('**/14*in.txt'):\n",
    "    txtfiles_in_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2015 = []\n",
    "for file in glob.glob('**/15*in.txt'):\n",
    "    txtfiles_in_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2016 = []\n",
    "for file in glob.glob('**/16*in.txt'):\n",
    "    txtfiles_in_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2017 = []\n",
    "for file in glob.glob('**/17*in.txt'):\n",
    "    txtfiles_in_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2018 = []\n",
    "for file in glob.glob('**/18*in.txt'):\n",
    "    txtfiles_in_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2019 = []\n",
    "for file in glob.glob('**/19*in.txt'):\n",
    "    txtfiles_in_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_in_2020 = []\n",
    "for file in glob.glob('**/20*in.txt'):\n",
    "    txtfiles_in_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2010 = []\n",
    "for file in glob.glob('**/10*jm.txt'):\n",
    "    txtfiles_jm_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2011 = []\n",
    "for file in glob.glob('**/11*jm.txt'):\n",
    "    txtfiles_jm_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2012 = []\n",
    "for file in glob.glob('**/12*jm.txt'):\n",
    "    txtfiles_jm_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2013 = []\n",
    "for file in glob.glob('**/13*jm.txt'):\n",
    "    txtfiles_jm_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2014 = []\n",
    "for file in glob.glob('**/14*jm.txt'):\n",
    "    txtfiles_jm_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2015 = []\n",
    "for file in glob.glob('**/15*jm.txt'):\n",
    "    txtfiles_jm_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2016 = []\n",
    "for file in glob.glob('**/16*jm.txt'):\n",
    "    txtfiles_jm_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2017 = []\n",
    "for file in glob.glob('**/17*jm.txt'):\n",
    "    txtfiles_jm_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2018 = []\n",
    "for file in glob.glob('**/18*jm.txt'):\n",
    "    txtfiles_jm_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2019 = []\n",
    "for file in glob.glob('**/19*jm.txt'):\n",
    "    txtfiles_jm_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_jm_2020 = []\n",
    "for file in glob.glob('**/20*jm.txt'):\n",
    "    txtfiles_jm_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2010 = []\n",
    "for file in glob.glob('**/10*ke.txt'):\n",
    "    txtfiles_ke_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2011 = []\n",
    "for file in glob.glob('**/11*ke.txt'):\n",
    "    txtfiles_ke_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2012 = []\n",
    "for file in glob.glob('**/12*ke.txt'):\n",
    "    txtfiles_ke_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2013 = []\n",
    "for file in glob.glob('**/13*ke.txt'):\n",
    "    txtfiles_ke_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2014 = []\n",
    "for file in glob.glob('**/14*ke.txt'):\n",
    "    txtfiles_ke_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2015 = []\n",
    "for file in glob.glob('**/15*ke.txt'):\n",
    "    txtfiles_ke_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2016 = []\n",
    "for file in glob.glob('**/16*ke.txt'):\n",
    "    txtfiles_ke_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2017 = []\n",
    "for file in glob.glob('**/17*ke.txt'):\n",
    "    txtfiles_ke_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2018 = []\n",
    "for file in glob.glob('**/18*ke.txt'):\n",
    "    txtfiles_ke_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2019 = []\n",
    "for file in glob.glob('**/19*ke.txt'):\n",
    "    txtfiles_ke_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ke_2020 = []\n",
    "for file in glob.glob('**/20*ke.txt'):\n",
    "    txtfiles_ke_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2010 = []\n",
    "for file in glob.glob('**/10*lk.txt'):\n",
    "    txtfiles_lk_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2011 = []\n",
    "for file in glob.glob('**/11*lk.txt'):\n",
    "    txtfiles_lk_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2012 = []\n",
    "for file in glob.glob('**/12*lk.txt'):\n",
    "    txtfiles_lk_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2013 = []\n",
    "for file in glob.glob('**/13*lk.txt'):\n",
    "    txtfiles_lk_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2014 = []\n",
    "for file in glob.glob('**/14*lk.txt'):\n",
    "    txtfiles_lk_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2015 = []\n",
    "for file in glob.glob('**/15*lk.txt'):\n",
    "    txtfiles_lk_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2016 = []\n",
    "for file in glob.glob('**/16*lk.txt'):\n",
    "    txtfiles_lk_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2017 = []\n",
    "for file in glob.glob('**/17*lk.txt'):\n",
    "    txtfiles_lk_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2018 = []\n",
    "for file in glob.glob('**/18*lk.txt'):\n",
    "    txtfiles_lk_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2019 = []\n",
    "for file in glob.glob('**/19*lk.txt'):\n",
    "    txtfiles_lk_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_lk_2020 = []\n",
    "for file in glob.glob('**/20*lk.txt'):\n",
    "    txtfiles_lk_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2010 = []\n",
    "for file in glob.glob('**/10*my.txt'):\n",
    "    txtfiles_my_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2011 = []\n",
    "for file in glob.glob('**/11*my.txt'):\n",
    "    txtfiles_my_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2012 = []\n",
    "for file in glob.glob('**/12*my.txt'):\n",
    "    txtfiles_my_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2013 = []\n",
    "for file in glob.glob('**/13*my.txt'):\n",
    "    txtfiles_my_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2014 = []\n",
    "for file in glob.glob('**/14*my.txt'):\n",
    "    txtfiles_my_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2015 = []\n",
    "for file in glob.glob('**/15*my.txt'):\n",
    "    txtfiles_my_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2016 = []\n",
    "for file in glob.glob('**/16*my.txt'):\n",
    "    txtfiles_my_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2017 = []\n",
    "for file in glob.glob('**/17*my.txt'):\n",
    "    txtfiles_my_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2018 = []\n",
    "for file in glob.glob('**/18*my.txt'):\n",
    "    txtfiles_my_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2019 = []\n",
    "for file in glob.glob('**/19*my.txt'):\n",
    "    txtfiles_my_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_my_2020 = []\n",
    "for file in glob.glob('**/20*my.txt'):\n",
    "    txtfiles_my_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2010 = []\n",
    "for file in glob.glob('**/10*ng.txt'):\n",
    "    txtfiles_ng_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2011 = []\n",
    "for file in glob.glob('**/11*ng.txt'):\n",
    "    txtfiles_ng_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2012 = []\n",
    "for file in glob.glob('**/12*ng.txt'):\n",
    "    txtfiles_ng_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2013 = []\n",
    "for file in glob.glob('**/13*ng.txt'):\n",
    "    txtfiles_ng_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2014 = []\n",
    "for file in glob.glob('**/14*ng.txt'):\n",
    "    txtfiles_ng_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2015 = []\n",
    "for file in glob.glob('**/15*ng.txt'):\n",
    "    txtfiles_ng_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2016 = []\n",
    "for file in glob.glob('**/16*ng.txt'):\n",
    "    txtfiles_ng_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2017 = []\n",
    "for file in glob.glob('**/17*ng.txt'):\n",
    "    txtfiles_ng_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2018 = []\n",
    "for file in glob.glob('**/18*ng.txt'):\n",
    "    txtfiles_ng_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2019 = []\n",
    "for file in glob.glob('**/19*ng.txt'):\n",
    "    txtfiles_ng_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ng_2020 = []\n",
    "for file in glob.glob('**/20*ng.txt'):\n",
    "    txtfiles_ng_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2010 = []\n",
    "for file in glob.glob('**/10*nz.txt'):\n",
    "    txtfiles_nz_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2011 = []\n",
    "for file in glob.glob('**/11*nz.txt'):\n",
    "    txtfiles_nz_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2012 = []\n",
    "for file in glob.glob('**/12*nz.txt'):\n",
    "    txtfiles_nz_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2013 = []\n",
    "for file in glob.glob('**/13*nz.txt'):\n",
    "    txtfiles_nz_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2014 = []\n",
    "for file in glob.glob('**/14*nz.txt'):\n",
    "    txtfiles_nz_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2015 = []\n",
    "for file in glob.glob('**/15*nz.txt'):\n",
    "    txtfiles_nz_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2016 = []\n",
    "for file in glob.glob('**/16*nz.txt'):\n",
    "    txtfiles_nz_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2017 = []\n",
    "for file in glob.glob('**/17*nz.txt'):\n",
    "    txtfiles_nz_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2018 = []\n",
    "for file in glob.glob('**/18*nz.txt'):\n",
    "    txtfiles_nz_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2019 = []\n",
    "for file in glob.glob('**/19*nz.txt'):\n",
    "    txtfiles_nz_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_nz_2020 = []\n",
    "for file in glob.glob('**/20*nz.txt'):\n",
    "    txtfiles_nz_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2010 = []\n",
    "for file in glob.glob('**/10*ph.txt'):\n",
    "    txtfiles_ph_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2011 = []\n",
    "for file in glob.glob('**/11*ph.txt'):\n",
    "    txtfiles_ph_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2012 = []\n",
    "for file in glob.glob('**/12*ph.txt'):\n",
    "    txtfiles_ph_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2013 = []\n",
    "for file in glob.glob('**/13*ph.txt'):\n",
    "    txtfiles_ph_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2014 = []\n",
    "for file in glob.glob('**/14*ph.txt'):\n",
    "    txtfiles_ph_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2015 = []\n",
    "for file in glob.glob('**/15*ph.txt'):\n",
    "    txtfiles_ph_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2016 = []\n",
    "for file in glob.glob('**/16*ph.txt'):\n",
    "    txtfiles_ph_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2017 = []\n",
    "for file in glob.glob('**/17*ph.txt'):\n",
    "    txtfiles_ph_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2018 = []\n",
    "for file in glob.glob('**/18*ph.txt'):\n",
    "    txtfiles_ph_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2019 = []\n",
    "for file in glob.glob('**/19*ph.txt'):\n",
    "    txtfiles_ph_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_ph_2020 = []\n",
    "for file in glob.glob('**/20*ph.txt'):\n",
    "    txtfiles_ph_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2010 = []\n",
    "for file in glob.glob('**/10*pk.txt'):\n",
    "    txtfiles_pk_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2011 = []\n",
    "for file in glob.glob('**/11*pk.txt'):\n",
    "    txtfiles_pk_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2012 = []\n",
    "for file in glob.glob('**/12*pk.txt'):\n",
    "    txtfiles_pk_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2013 = []\n",
    "for file in glob.glob('**/13*pk.txt'):\n",
    "    txtfiles_pk_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2014 = []\n",
    "for file in glob.glob('**/14*pk.txt'):\n",
    "    txtfiles_pk_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2015 = []\n",
    "for file in glob.glob('**/15*pk.txt'):\n",
    "    txtfiles_pk_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2016 = []\n",
    "for file in glob.glob('**/16*pk.txt'):\n",
    "    txtfiles_pk_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2017 = []\n",
    "for file in glob.glob('**/17*pk.txt'):\n",
    "    txtfiles_pk_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2018 = []\n",
    "for file in glob.glob('**/18*pk.txt'):\n",
    "    txtfiles_pk_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2019 = []\n",
    "for file in glob.glob('**/19*pk.txt'):\n",
    "    txtfiles_pk_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_pk_2020 = []\n",
    "for file in glob.glob('**/20*pk.txt'):\n",
    "    txtfiles_pk_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2010 = []\n",
    "for file in glob.glob('**/10*sg.txt'):\n",
    "    txtfiles_sg_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2011 = []\n",
    "for file in glob.glob('**/11*sg.txt'):\n",
    "    txtfiles_sg_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2012 = []\n",
    "for file in glob.glob('**/12*sg.txt'):\n",
    "    txtfiles_sg_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2013 = []\n",
    "for file in glob.glob('**/13*sg.txt'):\n",
    "    txtfiles_sg_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2014 = []\n",
    "for file in glob.glob('**/14*sg.txt'):\n",
    "    txtfiles_sg_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2015 = []\n",
    "for file in glob.glob('**/15*sg.txt'):\n",
    "    txtfiles_sg_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2016 = []\n",
    "for file in glob.glob('**/16*sg.txt'):\n",
    "    txtfiles_sg_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2017 = []\n",
    "for file in glob.glob('**/17*sg.txt'):\n",
    "    txtfiles_sg_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2018 = []\n",
    "for file in glob.glob('**/18*sg.txt'):\n",
    "    txtfiles_sg_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2019 = []\n",
    "for file in glob.glob('**/19*sg.txt'):\n",
    "    txtfiles_sg_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_sg_2020 = []\n",
    "for file in glob.glob('**/20*sg.txt'):\n",
    "    txtfiles_sg_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2010 = []\n",
    "for file in glob.glob('**/10*tz.txt'):\n",
    "    txtfiles_tz_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2011 = []\n",
    "for file in glob.glob('**/11*tz.txt'):\n",
    "    txtfiles_tz_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2012 = []\n",
    "for file in glob.glob('**/12*tz.txt'):\n",
    "    txtfiles_tz_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2013 = []\n",
    "for file in glob.glob('**/13*tz.txt'):\n",
    "    txtfiles_tz_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2014 = []\n",
    "for file in glob.glob('**/14*tz.txt'):\n",
    "    txtfiles_tz_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2015 = []\n",
    "for file in glob.glob('**/15*tz.txt'):\n",
    "    txtfiles_tz_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2016 = []\n",
    "for file in glob.glob('**/16*tz.txt'):\n",
    "    txtfiles_tz_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2017 = []\n",
    "for file in glob.glob('**/17*tz.txt'):\n",
    "    txtfiles_tz_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2018 = []\n",
    "for file in glob.glob('**/18*tz.txt'):\n",
    "    txtfiles_tz_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2019 = []\n",
    "for file in glob.glob('**/19*tz.txt'):\n",
    "    txtfiles_tz_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tz_2020 = []\n",
    "for file in glob.glob('**/20*tz.txt'):\n",
    "    txtfiles_tz_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2010 = []\n",
    "for file in glob.glob('**/10*us.txt'):\n",
    "    txtfiles_us_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2011 = []\n",
    "for file in glob.glob('**/11*us.txt'):\n",
    "    txtfiles_us_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2012 = []\n",
    "for file in glob.glob('**/12*us.txt'):\n",
    "    txtfiles_us_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2013 = []\n",
    "for file in glob.glob('**/13*us.txt'):\n",
    "    txtfiles_us_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2014 = []\n",
    "for file in glob.glob('**/14*us.txt'):\n",
    "    txtfiles_us_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2015 = []\n",
    "for file in glob.glob('**/15*us.txt'):\n",
    "    txtfiles_us_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2016 = []\n",
    "for file in glob.glob('**/16*us.txt'):\n",
    "    txtfiles_us_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2017 = []\n",
    "for file in glob.glob('**/17*us.txt'):\n",
    "    txtfiles_us_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2018 = []\n",
    "for file in glob.glob('**/18*us.txt'):\n",
    "    txtfiles_us_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2019 = []\n",
    "for file in glob.glob('**/19*us.txt'):\n",
    "    txtfiles_us_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_us_2020 = []\n",
    "for file in glob.glob('**/20*us.txt'):\n",
    "    txtfiles_us_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2010 = []\n",
    "for file in glob.glob('**/10*za.txt'):\n",
    "    txtfiles_za_2010.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2011 = []\n",
    "for file in glob.glob('**/11*za.txt'):\n",
    "    txtfiles_za_2011.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2012 = []\n",
    "for file in glob.glob('**/12*za.txt'):\n",
    "    txtfiles_za_2012.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2013 = []\n",
    "for file in glob.glob('**/13*za.txt'):\n",
    "    txtfiles_za_2013.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2014 = []\n",
    "for file in glob.glob('**/14*za.txt'):\n",
    "    txtfiles_za_2014.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2015 = []\n",
    "for file in glob.glob('**/15*za.txt'):\n",
    "    txtfiles_za_2015.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2016 = []\n",
    "for file in glob.glob('**/16*za.txt'):\n",
    "    txtfiles_za_2016.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2017 = []\n",
    "for file in glob.glob('**/17*za.txt'):\n",
    "    txtfiles_za_2017.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2018 = []\n",
    "for file in glob.glob('**/18*za.txt'):\n",
    "    txtfiles_za_2018.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2019 = []\n",
    "for file in glob.glob('**/19*za.txt'):\n",
    "    txtfiles_za_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_za_2020 = []\n",
    "for file in glob.glob('**/20*za.txt'):\n",
    "    txtfiles_za_2020.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_10-01\\\\10-01-au.txt',\n",
       " 'text_10-02\\\\10-02-au.txt',\n",
       " 'text_10-03\\\\10-03-au.txt',\n",
       " 'text_10-04\\\\10-04-au.txt',\n",
       " 'text_10-05\\\\10-05-au.txt',\n",
       " 'text_10-06\\\\10-06-au.txt',\n",
       " 'text_10-07\\\\10-07-au.txt',\n",
       " 'text_10-08\\\\10-08-au.txt',\n",
       " 'text_10-09\\\\10-09-au.txt',\n",
       " 'text_10-10\\\\10-10-au.txt',\n",
       " 'text_10-11\\\\10-11-au.txt',\n",
       " 'text_10-12\\\\10-12-au.txt']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_au_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_12-01\\\\12-01-bd.txt',\n",
       " 'text_12-02\\\\12-02-bd.txt',\n",
       " 'text_12-03\\\\12-03-bd.txt',\n",
       " 'text_12-04\\\\12-04-bd.txt',\n",
       " 'text_12-05\\\\12-05-bd.txt',\n",
       " 'text_12-06\\\\12-06-bd.txt',\n",
       " 'text_12-07\\\\12-07-bd.txt',\n",
       " 'text_12-08\\\\12-08-bd.txt',\n",
       " 'text_12-09\\\\12-09-bd.txt',\n",
       " 'text_12-10\\\\12-10-bd.txt',\n",
       " 'text_12-11\\\\12-11-bd.txt',\n",
       " 'text_12-12\\\\12-12-bd.txt']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_bd_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_10-08\\10-08-au.txt\n"
     ]
    }
   ],
   "source": [
    "print(txtfiles_au_2010[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re, random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "df_2010 = pd.DataFrame()\n",
    "df_2011 = pd.DataFrame()\n",
    "df_2012 = pd.DataFrame()\n",
    "df_2013 = pd.DataFrame()\n",
    "df_2014 = pd.DataFrame()\n",
    "df_2015 = pd.DataFrame()\n",
    "df_2016 = pd.DataFrame()\n",
    "df_2017 = pd.DataFrame()\n",
    "df_2018 = pd.DataFrame()\n",
    "df_2019 = pd.DataFrame()\n",
    "df_2020 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_au_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2010 = Counter(x for sublist in wordfreq for x in sublist)\n",
    "    \n",
    "# f = open(txtfiles_ca[7], \"r\")\n",
    "# article1 = f.read()\n",
    "# print(article1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_au_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_au_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_au_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_au_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_au_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_au_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-238-6722eafae8e5>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_au_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_au_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-239-810b0ced427f>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_au_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_au_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-240-3b570509e9ab>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_au_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_au_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-241-57ac1042112b>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_au_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_au_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_au_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_au_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_au_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_bd_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_bd_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_bd_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_bd_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_bd_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_bd_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_bd_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-249-6a4c7fa9a4f9>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_bd_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_bd_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-d1ee73767eed>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_bd_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_bd_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-251-b6986db1546b>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_bd_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_bd_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-252-3ac65198a67d>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_bd_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_bd_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_bd_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_bd_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_bd_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ca_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ca_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ca_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ca_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ca_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ca_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ca_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-260-c559aab9fa96>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_ca_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ca_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-261-7acad75883ed>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_ca_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ca_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-262-92d4ba0814e7>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_ca_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ca_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-263-e666cf628315>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_ca_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ca_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ca_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ca_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ca_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_gb_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_gb_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_gb_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_gb_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_gb_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_gb_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_gb_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-271-d721f346af9e>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_gb_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_gb_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-272-ba0f3ccf74a1>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_gb_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_gb_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-273-ac8b33aca5ea>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_gb_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_gb_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-274-d62b6f38a86f>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_gb_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gb_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_gb_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gb_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gb_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_gh_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_gh_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_gh_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_gh_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_gh_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_gh_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_gh_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-282-d4ebba95a316>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_gh_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_gh_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-283-9d18a2f7575b>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_gh_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_gh_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-284-6c7e75e9a1f9>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_gh_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_gh_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-285-e21890a38587>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_gh_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_gh_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_gh_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_gh_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_gh_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_hk_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_hk_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_hk_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_hk_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_hk_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_hk_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_hk_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-293-51dbc51297ac>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_hk_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_hk_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-294-8abb19d4636b>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_hk_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_hk_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-295-6ad5a917dad7>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_hk_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_hk_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-296-dfda938bfca5>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_hk_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_hk_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_hk_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_hk_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_hk_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ie_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ie_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ie_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ie_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ie_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ie_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ie_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-304-572e715c3733>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_ie_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ie_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-305-0da94ca3d506>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_ie_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ie_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-306-8b001e06eab6>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_ie_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ie_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-307-ca8bec5ab173>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_ie_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ie_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ie_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ie_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ie_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_in_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_in_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_in_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_in_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_in_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_in_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_in_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-315-9c3bf72896a6>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_in_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_in_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-316-45a070888c15>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_in_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_in_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-317-a14683028e17>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_in_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_in_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-318-9fcdd0751f7a>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_in_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_in_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_in_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_in_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_in_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_jm_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_jm_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_jm_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_jm_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_jm_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_jm_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_jm_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-326-27ab4f8589b8>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_jm_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_jm_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-327-11412362e19a>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_jm_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_jm_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-328-310a9c014e9d>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_jm_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_jm_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-329-f3713694a9ad>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_jm_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_jm_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_jm_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_jm_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_jm_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ke_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ke_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ke_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ke_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ke_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ke_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ke_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-337-96b7b9b2939f>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_ke_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ke_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-338-0a224e884698>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_ke_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ke_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-339-d6dfadd5e55d>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_ke_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ke_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-340-d908165dfa47>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_ke_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ke_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ke_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ke_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ke_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_lk_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_lk_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_lk_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_lk_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_lk_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_lk_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_lk_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-348-46b330632f1f>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_lk_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_lk_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-349-cc75f82ef88f>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_lk_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_lk_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-350-f69b5ea9e3a6>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_lk_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_lk_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-351-2066a80842bc>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_lk_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_lk_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_lk_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_lk_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_lk_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_my_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_my_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_my_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_my_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_my_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_my_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_my_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-359-e407b65d4da4>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_my_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_my_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-360-e83daba252b4>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_my_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_my_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-361-929f9d26a66c>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_my_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_my_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-362-9f8eb7bd4d5d>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_my_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_my_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_my_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_my_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_my_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ng_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ng_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ng_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ng_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ng_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ng_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ng_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-370-969306ea6081>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_ng_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ng_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-371-203e6389d0a1>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_ng_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ng_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-372-1b0f511abedd>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_ng_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ng_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-373-24251e5b50ab>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_ng_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ng_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ng_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ng_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ng_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_nz_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_nz_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_nz_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_nz_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_nz_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_nz_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_nz_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-381-639319ffca1c>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_nz_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_nz_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-382-2842d9e5a2f3>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_nz_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_nz_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-383-b92fc28dcc53>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_nz_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_nz_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-384-96f88034e010>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_nz_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_nz_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_nz_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_nz_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_nz_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_ph_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_ph_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_ph_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_ph_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_ph_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_ph_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_ph_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-392-03843be60f83>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_ph_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_ph_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-393-b00629bd3be9>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_ph_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_ph_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-394-a4b438b6071a>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_ph_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_ph_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-395-fc1bcfabb0da>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_ph_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_ph_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_ph_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_ph_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_ph_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_pk_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_pk_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_pk_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_pk_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_pk_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_pk_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_pk_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-403-3ecf72d97f05>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_pk_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_pk_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-404-d908aefda490>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_pk_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_pk_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-405-6a3fff499d33>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_pk_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_pk_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-406-4eefcb090539>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_pk_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_pk_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_pk_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_pk_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_pk_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_sg_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_sg_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_sg_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_sg_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_sg_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_sg_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_sg_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-414-041cce49dd29>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_sg_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_sg_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-415-126650c51212>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_sg_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_sg_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-416-04ee8c0934cd>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_sg_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_sg_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-417-eb544e94a4fe>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_sg_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_sg_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_sg_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_sg_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_sg_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_tz_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_tz_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_tz_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_tz_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_tz_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_tz_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_tz_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-425-1adf11536ab5>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_tz_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_tz_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-426-d77a7aa08551>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_tz_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_tz_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-427-32a3475b9a34>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_tz_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_tz_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-428-519984b344ca>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_tz_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_tz_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_tz_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_tz_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_tz_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_us_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_us_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_us_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_us_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_us_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_us_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_us_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-436-0b20462dd8d4>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_us_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_us_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-437-12f043066cf2>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_us_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_us_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-438-4a98b300921b>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_us_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_us_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-439-d196b2ebe899>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_us_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_us_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_us_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_us_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_us_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2010:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2010['txtfiles_za_2010'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2010 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2011:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2011['txtfiles_za_2011'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2011 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2011 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2012:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2012['txtfiles_za_2012'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2012 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2012 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2013:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2013['txtfiles_za_2013'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2013 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2013 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2014:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2014['txtfiles_za_2014'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2014 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2014 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2015:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2015['txtfiles_za_2015'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2015 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2015 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2016:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2016['txtfiles_za_2016'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2016 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2016 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-447-16edf22ef0c7>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2017['txtfiles_za_2017'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2017:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2017['txtfiles_za_2017'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2017 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2017 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-448-ddda711e4d0b>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2018['txtfiles_za_2018'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2018:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2018['txtfiles_za_2018'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2018 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2018 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-449-50a445119927>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2019['txtfiles_za_2019'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2019:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2019['txtfiles_za_2019'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2019 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2019 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-450-6353f5b26446>:17: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df_2020['txtfiles_za_2020'] = pd.Series(items)\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "\n",
    "for i in txtfiles_za_2020:\n",
    "    f = open(i, \"r\")\n",
    "    article1 = f.read()\n",
    "    \n",
    "    new1=article1.replace('','\"')\n",
    "\n",
    "    new2=new1.replace('','\"')\n",
    "\n",
    "    new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "    quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "    items.append(quo)\n",
    "\n",
    "df_2020['txtfiles_za_2020'] = pd.Series(items)\n",
    "\n",
    "wordfreq = []\n",
    "counts_za_2020 = {}\n",
    "for article in items:\n",
    "    for quotes in article:\n",
    "        words = quotes.lower().split()\n",
    "        wordfreq.append(words)\n",
    "counts_za_2020 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2010</th>\n",
       "      <th>txtfiles_bd_2010</th>\n",
       "      <th>txtfiles_ca_2010</th>\n",
       "      <th>txtfiles_gb_2010</th>\n",
       "      <th>txtfiles_gh_2010</th>\n",
       "      <th>txtfiles_hk_2010</th>\n",
       "      <th>txtfiles_ie_2010</th>\n",
       "      <th>txtfiles_in_2010</th>\n",
       "      <th>txtfiles_jm_2010</th>\n",
       "      <th>txtfiles_ke_2010</th>\n",
       "      <th>txtfiles_lk_2010</th>\n",
       "      <th>txtfiles_my_2010</th>\n",
       "      <th>txtfiles_ng_2010</th>\n",
       "      <th>txtfiles_nz_2010</th>\n",
       "      <th>txtfiles_ph_2010</th>\n",
       "      <th>txtfiles_pk_2010</th>\n",
       "      <th>txtfiles_sg_2010</th>\n",
       "      <th>txtfiles_tz_2010</th>\n",
       "      <th>txtfiles_us_2010</th>\n",
       "      <th>txtfiles_za_2010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Do you want @ @ @ @ @ @ @ @ @ @ in it ? ... ...</td>\n",
       "      <td>[ experts . ,  It is not possible for us to ta...</td>\n",
       "      <td>[ Every kid should be able to do things that a...</td>\n",
       "      <td>[ obesity time-bomb ,  Labour has neglected th...</td>\n",
       "      <td>[ Sounds good ! I went on to read the ingredie...</td>\n",
       "      <td>[ happiest cities ,  human touch , ,  peace th...</td>\n",
       "      <td>[ Much of the growth seen and expected in the ...</td>\n",
       "      <td>[ If we wish to be free , we must fight . Shal...</td>\n",
       "      <td>[ of that mysterious organ found between a wom...</td>\n",
       "      <td>[ He is persona non grata here in Kenya , ,  s...</td>\n",
       "      <td>[ Endemic biodiversity in Sri Lanka is excepti...</td>\n",
       "      <td>[ Caddy that zigs ,  Following the accomplishm...</td>\n",
       "      <td>[ SAPELE town in Delta State was thrown into c...</td>\n",
       "      <td>[ framed ,  but because of the political calcu...</td>\n",
       "      <td>[ Pacman ,  No I would n't take it are you cra...</td>\n",
       "      <td>[ said Superintendent of Police , Ratanlal Dan...</td>\n",
       "      <td>[ opposition to the actions of the Government ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ We need to stack the gear closely to get the...</td>\n",
       "      <td>[ The interest in both players is growing and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ Because of the side effects and because only...</td>\n",
       "      <td>[  Khan Brothers , which plans to develop the ...</td>\n",
       "      <td>[ It was like , ' Uh-oh , what am I doing here...</td>\n",
       "      <td>[ charged out ,  It 's not like he dawdles . H...</td>\n",
       "      <td>[ @ @ @ @ @ @ @ @ @ @ rapidly improved . ,  ta...</td>\n",
       "      <td>[ Wo ,  News 1+1 ,  ' Fake Donation-Gate ' Tur...</td>\n",
       "      <td>[ This is me out of shape , ,  I knew this guy...</td>\n",
       "      <td>[ people are entitled to participate in , cont...</td>\n",
       "      <td>[ At 18 I just got caught up on the wrong side...</td>\n",
       "      <td>[ If you rattle a snake , you must be prepared...</td>\n",
       "      <td>[ The hasty decision was taken by Mulayam Sing...</td>\n",
       "      <td>[ I have painted different styles of ancient b...</td>\n",
       "      <td>[ Peter Obi of the All Progressives Grand Alli...</td>\n",
       "      <td>[ copper loop ,  node ,  myth ,  The Big Fat T...</td>\n",
       "      <td>[ Laki sa Gatas ,  The Laki sa Gatas team hold...</td>\n",
       "      <td>[ Normal Wudu , an individual uses about 8-10 ...</td>\n",
       "      <td>[ too dangerous ,  If I start sending her to s...</td>\n",
       "      <td>[ We have received the green light : TIMM can ...</td>\n",
       "      <td>[ minder ,  total ,  Constitution , ,  ratifie...</td>\n",
       "      <td>[ The burden of history : The legacy of the ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ at work , ,  distractions ,  fucking tourist...</td>\n",
       "      <td>[ Digital Bangladesh ,  Citizen 's help reques...</td>\n",
       "      <td>[ These are staggering numbers , ,  It @ @ @ @...</td>\n",
       "      <td>[ My character is this and my character is tha...</td>\n",
       "      <td>[ the independence of Ghana is meaningless unl...</td>\n",
       "      <td>[ for a cleaner , healthier future . ,  Today ...</td>\n",
       "      <td>[  Paul Flynn is one of Ireland 's top chefs ....</td>\n",
       "      <td>[ a nation that has been plunged into barbaris...</td>\n",
       "      <td>[ the shortages of nurses will only continue t...</td>\n",
       "      <td>[ We are doing this translocation to restock A...</td>\n",
       "      <td>[ firsts ,  have gangsters close to you if you...</td>\n",
       "      <td>[ We 're fully booked until year-end , ,  Peop...</td>\n",
       "      <td>[ women in developing world face numerous prob...</td>\n",
       "      <td>[ Can Telecom survive ,  We bought the GSM spe...</td>\n",
       "      <td>[ El Terrible ,  The Champion Returns ,  The C...</td>\n",
       "      <td>[ Strengthening Women 's Strategic Use of Info...</td>\n",
       "      <td>[ We are not offering 3D as a niche product , ...</td>\n",
       "      <td>[ Freesat ,  The Bill will help broadcasters p...</td>\n",
       "      <td>[ Meet me at the bar and tell me what 's going...</td>\n",
       "      <td>[ said Touchline Media Group Publisher , Desir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ classical ,  The Foggy Dew ,  The Foggy Dew ...</td>\n",
       "      <td>[ Now is the Son of Man glorified and God is g...</td>\n",
       "      <td>[ embarrassing ,  Top 5 reasons to attend Rotm...</td>\n",
       "      <td>[ 360 ? Time Slice ,  Release the Kraken ! ,  ...</td>\n",
       "      <td>[ universal recipeint ,  almost dropped it in ...</td>\n",
       "      <td>[ Walking on Air ,  I am confident that the di...</td>\n",
       "      <td>[ Gooliegate ,  .  He has just opened the Gran...</td>\n",
       "      <td>[ unconstitutional ,  rates over and above the...</td>\n",
       "      <td>[ warmth and friendliness of its people , ,  M...</td>\n",
       "      <td>[ You have to run under 2:10 , ,  At 2:10 you ...</td>\n",
       "      <td>[ establishing marketing as the driving force ...</td>\n",
       "      <td>[ Over the past 18 years , we have witnessed c...</td>\n",
       "      <td>[ as is ,  Opt out of Registration Information...</td>\n",
       "      <td>[ This will spell the end of the current monar...</td>\n",
       "      <td>[ said Panoncillo .  Meanwhile , this will be ...</td>\n",
       "      <td>[ Our thoughts and prayers are with his devote...</td>\n",
       "      <td>[ regards to the queries that you have raised ...</td>\n",
       "      <td>[ strategic ,  the type of minerals and the le...</td>\n",
       "      <td>[ better living through chemistry ,  It 's not...</td>\n",
       "      <td>[ freeconomics ,  People thought I was crazy l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ flash ,  This appears to be a car bomb that ...</td>\n",
       "      <td>[ It is totally unexpected and unacceptable , ...</td>\n",
       "      <td>[ factory towns ,  Sir John is Dead ,  The Old...</td>\n",
       "      <td>[ antiseptic architecture ,  The rules around ...</td>\n",
       "      <td>[ real ,  widely practiced in the Volta Region...</td>\n",
       "      <td>[ tank ,  Expo Shanghai 2010 , ,  Spirit and G...</td>\n",
       "      <td>[ junk ' ' this week .  Compared to that , Ire...</td>\n",
       "      <td>[ We are trying our best to make ourselves hea...</td>\n",
       "      <td>[ a rapture of succulence , sweetness and tend...</td>\n",
       "      <td>[ Unfinished ,  I have heard about that group ...</td>\n",
       "      <td>[ They impose high taxes on newsprint , making...</td>\n",
       "      <td>[ are ,  who ,  women ,   -- Adam  1 . Adam , ...</td>\n",
       "      <td>[ fashion film ,  sneak peek ,  You already kn...</td>\n",
       "      <td>[ . \\n@@1470479  WIGAN committee is swamped ea...</td>\n",
       "      <td>[ Galing at Talino . ,  very intelligent , ,  ...</td>\n",
       "      <td>[ It is not that we are particularly fond of B...</td>\n",
       "      <td>[ entry level ,  test water ,  race ,  race , ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ There 's lots of evidence that kids immersed...</td>\n",
       "      <td>[ Some people may not be measuring up to the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ possibly collected in Tasmania by early Euro...</td>\n",
       "      <td>[ he said . \\n@@4515035  Jamrul tree grows up ...</td>\n",
       "      <td>[ better ,  evolution not revolution . ,  norm...</td>\n",
       "      <td>[ protect my goods , segregated community , go...</td>\n",
       "      <td>[ Falling Standards Of Education ,  National A...</td>\n",
       "      <td>[ First Pacific ,  the Company ,  on-market re...</td>\n",
       "      <td>[ There is no other known photo of Bobby ( Ken...</td>\n",
       "      <td>[ constabulary ,  white elephants ,  distant c...</td>\n",
       "      <td>[ Grant ,  IEPCA ,  spouse ,  a single woman w...</td>\n",
       "      <td>[ Before this , Kenya had almost the range of ...</td>\n",
       "      <td>[ we sell or else . ,  \\n@@1510513 &lt;h&gt; 3D TV n...</td>\n",
       "      <td>[ ego trips ,  in its own mould ,  Unless thes...</td>\n",
       "      <td>[ beware of small things that start big ,  bre...</td>\n",
       "      <td>[ Leading jumps jockey Tommy Hazlett reckons h...</td>\n",
       "      <td>[ forever ,  undying devotion ,  taken in mode...</td>\n",
       "      <td>[ Stage plays have always been a highlight of ...</td>\n",
       "      <td>[ Singapore 's History : Who Writes The Script...</td>\n",
       "      <td>[ AsiaSat 3S continues @ @ @ @ @ @ @ @ @ @ sat...</td>\n",
       "      <td>[ aha moment , ,  changing the breadth of visu...</td>\n",
       "      <td>[ I am very excited although at this stage I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ Nino ,  stealth mode ,  allow a program or f...</td>\n",
       "      <td>[  A litre of low-quality lubricant costs only...</td>\n",
       "      <td>[ citizen experts . ,  national church ,  Cana...</td>\n",
       "      <td>[ Nature is a dictionary , ,  one draws words ...</td>\n",
       "      <td>[ Invectus ,  How do we inspire ourselves @ @ ...</td>\n",
       "      <td>[ This is the first time in my over three deca...</td>\n",
       "      <td>[ There has been a growing body of evidence ov...</td>\n",
       "      <td>[ Twelve years ago , I purchased 15 acres land...</td>\n",
       "      <td>[ Other monetary seizures under POCA for the s...</td>\n",
       "      <td>[ In the spirit of the East African Community ...</td>\n",
       "      <td>[ Rajatha Dekma ,  The airline industry has sh...</td>\n",
       "      <td>[ toy kingdom ,  toy kingdom , ,  This boy lov...</td>\n",
       "      <td>[ the one the court recognises , Gadi , and Ga...</td>\n",
       "      <td>[ This trend is not sustainable and needs to b...</td>\n",
       "      <td>[ pianist 's pianist ,  SIR D ,  Better TWOget...</td>\n",
       "      <td>[ Khudi Ke Basti ,  Menu Teray Jeya Sohna Hor ...</td>\n",
       "      <td>[ We 've yet to push people into the train ,  ...</td>\n",
       "      <td>[ The Hub HD is a great new addition to the Fi...</td>\n",
       "      <td>[ the American K-12 system is a failure , but ...</td>\n",
       "      <td>[ Stuart 's mother 's ,  I Heart Mulholland 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ Socialism in the 21st century ,  How 's it g...</td>\n",
       "      <td>[ Wild animals are always on the run as poache...</td>\n",
       "      <td>[ general basket description that causes all k...</td>\n",
       "      <td>[ The water in the channels is denser than the...</td>\n",
       "      <td>[ TOR Debt ,  insider ,  GHAIP ,  Tema Oil Ref...</td>\n",
       "      <td>[ The large amount of waste in the dam area co...</td>\n",
       "      <td>[ and ca n't you feel the chill factor already...</td>\n",
       "      <td>[ deterrent effect ,  rarest of rare ,  person...</td>\n",
       "      <td>[ make a little brown baby ,  auto-racism ,  w...</td>\n",
       "      <td>[ Our target is women aged between 25 and 40 a...</td>\n",
       "      <td>[ On or about 10 July 2010 a midwife ( Family ...</td>\n",
       "      <td>[ I regard today 's victory as the most import...</td>\n",
       "      <td>[ AT least 61 prominent Deltans including one-...</td>\n",
       "      <td>[ staggering ,  Sometimes we do n't totally @ ...</td>\n",
       "      <td>[ wand of light ,  Filipinos should address th...</td>\n",
       "      <td>[ The water level in the lake is rising . The ...</td>\n",
       "      <td>[ Adult Movies , ,  pink ,  Female customers c...</td>\n",
       "      <td>[ The one where the guy is staring at the girl...</td>\n",
       "      <td>[ Buster ,  Black in America : Churched , ,  T...</td>\n",
       "      <td>[ There is no way we can be held responsible f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ thank you ,  There is always this very marke...</td>\n",
       "      <td>[ I do n't like to live in Dhaka , ,  But we h...</td>\n",
       "      <td>[ Bad , Bad news from Toronto re : #TIFF10 : T...</td>\n",
       "      <td>[ You idiot . You naive , foolish , irresponsi...</td>\n",
       "      <td>[ one disease ,  what strategies and mechanism...</td>\n",
       "      <td>[ As we have been demonstrating this past week...</td>\n",
       "      <td>[ in the interest of providing the highest qua...</td>\n",
       "      <td>[ I wish I had more troubles . You were always...</td>\n",
       "      <td>[ Many would wonder how this all started or ho...</td>\n",
       "      <td>[ Farmers are currently getting about two metr...</td>\n",
       "      <td>[ Farmers ' Week ,  pooja ,  over political ve...</td>\n",
       "      <td>[ on the day of competition ,  real ,  ready f...</td>\n",
       "      <td>[ The investment you have made to the universi...</td>\n",
       "      <td>[ There were ice cream cones coming out of the...</td>\n",
       "      <td>[ actors ,  sharpen ,  I will retain for the i...</td>\n",
       "      <td>[ people 's representatives ,  one of the firs...</td>\n",
       "      <td>[ Marina Bay Sands has completed quite a numbe...</td>\n",
       "      <td>[ PCCW has led the way in Asia 's IPTV market ...</td>\n",
       "      <td>[ family ,  We have moved from what 's most id...</td>\n",
       "      <td>[ DMR 's ' ) proposed amendments to South Afri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ thank you ,  unsubstantiated ,  bizarre ,  a...</td>\n",
       "      <td>[ This combination creates a top tier global t...</td>\n",
       "      <td>[ life affirming ,  I subsisted on Pop-Tarts a...</td>\n",
       "      <td>[ A man came to me and put a life preserver on...</td>\n",
       "      <td>[ given teachers a worthy alternative in the f...</td>\n",
       "      <td>[ persuade ,  This would reduce China 's globa...</td>\n",
       "      <td>[ progressing well ,  A substantial number of ...</td>\n",
       "      <td>[ no Hindu temple had ever came under attack i...</td>\n",
       "      <td>[ If you eat the common fowl that eat natural ...</td>\n",
       "      <td>[ Following a recent review , the Board of the...</td>\n",
       "      <td>[ I initially dedicated this place to Jesus Ch...</td>\n",
       "      <td>[ a simple woman who did not seek for fame and...</td>\n",
       "      <td>[ Centre of Excellence ,  law school ,  A law ...</td>\n",
       "      <td>[ possibility that the All Blacks will lose ha...</td>\n",
       "      <td>[ So far , ABS-CBN is dominant , ,  I 'm happy...</td>\n",
       "      <td>[ Financing quality basic education in Pakista...</td>\n",
       "      <td>[ baby-dumping ,  The government ... has start...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ how can I engineer this to work better ? ,  ...</td>\n",
       "      <td>[ It 's a crisis . There 's total anarchy , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ uncertain ,  think tanks ,  institutes ,  co...</td>\n",
       "      <td>[ The anti-liberation forces killed the four n...</td>\n",
       "      <td>[ The government must make it a priority to fi...</td>\n",
       "      <td>[ Around the time that I recorded the first FF...</td>\n",
       "      <td>[ Kotoko best club of the century ,  routines ...</td>\n",
       "      <td>[ Fading consumer demand is an unpleasant surp...</td>\n",
       "      <td>[ foreign ,  We would normally buy a large qua...</td>\n",
       "      <td>[ recognised unions ,  certification of a barg...</td>\n",
       "      <td>[ Album of the Century ,  Song of the Century ...</td>\n",
       "      <td>[ I helped Mr Nthurima here , to be elected as...</td>\n",
       "      <td>[ The Education Ministry has told us to award ...</td>\n",
       "      <td>[ gel ,  wow ,  If it 's good for the team ......</td>\n",
       "      <td>[ window ,  we have filed a preliminary suit o...</td>\n",
       "      <td>[ I forgot to tell you , ,  we are donating so...</td>\n",
       "      <td>[ math ,  maths , ,  sports ,  sport ,  orient...</td>\n",
       "      <td>[  No progressive thinker of today , he argued...</td>\n",
       "      <td>[ I feel that there 's a sense , there 's a di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ commercial and manufacturing supremacy , ,  ...</td>\n",
       "      <td>[ This is my last official public function . B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[ Pick your best customers @ @ @ @ @ @ @ @ @ @...</td>\n",
       "      <td>[  ,  said the minister .  Matia also disclose...</td>\n",
       "      <td>[ Now in the world , people are confused with ...</td>\n",
       "      <td>[ the owner of the Sun , a star of spectral ty...</td>\n",
       "      <td>[ It came as a little bit of surprise because ...</td>\n",
       "      <td>[ Designed in California , USA ,  It 's like y...</td>\n",
       "      <td>[ temporary wife ,  victim of a misogynous soc...</td>\n",
       "      <td>[ The transcripts suggest @ @ @ @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ This remarkable breakthrough comes at a time...</td>\n",
       "      <td>[ reflect a new era for the sport in the count...</td>\n",
       "      <td>[ Uththama Pooja Prenama medal ,  Down with an...</td>\n",
       "      <td>[ art ,  Besides that , in the past , cigarett...</td>\n",
       "      <td>[ a live human-made micro organism is patentab...</td>\n",
       "      <td>[ The demand-driven nature of the LBSPG ... ca...</td>\n",
       "      <td>[ We are delighted to have Hollie join our cov...</td>\n",
       "      <td>[ For the last one to two weeks , liquidity wa...</td>\n",
       "      <td>[ male Chinese pervert ,  Pahlawan City will h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ No , ice is ice and in a cocktail you can sh...</td>\n",
       "      <td>[ alleged gross transgression of Nersa 's code...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     txtfiles_au_2010  \\\n",
       "0   [ Do you want @ @ @ @ @ @ @ @ @ @ in it ? ... ...   \n",
       "1   [ Because of the side effects and because only...   \n",
       "2   [ at work , ,  distractions ,  fucking tourist...   \n",
       "3   [ classical ,  The Foggy Dew ,  The Foggy Dew ...   \n",
       "4   [ flash ,  This appears to be a car bomb that ...   \n",
       "5   [ possibly collected in Tasmania by early Euro...   \n",
       "6   [ Nino ,  stealth mode ,  allow a program or f...   \n",
       "7   [ Socialism in the 21st century ,  How 's it g...   \n",
       "8   [ thank you ,  There is always this very marke...   \n",
       "9   [ thank you ,  unsubstantiated ,  bizarre ,  a...   \n",
       "10  [ uncertain ,  think tanks ,  institutes ,  co...   \n",
       "11  [ Pick your best customers @ @ @ @ @ @ @ @ @ @...   \n",
       "\n",
       "                                     txtfiles_bd_2010  \\\n",
       "0   [ experts . ,  It is not possible for us to ta...   \n",
       "1   [  Khan Brothers , which plans to develop the ...   \n",
       "2   [ Digital Bangladesh ,  Citizen 's help reques...   \n",
       "3   [ Now is the Son of Man glorified and God is g...   \n",
       "4   [ It is totally unexpected and unacceptable , ...   \n",
       "5   [ he said . \\n@@4515035  Jamrul tree grows up ...   \n",
       "6   [  A litre of low-quality lubricant costs only...   \n",
       "7   [ Wild animals are always on the run as poache...   \n",
       "8   [ I do n't like to live in Dhaka , ,  But we h...   \n",
       "9   [ This combination creates a top tier global t...   \n",
       "10  [ The anti-liberation forces killed the four n...   \n",
       "11  [  ,  said the minister .  Matia also disclose...   \n",
       "\n",
       "                                     txtfiles_ca_2010  \\\n",
       "0   [ Every kid should be able to do things that a...   \n",
       "1   [ It was like , ' Uh-oh , what am I doing here...   \n",
       "2   [ These are staggering numbers , ,  It @ @ @ @...   \n",
       "3   [ embarrassing ,  Top 5 reasons to attend Rotm...   \n",
       "4   [ factory towns ,  Sir John is Dead ,  The Old...   \n",
       "5   [ better ,  evolution not revolution . ,  norm...   \n",
       "6   [ citizen experts . ,  national church ,  Cana...   \n",
       "7   [ general basket description that causes all k...   \n",
       "8   [ Bad , Bad news from Toronto re : #TIFF10 : T...   \n",
       "9   [ life affirming ,  I subsisted on Pop-Tarts a...   \n",
       "10  [ The government must make it a priority to fi...   \n",
       "11  [ Now in the world , people are confused with ...   \n",
       "\n",
       "                                     txtfiles_gb_2010  \\\n",
       "0   [ obesity time-bomb ,  Labour has neglected th...   \n",
       "1   [ charged out ,  It 's not like he dawdles . H...   \n",
       "2   [ My character is this and my character is tha...   \n",
       "3   [ 360 ? Time Slice ,  Release the Kraken ! ,  ...   \n",
       "4   [ antiseptic architecture ,  The rules around ...   \n",
       "5   [ protect my goods , segregated community , go...   \n",
       "6   [ Nature is a dictionary , ,  one draws words ...   \n",
       "7   [ The water in the channels is denser than the...   \n",
       "8   [ You idiot . You naive , foolish , irresponsi...   \n",
       "9   [ A man came to me and put a life preserver on...   \n",
       "10  [ Around the time that I recorded the first FF...   \n",
       "11  [ the owner of the Sun , a star of spectral ty...   \n",
       "\n",
       "                                     txtfiles_gh_2010  \\\n",
       "0   [ Sounds good ! I went on to read the ingredie...   \n",
       "1   [ @ @ @ @ @ @ @ @ @ @ rapidly improved . ,  ta...   \n",
       "2   [ the independence of Ghana is meaningless unl...   \n",
       "3   [ universal recipeint ,  almost dropped it in ...   \n",
       "4   [ real ,  widely practiced in the Volta Region...   \n",
       "5   [ Falling Standards Of Education ,  National A...   \n",
       "6   [ Invectus ,  How do we inspire ourselves @ @ ...   \n",
       "7   [ TOR Debt ,  insider ,  GHAIP ,  Tema Oil Ref...   \n",
       "8   [ one disease ,  what strategies and mechanism...   \n",
       "9   [ given teachers a worthy alternative in the f...   \n",
       "10  [ Kotoko best club of the century ,  routines ...   \n",
       "11  [ It came as a little bit of surprise because ...   \n",
       "\n",
       "                                     txtfiles_hk_2010  \\\n",
       "0   [ happiest cities ,  human touch , ,  peace th...   \n",
       "1   [ Wo ,  News 1+1 ,  ' Fake Donation-Gate ' Tur...   \n",
       "2   [ for a cleaner , healthier future . ,  Today ...   \n",
       "3   [ Walking on Air ,  I am confident that the di...   \n",
       "4   [ tank ,  Expo Shanghai 2010 , ,  Spirit and G...   \n",
       "5   [ First Pacific ,  the Company ,  on-market re...   \n",
       "6   [ This is the first time in my over three deca...   \n",
       "7   [ The large amount of waste in the dam area co...   \n",
       "8   [ As we have been demonstrating this past week...   \n",
       "9   [ persuade ,  This would reduce China 's globa...   \n",
       "10  [ Fading consumer demand is an unpleasant surp...   \n",
       "11  [ Designed in California , USA ,  It 's like y...   \n",
       "\n",
       "                                     txtfiles_ie_2010  \\\n",
       "0   [ Much of the growth seen and expected in the ...   \n",
       "1   [ This is me out of shape , ,  I knew this guy...   \n",
       "2   [  Paul Flynn is one of Ireland 's top chefs ....   \n",
       "3   [ Gooliegate ,  .  He has just opened the Gran...   \n",
       "4   [ junk ' ' this week .  Compared to that , Ire...   \n",
       "5   [ There is no other known photo of Bobby ( Ken...   \n",
       "6   [ There has been a growing body of evidence ov...   \n",
       "7   [ and ca n't you feel the chill factor already...   \n",
       "8   [ in the interest of providing the highest qua...   \n",
       "9   [ progressing well ,  A substantial number of ...   \n",
       "10  [ foreign ,  We would normally buy a large qua...   \n",
       "11  [ temporary wife ,  victim of a misogynous soc...   \n",
       "\n",
       "                                     txtfiles_in_2010  \\\n",
       "0   [ If we wish to be free , we must fight . Shal...   \n",
       "1   [ people are entitled to participate in , cont...   \n",
       "2   [ a nation that has been plunged into barbaris...   \n",
       "3   [ unconstitutional ,  rates over and above the...   \n",
       "4   [ We are trying our best to make ourselves hea...   \n",
       "5   [ constabulary ,  white elephants ,  distant c...   \n",
       "6   [ Twelve years ago , I purchased 15 acres land...   \n",
       "7   [ deterrent effect ,  rarest of rare ,  person...   \n",
       "8   [ I wish I had more troubles . You were always...   \n",
       "9   [ no Hindu temple had ever came under attack i...   \n",
       "10  [ recognised unions ,  certification of a barg...   \n",
       "11  [ The transcripts suggest @ @ @ @ @ @ @ @ @ @ ...   \n",
       "\n",
       "                                     txtfiles_jm_2010  \\\n",
       "0   [ of that mysterious organ found between a wom...   \n",
       "1   [ At 18 I just got caught up on the wrong side...   \n",
       "2   [ the shortages of nurses will only continue t...   \n",
       "3   [ warmth and friendliness of its people , ,  M...   \n",
       "4   [ a rapture of succulence , sweetness and tend...   \n",
       "5   [ Grant ,  IEPCA ,  spouse ,  a single woman w...   \n",
       "6   [ Other monetary seizures under POCA for the s...   \n",
       "7   [ make a little brown baby ,  auto-racism ,  w...   \n",
       "8   [ Many would wonder how this all started or ho...   \n",
       "9   [ If you eat the common fowl that eat natural ...   \n",
       "10  [ Album of the Century ,  Song of the Century ...   \n",
       "11  [ This remarkable breakthrough comes at a time...   \n",
       "\n",
       "                                     txtfiles_ke_2010  \\\n",
       "0   [ He is persona non grata here in Kenya , ,  s...   \n",
       "1   [ If you rattle a snake , you must be prepared...   \n",
       "2   [ We are doing this translocation to restock A...   \n",
       "3   [ You have to run under 2:10 , ,  At 2:10 you ...   \n",
       "4   [ Unfinished ,  I have heard about that group ...   \n",
       "5   [ Before this , Kenya had almost the range of ...   \n",
       "6   [ In the spirit of the East African Community ...   \n",
       "7   [ Our target is women aged between 25 and 40 a...   \n",
       "8   [ Farmers are currently getting about two metr...   \n",
       "9   [ Following a recent review , the Board of the...   \n",
       "10  [ I helped Mr Nthurima here , to be elected as...   \n",
       "11  [ reflect a new era for the sport in the count...   \n",
       "\n",
       "                                     txtfiles_lk_2010  \\\n",
       "0   [ Endemic biodiversity in Sri Lanka is excepti...   \n",
       "1   [ The hasty decision was taken by Mulayam Sing...   \n",
       "2   [ firsts ,  have gangsters close to you if you...   \n",
       "3   [ establishing marketing as the driving force ...   \n",
       "4   [ They impose high taxes on newsprint , making...   \n",
       "5   [ we sell or else . ,  \\n@@1510513 <h> 3D TV n...   \n",
       "6   [ Rajatha Dekma ,  The airline industry has sh...   \n",
       "7   [ On or about 10 July 2010 a midwife ( Family ...   \n",
       "8   [ Farmers ' Week ,  pooja ,  over political ve...   \n",
       "9   [ I initially dedicated this place to Jesus Ch...   \n",
       "10  [ The Education Ministry has told us to award ...   \n",
       "11  [ Uththama Pooja Prenama medal ,  Down with an...   \n",
       "\n",
       "                                     txtfiles_my_2010  \\\n",
       "0   [ Caddy that zigs ,  Following the accomplishm...   \n",
       "1   [ I have painted different styles of ancient b...   \n",
       "2   [ We 're fully booked until year-end , ,  Peop...   \n",
       "3   [ Over the past 18 years , we have witnessed c...   \n",
       "4   [ are ,  who ,  women ,   -- Adam  1 . Adam , ...   \n",
       "5   [ ego trips ,  in its own mould ,  Unless thes...   \n",
       "6   [ toy kingdom ,  toy kingdom , ,  This boy lov...   \n",
       "7   [ I regard today 's victory as the most import...   \n",
       "8   [ on the day of competition ,  real ,  ready f...   \n",
       "9   [ a simple woman who did not seek for fame and...   \n",
       "10  [ gel ,  wow ,  If it 's good for the team ......   \n",
       "11  [ art ,  Besides that , in the past , cigarett...   \n",
       "\n",
       "                                     txtfiles_ng_2010  \\\n",
       "0   [ SAPELE town in Delta State was thrown into c...   \n",
       "1   [ Peter Obi of the All Progressives Grand Alli...   \n",
       "2   [ women in developing world face numerous prob...   \n",
       "3   [ as is ,  Opt out of Registration Information...   \n",
       "4   [ fashion film ,  sneak peek ,  You already kn...   \n",
       "5   [ beware of small things that start big ,  bre...   \n",
       "6   [ the one the court recognises , Gadi , and Ga...   \n",
       "7   [ AT least 61 prominent Deltans including one-...   \n",
       "8   [ The investment you have made to the universi...   \n",
       "9   [ Centre of Excellence ,  law school ,  A law ...   \n",
       "10  [ window ,  we have filed a preliminary suit o...   \n",
       "11  [ a live human-made micro organism is patentab...   \n",
       "\n",
       "                                     txtfiles_nz_2010  \\\n",
       "0   [ framed ,  but because of the political calcu...   \n",
       "1   [ copper loop ,  node ,  myth ,  The Big Fat T...   \n",
       "2   [ Can Telecom survive ,  We bought the GSM spe...   \n",
       "3   [ This will spell the end of the current monar...   \n",
       "4   [ . \\n@@1470479  WIGAN committee is swamped ea...   \n",
       "5   [ Leading jumps jockey Tommy Hazlett reckons h...   \n",
       "6   [ This trend is not sustainable and needs to b...   \n",
       "7   [ staggering ,  Sometimes we do n't totally @ ...   \n",
       "8   [ There were ice cream cones coming out of the...   \n",
       "9   [ possibility that the All Blacks will lose ha...   \n",
       "10  [ I forgot to tell you , ,  we are donating so...   \n",
       "11  [ The demand-driven nature of the LBSPG ... ca...   \n",
       "\n",
       "                                     txtfiles_ph_2010  \\\n",
       "0   [ Pacman ,  No I would n't take it are you cra...   \n",
       "1   [ Laki sa Gatas ,  The Laki sa Gatas team hold...   \n",
       "2   [ El Terrible ,  The Champion Returns ,  The C...   \n",
       "3   [ said Panoncillo .  Meanwhile , this will be ...   \n",
       "4   [ Galing at Talino . ,  very intelligent , ,  ...   \n",
       "5   [ forever ,  undying devotion ,  taken in mode...   \n",
       "6   [ pianist 's pianist ,  SIR D ,  Better TWOget...   \n",
       "7   [ wand of light ,  Filipinos should address th...   \n",
       "8   [ actors ,  sharpen ,  I will retain for the i...   \n",
       "9   [ So far , ABS-CBN is dominant , ,  I 'm happy...   \n",
       "10  [ math ,  maths , ,  sports ,  sport ,  orient...   \n",
       "11  [ We are delighted to have Hollie join our cov...   \n",
       "\n",
       "                                     txtfiles_pk_2010  \\\n",
       "0   [ said Superintendent of Police , Ratanlal Dan...   \n",
       "1   [ Normal Wudu , an individual uses about 8-10 ...   \n",
       "2   [ Strengthening Women 's Strategic Use of Info...   \n",
       "3   [ Our thoughts and prayers are with his devote...   \n",
       "4   [ It is not that we are particularly fond of B...   \n",
       "5   [ Stage plays have always been a highlight of ...   \n",
       "6   [ Khudi Ke Basti ,  Menu Teray Jeya Sohna Hor ...   \n",
       "7   [ The water level in the lake is rising . The ...   \n",
       "8   [ people 's representatives ,  one of the firs...   \n",
       "9   [ Financing quality basic education in Pakista...   \n",
       "10  [  No progressive thinker of today , he argued...   \n",
       "11  [ For the last one to two weeks , liquidity wa...   \n",
       "\n",
       "                                     txtfiles_sg_2010  \\\n",
       "0   [ opposition to the actions of the Government ...   \n",
       "1   [ too dangerous ,  If I start sending her to s...   \n",
       "2   [ We are not offering 3D as a niche product , ...   \n",
       "3   [ regards to the queries that you have raised ...   \n",
       "4   [ entry level ,  test water ,  race ,  race , ...   \n",
       "5   [ Singapore 's History : Who Writes The Script...   \n",
       "6   [ We 've yet to push people into the train ,  ...   \n",
       "7   [ Adult Movies , ,  pink ,  Female customers c...   \n",
       "8   [ Marina Bay Sands has completed quite a numbe...   \n",
       "9   [ baby-dumping ,  The government ... has start...   \n",
       "10  [ I feel that there 's a sense , there 's a di...   \n",
       "11  [ male Chinese pervert ,  Pahlawan City will h...   \n",
       "\n",
       "                                     txtfiles_tz_2010  \\\n",
       "0                                                  []   \n",
       "1   [ We have received the green light : TIMM can ...   \n",
       "2   [ Freesat ,  The Bill will help broadcasters p...   \n",
       "3   [ strategic ,  the type of minerals and the le...   \n",
       "4                                                  []   \n",
       "5   [ AsiaSat 3S continues @ @ @ @ @ @ @ @ @ @ sat...   \n",
       "6   [ The Hub HD is a great new addition to the Fi...   \n",
       "7   [ The one where the guy is staring at the girl...   \n",
       "8   [ PCCW has led the way in Asia 's IPTV market ...   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "\n",
       "                                     txtfiles_us_2010  \\\n",
       "0   [ We need to stack the gear closely to get the...   \n",
       "1   [ minder ,  total ,  Constitution , ,  ratifie...   \n",
       "2   [ Meet me at the bar and tell me what 's going...   \n",
       "3   [ better living through chemistry ,  It 's not...   \n",
       "4   [ There 's lots of evidence that kids immersed...   \n",
       "5   [ aha moment , ,  changing the breadth of visu...   \n",
       "6   [ the American K-12 system is a failure , but ...   \n",
       "7   [ Buster ,  Black in America : Churched , ,  T...   \n",
       "8   [ family ,  We have moved from what 's most id...   \n",
       "9   [ how can I engineer this to work better ? ,  ...   \n",
       "10  [ commercial and manufacturing supremacy , ,  ...   \n",
       "11  [ No , ice is ice and in a cocktail you can sh...   \n",
       "\n",
       "                                     txtfiles_za_2010  \n",
       "0   [ The interest in both players is growing and ...  \n",
       "1   [ The burden of history : The legacy of the ap...  \n",
       "2   [ said Touchline Media Group Publisher , Desir...  \n",
       "3   [ freeconomics ,  People thought I was crazy l...  \n",
       "4   [ Some people may not be measuring up to the t...  \n",
       "5   [ I am very excited although at this stage I a...  \n",
       "6   [ Stuart 's mother 's ,  I Heart Mulholland 's...  \n",
       "7   [ There is no way we can be held responsible f...  \n",
       "8   [ DMR 's ' ) proposed amendments to South Afri...  \n",
       "9   [ It 's a crisis . There 's total anarchy , , ...  \n",
       "10  [ This is my last official public function . B...  \n",
       "11  [ alleged gross transgression of Nersa 's code...  "
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2011</th>\n",
       "      <th>txtfiles_bd_2011</th>\n",
       "      <th>txtfiles_ca_2011</th>\n",
       "      <th>txtfiles_gb_2011</th>\n",
       "      <th>txtfiles_gh_2011</th>\n",
       "      <th>txtfiles_hk_2011</th>\n",
       "      <th>txtfiles_ie_2011</th>\n",
       "      <th>txtfiles_in_2011</th>\n",
       "      <th>txtfiles_jm_2011</th>\n",
       "      <th>txtfiles_ke_2011</th>\n",
       "      <th>txtfiles_lk_2011</th>\n",
       "      <th>txtfiles_my_2011</th>\n",
       "      <th>txtfiles_ng_2011</th>\n",
       "      <th>txtfiles_nz_2011</th>\n",
       "      <th>txtfiles_ph_2011</th>\n",
       "      <th>txtfiles_pk_2011</th>\n",
       "      <th>txtfiles_sg_2011</th>\n",
       "      <th>txtfiles_tz_2011</th>\n",
       "      <th>txtfiles_us_2011</th>\n",
       "      <th>txtfiles_za_2011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Settings ,  undo ,  Reply ,  Reply All ,  th...</td>\n",
       "      <td>[ death ,  passed away ,  reincarnated ,  an i...</td>\n",
       "      <td>[ the self-styled underground mayor of Yonge S...</td>\n",
       "      <td>[ not fit for purpose ,  defensive complacency...</td>\n",
       "      <td>[ strengthening care for the injured : success...</td>\n",
       "      <td>[ Yan Can Cook ,  smoke point , ,  Science &amp;am...</td>\n",
       "      <td>[ Early next year , when this new pre-clearanc...</td>\n",
       "      <td>[ Grandma , red is too bright . ,  People Firs...</td>\n",
       "      <td>[ The genesis @ @ @ @ @ @ @ @ @ @ as 1901 when...</td>\n",
       "      <td>[ The greatest mathematical discovery of all t...</td>\n",
       "      <td>[ Reduced Emissions from Deforestation and For...</td>\n",
       "      <td>[ a form of tennis ,  girl sport ,  Funding an...</td>\n",
       "      <td>[ has become a graveyard for some promising ca...</td>\n",
       "      <td>[ unaware of exactly what her husband got up t...</td>\n",
       "      <td>[ workhorse ,  like being in a washing machine...</td>\n",
       "      <td>[ There is still a large number of passengers ...</td>\n",
       "      <td>[ The Gem Collection : Exquisite , Elegant , a...</td>\n",
       "      <td>[ Imagine a person from Mbeya importing day-ol...</td>\n",
       "      <td>[ The son of Gd ,  Gd ,  Force ,  Gd Force ,  ...</td>\n",
       "      <td>[ Drinking treated effluent water is nothing n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ terrorists ,  literary proceeds ,  crimes ag...</td>\n",
       "      <td>[ ? lawyers , court employees , court clerks a...</td>\n",
       "      <td>[ The completion of this transaction brings th...</td>\n",
       "      <td>[ might be said to do the work of the sleeping...</td>\n",
       "      <td>[ leaked ,  I have called you specifically to ...</td>\n",
       "      <td>[ Lafite . ,  astronauts ,  Return Migration a...</td>\n",
       "      <td>[ fire ,  in the belly ,  or ,  ) , adorning i...</td>\n",
       "      <td>[ The India model is really one based largely ...</td>\n",
       "      <td>[ Frankly , I was pressured into doing my MBA ...</td>\n",
       "      <td>[ big men ,  We are creating these guidelines ...</td>\n",
       "      <td>[ to be very scrupulous ,  reasonably confiden...</td>\n",
       "      <td>[ breed like bunnies ,  multiply @ @ @ @ @ @ @...</td>\n",
       "      <td>[ They have this affinity with the Yoruba , wh...</td>\n",
       "      <td>[ ongoing disagreement ,  bullying ,  foreseea...</td>\n",
       "      <td>[ Do n't you remember ? ,   Lovi Poe says thin...</td>\n",
       "      <td>[ It has now been turned into a barn , ,  The ...</td>\n",
       "      <td>[ red-whiskered bulbul ,  Dr Goh believed firm...</td>\n",
       "      <td>[ brown sugar ,  Heroin abuse is very common ,...</td>\n",
       "      <td>[ Who 's minding the store ? ,  Honey , I forg...</td>\n",
       "      <td>[ It was agreed that accommodation shall be pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ Siba is the latest incarnation of what was p...</td>\n",
       "      <td>[ Roopban ,  The Bauls are mystic minstrels li...</td>\n",
       "      <td>[ Black History in Canada Education Guide ,  B...</td>\n",
       "      <td>[ spark ,  augmented reality ,  donated ,  his...</td>\n",
       "      <td>[ unfavorable market conditions , ,  cayenne p...</td>\n",
       "      <td>[ Stalin 's Lost Oil ,  Property agents earn j...</td>\n",
       "      <td>[ The only possible candidate for this fecund ...</td>\n",
       "      <td>[ rarest of rare ,  The court looking into the...</td>\n",
       "      <td>[ Eligible organisations would then respond by...</td>\n",
       "      <td>[ A house that costs Sh2.5 million to build in...</td>\n",
       "      <td>[ The importance of fisheries to Sri Lanka goe...</td>\n",
       "      <td>[ You look great Michael , you do n't looked l...</td>\n",
       "      <td>[ . In Bamako in 2000 , the Organization Inter...</td>\n",
       "      <td>[ You do n't have to be in the charitable area...</td>\n",
       "      <td>[ consistent pro-people , nationalist educatio...</td>\n",
       "      <td>[ comparative studies of religious books ,  Ed...</td>\n",
       "      <td>[ I have heard anecdotes of students afraid to...</td>\n",
       "      <td>[ Sustainable development , peace and security...</td>\n",
       "      <td>[ Justice ,  Preserve the Constitution ,  evil...</td>\n",
       "      <td>[ crowd out ,  The currently high household de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ My client may be 56 but he is as motivated t...</td>\n",
       "      <td>[ I went to Captain Abdul Halim Chowdhury at S...</td>\n",
       "      <td>[ the creeps ,  He was a good kid , ,  Just a ...</td>\n",
       "      <td>[ All right , pal , keep your hair on , ,  We ...</td>\n",
       "      <td>[ There should be acknowledgement of creativit...</td>\n",
       "      <td>[ Our 2010 fourth quarter and full year result...</td>\n",
       "      <td>[ Obesity is strongly related to diabetes and ...</td>\n",
       "      <td>[ This is good news , but a lot more work need...</td>\n",
       "      <td>[ is going to take the historic and progressiv...</td>\n",
       "      <td>[ Start and Improve Your Business ,  Training ...</td>\n",
       "      <td>[ We as an undeterred nation has achieved many...</td>\n",
       "      <td>[ Christian Publication ,  I hope this 10 poin...</td>\n",
       "      <td>[ why CBN should endlessly be mopping up exces...</td>\n",
       "      <td>[ perfect storm ,  All people look at is the i...</td>\n",
       "      <td>[ Tarangban ( Cave ) , ,  A damn good show , ,...</td>\n",
       "      <td>[ This is actually International Baccalaureate...</td>\n",
       "      <td>[ Nike is the Greek Goddess of Victory and all...</td>\n",
       "      <td>[ The lack of access to drinking water and san...</td>\n",
       "      <td>[ There 's as many reasons as there are vets y...</td>\n",
       "      <td>[ How can one trust an institution that promis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ Red Lands ,  the IMF points out that the ban...</td>\n",
       "      <td>[ Sri Lanka Cricket wishes to express its conc...</td>\n",
       "      <td>[ The parliamentary tradition is that you go w...</td>\n",
       "      <td>[ An unknown British Soldier ,  Lead kindly li...</td>\n",
       "      <td>[ I was screaming in a taxi ! ,  I 've been to...</td>\n",
       "      <td>[ Our message to the employers is loud and cle...</td>\n",
       "      <td>[ Uisce beatha ,  water of life . ,  blended ,...</td>\n",
       "      <td>[ The monarchy is fascinating to most people ....</td>\n",
       "      <td>[ but they have been impeded by capacity and r...</td>\n",
       "      <td>[ The licensees are in the process of effectin...</td>\n",
       "      <td>[ Ban Ki-moon and his three member panel to pe...</td>\n",
       "      <td>[ Under the enhance deposit insurance system ,...</td>\n",
       "      <td>[ their dominative tendencies ,  complete ,  m...</td>\n",
       "      <td>[ Lots of my friends and family were there and...</td>\n",
       "      <td>[ A great leap in the development of the Clark...</td>\n",
       "      <td>[ an estimated 80 per cent of women ,  they ha...</td>\n",
       "      <td>[ aunties ,  recognised her and had come to ac...</td>\n",
       "      <td>[ The goal is to improve transparency , ensure...</td>\n",
       "      <td>[ There 's tons of character development . ,  ...</td>\n",
       "      <td>[ Similar cardless transaction options from ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ In the younger group we think this is occurr...</td>\n",
       "      <td>[ I have already sent all the necessary papers...</td>\n",
       "      <td>[ The law states you are not allowed to take t...</td>\n",
       "      <td>[ limousine ,  A response to a cyber-incident ...</td>\n",
       "      <td>[ Concurrently , a sustainability plan is bein...</td>\n",
       "      <td>[ getting bored . ,  I 'm fascinated by all th...</td>\n",
       "      <td>[ A maid 's outfit ? ,  Oh ! I thought it was ...</td>\n",
       "      <td>[ The total number of ministers , including th...</td>\n",
       "      <td>[ Approximately one-third of the country 's pr...</td>\n",
       "      <td>[ The remaining roles of chief financial offic...</td>\n",
       "      <td>[ Defeating Terrorism-The Sri Lankan Experienc...</td>\n",
       "      <td>[ reasonable and competitive ,  This signing c...</td>\n",
       "      <td>[ To actualize their dream of imposing a speak...</td>\n",
       "      <td>[ threat to the women 's mental health ,  We c...</td>\n",
       "      <td>[ In direct contravention of the Board Resolut...</td>\n",
       "      <td>[ On Siachen , Joint Secretary ( Pakistan , Af...</td>\n",
       "      <td>[ urgent problems , ,  Look at this dead yello...</td>\n",
       "      <td>[ At least three to four cases of forcing mino...</td>\n",
       "      <td>[ consonant intervals , the smooth-sounding on...</td>\n",
       "      <td>[ marginalisation ,  desperate ,  myth ,  We c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ Write image file to disc ,  performative bra...</td>\n",
       "      <td>[ unlawful action ,  I was just told a few wee...</td>\n",
       "      <td>[ blotted my copybook ,  Bethune 's fame ,  is...</td>\n",
       "      <td>[ living ark ,  lifetime love ,  end-of-the-li...</td>\n",
       "      <td>[ permanent forest estate , ,  sustainably , ,...</td>\n",
       "      <td>[ Soviet Sectors ,  native ,  Period of Reform...</td>\n",
       "      <td>[ visiting a house near the Curragh of a decen...</td>\n",
       "      <td>[ Today , economic power has been captured by ...</td>\n",
       "      <td>[ We drew on several studies , there were focu...</td>\n",
       "      <td>[ has opened the first serious debate on forei...</td>\n",
       "      <td>[ The previous interim committee stayed on for...</td>\n",
       "      <td>[ aligned ,  attacking ,  team ,  My Coke item...</td>\n",
       "      <td>[ She had been sick for a while but it was n't...</td>\n",
       "      <td>[ There were days where there would just be ab...</td>\n",
       "      <td>[ The people will defend our rights . We will ...</td>\n",
       "      <td>[ repeat a lie till it becomes truth ,  I knew...</td>\n",
       "      <td>[ You had thus interfered in Singapore 's dome...</td>\n",
       "      <td>[ Kilimo Kwanza ,  Priority Agriculture Progra...</td>\n",
       "      <td>[ Cities I Have Loved ,  hit ,  says researche...</td>\n",
       "      <td>[ dirt diggers ,  custodianship ,  four wheel-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ good faith ,  it is the existence of the min...</td>\n",
       "      <td>[ Emerging power ,  emerging power ,  We have ...</td>\n",
       "      <td>[ We 've been working hard for a long time , ,...</td>\n",
       "      <td>[ History is that certainty produced at the po...</td>\n",
       "      <td>[ Harmful Effects Of Galamsey ,  economic bein...</td>\n",
       "      <td>[ He always seems selfish and arrogant , ' ' s...</td>\n",
       "      <td>[ tank ,  tank ,  Before ' Clash of the Titans...</td>\n",
       "      <td>[ The day was 7th August 1936 , as Germany fac...</td>\n",
       "      <td>[ Next time , next time . ,  going after criti...</td>\n",
       "      <td>[ With this acquisition , we @ @ @ @ @ @ @ @ @...</td>\n",
       "      <td>[ vaccine ,  the richest man in the world ,  d...</td>\n",
       "      <td>[ nothing-to-lose ,  In the first game , I mad...</td>\n",
       "      <td>[ My incarceration was as a result of a script...</td>\n",
       "      <td>[ These products change frequently , and new o...</td>\n",
       "      <td>[ We are shocked to find outrageously high lev...</td>\n",
       "      <td>[ American ,  ( flying economy on a Middle Eas...</td>\n",
       "      <td>[ Through a variety of means , some phony doct...</td>\n",
       "      <td>[ This bank would issue loans at a lower inter...</td>\n",
       "      <td>[ It was believed that the tomb of St. Philip ...</td>\n",
       "      <td>[ 20 percent time ,  prefetching ,  How can I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ The court has awarded my client with death ....</td>\n",
       "      <td>[ Shakib Al Hasan and Tamim Iqbal have been re...</td>\n",
       "      <td>[ A single vulnerable friend in a user 's soci...</td>\n",
       "      <td>[ Masturboats ,  Pussy Cradle ,  headless bedr...</td>\n",
       "      <td>[ Dzi wo fie asem ,  In the year 2010 we had 1...</td>\n",
       "      <td>[ nightmare . ,  Circular on Import Duty Exemp...</td>\n",
       "      <td>[ To think a group of scientists could identif...</td>\n",
       "      <td>[ No Fly Zone ,  despotic dynasty ,  peaceful ...</td>\n",
       "      <td>[ uncivil society ,  a wide range of disruptiv...</td>\n",
       "      <td>[ We have not received any food . It is now tw...</td>\n",
       "      <td>[ A further Nail In The LTTE Coffin ! Adele Ba...</td>\n",
       "      <td>[ 1,000,000 umat Islam sokong JAIS &amp;amp; Hasan...</td>\n",
       "      <td>[ jobs ,  runs ,  it is as if some legislators...</td>\n",
       "      <td>[ acutely aware ,  Children who live in povert...</td>\n",
       "      <td>[ Know your product , keep ahead of your compe...</td>\n",
       "      <td>[ Halka Halka Suroor ,  When you 're in a band...</td>\n",
       "      <td>[ S ,  This play is about the complacency of t...</td>\n",
       "      <td>[ promote the development of free , political ...</td>\n",
       "      <td>[ natural aristocracy @ @ @ @ @ @ @ @ @ @ ,  b...</td>\n",
       "      <td>[ priority areas ,  health , education and rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ looking for ,  what he/she @ @ @ @ @ @ @ @ @...</td>\n",
       "      <td>[ Bengalis did commit atrocities including rap...</td>\n",
       "      <td>[ tender human side ,  it was frequently overs...</td>\n",
       "      <td>[ I am so confused ,  What the hell is going o...</td>\n",
       "      <td>[ unrivalled ,  Dark Continent ,  rejected ,  ...</td>\n",
       "      <td>[ You should assume that as of the publication...</td>\n",
       "      <td>[ They relied on each other . They were very ,...</td>\n",
       "      <td>[ Father of the Nation ,  RSP has always been ...</td>\n",
       "      <td>[ Going there , we found in actuality , there ...</td>\n",
       "      <td>[ The impending law would increase the cost of...</td>\n",
       "      <td>[ This is the 12th intake for this external de...</td>\n",
       "      <td>[ Beating Taufik Hidayat has always been of my...</td>\n",
       "      <td>[ the trouble with Nigeria ,  colonial contrap...</td>\n",
       "      <td>[ we blinked first ,  Whether you 're a race s...</td>\n",
       "      <td>[ emotional and physical exhaustion . ,  Glitt...</td>\n",
       "      <td>[ Take no political pressure from any quarter ...</td>\n",
       "      <td>[ Let 's Protect Tohoku ,  vulgar visual of th...</td>\n",
       "      <td>[ The issue of updating voters ' register is n...</td>\n",
       "      <td>[ Whether they were playing a major-league tea...</td>\n",
       "      <td>[ The International Symposium on Exchange amon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ A critical element of this work will be the ...</td>\n",
       "      <td>[ said Senjuti .  The Ethnological Museum in C...</td>\n",
       "      <td>[ This discovery of yours will create forgetfu...</td>\n",
       "      <td>[ At first I used to argue that a horse like P...</td>\n",
       "      <td>[ Expose names of failed 1,696 nurses ,  the c...</td>\n",
       "      <td>[ 40 million vendors ,  poorly served ,  lots ...</td>\n",
       "      <td>[ legal , binding agreement ,  has already obt...</td>\n",
       "      <td>[ To visualise that time close your eyes and i...</td>\n",
       "      <td>[ the oldest continuously @ @ @ @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ My ministry in collaboration with the Kenya ...</td>\n",
       "      <td>[ very serious nature . ,  The facts of the ca...</td>\n",
       "      <td>[ innocent ,  look good ,  crosses the line , ...</td>\n",
       "      <td>[ least human development ' ' countries in ter...</td>\n",
       "      <td>[ terribly sad ' ' he was now a criminal .  Hi...</td>\n",
       "      <td>[ man of the masses . ,  Da King 's ,  true Pr...</td>\n",
       "      <td>[ sales talk ,  weather conditions ,  weather ...</td>\n",
       "      <td>[ CD-quality ,  Since the launch of the servic...</td>\n",
       "      <td>[ I think the practice of arresting only those...</td>\n",
       "      <td>[ stress hormone , ,  First we heard a rumble ...</td>\n",
       "      <td>[ said Srivastava .  Pre-paid customers who pu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     txtfiles_au_2011  \\\n",
       "0   [ Settings ,  undo ,  Reply ,  Reply All ,  th...   \n",
       "1   [ terrorists ,  literary proceeds ,  crimes ag...   \n",
       "2   [ Siba is the latest incarnation of what was p...   \n",
       "3   [ My client may be 56 but he is as motivated t...   \n",
       "4   [ Red Lands ,  the IMF points out that the ban...   \n",
       "5   [ In the younger group we think this is occurr...   \n",
       "6   [ Write image file to disc ,  performative bra...   \n",
       "7   [ good faith ,  it is the existence of the min...   \n",
       "8   [ The court has awarded my client with death ....   \n",
       "9   [ looking for ,  what he/she @ @ @ @ @ @ @ @ @...   \n",
       "10  [ A critical element of this work will be the ...   \n",
       "\n",
       "                                     txtfiles_bd_2011  \\\n",
       "0   [ death ,  passed away ,  reincarnated ,  an i...   \n",
       "1   [ ? lawyers , court employees , court clerks a...   \n",
       "2   [ Roopban ,  The Bauls are mystic minstrels li...   \n",
       "3   [ I went to Captain Abdul Halim Chowdhury at S...   \n",
       "4   [ Sri Lanka Cricket wishes to express its conc...   \n",
       "5   [ I have already sent all the necessary papers...   \n",
       "6   [ unlawful action ,  I was just told a few wee...   \n",
       "7   [ Emerging power ,  emerging power ,  We have ...   \n",
       "8   [ Shakib Al Hasan and Tamim Iqbal have been re...   \n",
       "9   [ Bengalis did commit atrocities including rap...   \n",
       "10  [ said Senjuti .  The Ethnological Museum in C...   \n",
       "\n",
       "                                     txtfiles_ca_2011  \\\n",
       "0   [ the self-styled underground mayor of Yonge S...   \n",
       "1   [ The completion of this transaction brings th...   \n",
       "2   [ Black History in Canada Education Guide ,  B...   \n",
       "3   [ the creeps ,  He was a good kid , ,  Just a ...   \n",
       "4   [ The parliamentary tradition is that you go w...   \n",
       "5   [ The law states you are not allowed to take t...   \n",
       "6   [ blotted my copybook ,  Bethune 's fame ,  is...   \n",
       "7   [ We 've been working hard for a long time , ,...   \n",
       "8   [ A single vulnerable friend in a user 's soci...   \n",
       "9   [ tender human side ,  it was frequently overs...   \n",
       "10  [ This discovery of yours will create forgetfu...   \n",
       "\n",
       "                                     txtfiles_gb_2011  \\\n",
       "0   [ not fit for purpose ,  defensive complacency...   \n",
       "1   [ might be said to do the work of the sleeping...   \n",
       "2   [ spark ,  augmented reality ,  donated ,  his...   \n",
       "3   [ All right , pal , keep your hair on , ,  We ...   \n",
       "4   [ An unknown British Soldier ,  Lead kindly li...   \n",
       "5   [ limousine ,  A response to a cyber-incident ...   \n",
       "6   [ living ark ,  lifetime love ,  end-of-the-li...   \n",
       "7   [ History is that certainty produced at the po...   \n",
       "8   [ Masturboats ,  Pussy Cradle ,  headless bedr...   \n",
       "9   [ I am so confused ,  What the hell is going o...   \n",
       "10  [ At first I used to argue that a horse like P...   \n",
       "\n",
       "                                     txtfiles_gh_2011  \\\n",
       "0   [ strengthening care for the injured : success...   \n",
       "1   [ leaked ,  I have called you specifically to ...   \n",
       "2   [ unfavorable market conditions , ,  cayenne p...   \n",
       "3   [ There should be acknowledgement of creativit...   \n",
       "4   [ I was screaming in a taxi ! ,  I 've been to...   \n",
       "5   [ Concurrently , a sustainability plan is bein...   \n",
       "6   [ permanent forest estate , ,  sustainably , ,...   \n",
       "7   [ Harmful Effects Of Galamsey ,  economic bein...   \n",
       "8   [ Dzi wo fie asem ,  In the year 2010 we had 1...   \n",
       "9   [ unrivalled ,  Dark Continent ,  rejected ,  ...   \n",
       "10  [ Expose names of failed 1,696 nurses ,  the c...   \n",
       "\n",
       "                                     txtfiles_hk_2011  \\\n",
       "0   [ Yan Can Cook ,  smoke point , ,  Science &am...   \n",
       "1   [ Lafite . ,  astronauts ,  Return Migration a...   \n",
       "2   [ Stalin 's Lost Oil ,  Property agents earn j...   \n",
       "3   [ Our 2010 fourth quarter and full year result...   \n",
       "4   [ Our message to the employers is loud and cle...   \n",
       "5   [ getting bored . ,  I 'm fascinated by all th...   \n",
       "6   [ Soviet Sectors ,  native ,  Period of Reform...   \n",
       "7   [ He always seems selfish and arrogant , ' ' s...   \n",
       "8   [ nightmare . ,  Circular on Import Duty Exemp...   \n",
       "9   [ You should assume that as of the publication...   \n",
       "10  [ 40 million vendors ,  poorly served ,  lots ...   \n",
       "\n",
       "                                     txtfiles_ie_2011  \\\n",
       "0   [ Early next year , when this new pre-clearanc...   \n",
       "1   [ fire ,  in the belly ,  or ,  ) , adorning i...   \n",
       "2   [ The only possible candidate for this fecund ...   \n",
       "3   [ Obesity is strongly related to diabetes and ...   \n",
       "4   [ Uisce beatha ,  water of life . ,  blended ,...   \n",
       "5   [ A maid 's outfit ? ,  Oh ! I thought it was ...   \n",
       "6   [ visiting a house near the Curragh of a decen...   \n",
       "7   [ tank ,  tank ,  Before ' Clash of the Titans...   \n",
       "8   [ To think a group of scientists could identif...   \n",
       "9   [ They relied on each other . They were very ,...   \n",
       "10  [ legal , binding agreement ,  has already obt...   \n",
       "\n",
       "                                     txtfiles_in_2011  \\\n",
       "0   [ Grandma , red is too bright . ,  People Firs...   \n",
       "1   [ The India model is really one based largely ...   \n",
       "2   [ rarest of rare ,  The court looking into the...   \n",
       "3   [ This is good news , but a lot more work need...   \n",
       "4   [ The monarchy is fascinating to most people ....   \n",
       "5   [ The total number of ministers , including th...   \n",
       "6   [ Today , economic power has been captured by ...   \n",
       "7   [ The day was 7th August 1936 , as Germany fac...   \n",
       "8   [ No Fly Zone ,  despotic dynasty ,  peaceful ...   \n",
       "9   [ Father of the Nation ,  RSP has always been ...   \n",
       "10  [ To visualise that time close your eyes and i...   \n",
       "\n",
       "                                     txtfiles_jm_2011  \\\n",
       "0   [ The genesis @ @ @ @ @ @ @ @ @ @ as 1901 when...   \n",
       "1   [ Frankly , I was pressured into doing my MBA ...   \n",
       "2   [ Eligible organisations would then respond by...   \n",
       "3   [ is going to take the historic and progressiv...   \n",
       "4   [ but they have been impeded by capacity and r...   \n",
       "5   [ Approximately one-third of the country 's pr...   \n",
       "6   [ We drew on several studies , there were focu...   \n",
       "7   [ Next time , next time . ,  going after criti...   \n",
       "8   [ uncivil society ,  a wide range of disruptiv...   \n",
       "9   [ Going there , we found in actuality , there ...   \n",
       "10  [ the oldest continuously @ @ @ @ @ @ @ @ @ @ ...   \n",
       "\n",
       "                                     txtfiles_ke_2011  \\\n",
       "0   [ The greatest mathematical discovery of all t...   \n",
       "1   [ big men ,  We are creating these guidelines ...   \n",
       "2   [ A house that costs Sh2.5 million to build in...   \n",
       "3   [ Start and Improve Your Business ,  Training ...   \n",
       "4   [ The licensees are in the process of effectin...   \n",
       "5   [ The remaining roles of chief financial offic...   \n",
       "6   [ has opened the first serious debate on forei...   \n",
       "7   [ With this acquisition , we @ @ @ @ @ @ @ @ @...   \n",
       "8   [ We have not received any food . It is now tw...   \n",
       "9   [ The impending law would increase the cost of...   \n",
       "10  [ My ministry in collaboration with the Kenya ...   \n",
       "\n",
       "                                     txtfiles_lk_2011  \\\n",
       "0   [ Reduced Emissions from Deforestation and For...   \n",
       "1   [ to be very scrupulous ,  reasonably confiden...   \n",
       "2   [ The importance of fisheries to Sri Lanka goe...   \n",
       "3   [ We as an undeterred nation has achieved many...   \n",
       "4   [ Ban Ki-moon and his three member panel to pe...   \n",
       "5   [ Defeating Terrorism-The Sri Lankan Experienc...   \n",
       "6   [ The previous interim committee stayed on for...   \n",
       "7   [ vaccine ,  the richest man in the world ,  d...   \n",
       "8   [ A further Nail In The LTTE Coffin ! Adele Ba...   \n",
       "9   [ This is the 12th intake for this external de...   \n",
       "10  [ very serious nature . ,  The facts of the ca...   \n",
       "\n",
       "                                     txtfiles_my_2011  \\\n",
       "0   [ a form of tennis ,  girl sport ,  Funding an...   \n",
       "1   [ breed like bunnies ,  multiply @ @ @ @ @ @ @...   \n",
       "2   [ You look great Michael , you do n't looked l...   \n",
       "3   [ Christian Publication ,  I hope this 10 poin...   \n",
       "4   [ Under the enhance deposit insurance system ,...   \n",
       "5   [ reasonable and competitive ,  This signing c...   \n",
       "6   [ aligned ,  attacking ,  team ,  My Coke item...   \n",
       "7   [ nothing-to-lose ,  In the first game , I mad...   \n",
       "8   [ 1,000,000 umat Islam sokong JAIS &amp; Hasan...   \n",
       "9   [ Beating Taufik Hidayat has always been of my...   \n",
       "10  [ innocent ,  look good ,  crosses the line , ...   \n",
       "\n",
       "                                     txtfiles_ng_2011  \\\n",
       "0   [ has become a graveyard for some promising ca...   \n",
       "1   [ They have this affinity with the Yoruba , wh...   \n",
       "2   [ . In Bamako in 2000 , the Organization Inter...   \n",
       "3   [ why CBN should endlessly be mopping up exces...   \n",
       "4   [ their dominative tendencies ,  complete ,  m...   \n",
       "5   [ To actualize their dream of imposing a speak...   \n",
       "6   [ She had been sick for a while but it was n't...   \n",
       "7   [ My incarceration was as a result of a script...   \n",
       "8   [ jobs ,  runs ,  it is as if some legislators...   \n",
       "9   [ the trouble with Nigeria ,  colonial contrap...   \n",
       "10  [ least human development ' ' countries in ter...   \n",
       "\n",
       "                                     txtfiles_nz_2011  \\\n",
       "0   [ unaware of exactly what her husband got up t...   \n",
       "1   [ ongoing disagreement ,  bullying ,  foreseea...   \n",
       "2   [ You do n't have to be in the charitable area...   \n",
       "3   [ perfect storm ,  All people look at is the i...   \n",
       "4   [ Lots of my friends and family were there and...   \n",
       "5   [ threat to the women 's mental health ,  We c...   \n",
       "6   [ There were days where there would just be ab...   \n",
       "7   [ These products change frequently , and new o...   \n",
       "8   [ acutely aware ,  Children who live in povert...   \n",
       "9   [ we blinked first ,  Whether you 're a race s...   \n",
       "10  [ terribly sad ' ' he was now a criminal .  Hi...   \n",
       "\n",
       "                                     txtfiles_ph_2011  \\\n",
       "0   [ workhorse ,  like being in a washing machine...   \n",
       "1   [ Do n't you remember ? ,   Lovi Poe says thin...   \n",
       "2   [ consistent pro-people , nationalist educatio...   \n",
       "3   [ Tarangban ( Cave ) , ,  A damn good show , ,...   \n",
       "4   [ A great leap in the development of the Clark...   \n",
       "5   [ In direct contravention of the Board Resolut...   \n",
       "6   [ The people will defend our rights . We will ...   \n",
       "7   [ We are shocked to find outrageously high lev...   \n",
       "8   [ Know your product , keep ahead of your compe...   \n",
       "9   [ emotional and physical exhaustion . ,  Glitt...   \n",
       "10  [ man of the masses . ,  Da King 's ,  true Pr...   \n",
       "\n",
       "                                     txtfiles_pk_2011  \\\n",
       "0   [ There is still a large number of passengers ...   \n",
       "1   [ It has now been turned into a barn , ,  The ...   \n",
       "2   [ comparative studies of religious books ,  Ed...   \n",
       "3   [ This is actually International Baccalaureate...   \n",
       "4   [ an estimated 80 per cent of women ,  they ha...   \n",
       "5   [ On Siachen , Joint Secretary ( Pakistan , Af...   \n",
       "6   [ repeat a lie till it becomes truth ,  I knew...   \n",
       "7   [ American ,  ( flying economy on a Middle Eas...   \n",
       "8   [ Halka Halka Suroor ,  When you 're in a band...   \n",
       "9   [ Take no political pressure from any quarter ...   \n",
       "10  [ sales talk ,  weather conditions ,  weather ...   \n",
       "\n",
       "                                     txtfiles_sg_2011  \\\n",
       "0   [ The Gem Collection : Exquisite , Elegant , a...   \n",
       "1   [ red-whiskered bulbul ,  Dr Goh believed firm...   \n",
       "2   [ I have heard anecdotes of students afraid to...   \n",
       "3   [ Nike is the Greek Goddess of Victory and all...   \n",
       "4   [ aunties ,  recognised her and had come to ac...   \n",
       "5   [ urgent problems , ,  Look at this dead yello...   \n",
       "6   [ You had thus interfered in Singapore 's dome...   \n",
       "7   [ Through a variety of means , some phony doct...   \n",
       "8   [ S ,  This play is about the complacency of t...   \n",
       "9   [ Let 's Protect Tohoku ,  vulgar visual of th...   \n",
       "10  [ CD-quality ,  Since the launch of the servic...   \n",
       "\n",
       "                                     txtfiles_tz_2011  \\\n",
       "0   [ Imagine a person from Mbeya importing day-ol...   \n",
       "1   [ brown sugar ,  Heroin abuse is very common ,...   \n",
       "2   [ Sustainable development , peace and security...   \n",
       "3   [ The lack of access to drinking water and san...   \n",
       "4   [ The goal is to improve transparency , ensure...   \n",
       "5   [ At least three to four cases of forcing mino...   \n",
       "6   [ Kilimo Kwanza ,  Priority Agriculture Progra...   \n",
       "7   [ This bank would issue loans at a lower inter...   \n",
       "8   [ promote the development of free , political ...   \n",
       "9   [ The issue of updating voters ' register is n...   \n",
       "10  [ I think the practice of arresting only those...   \n",
       "\n",
       "                                     txtfiles_us_2011  \\\n",
       "0   [ The son of Gd ,  Gd ,  Force ,  Gd Force ,  ...   \n",
       "1   [ Who 's minding the store ? ,  Honey , I forg...   \n",
       "2   [ Justice ,  Preserve the Constitution ,  evil...   \n",
       "3   [ There 's as many reasons as there are vets y...   \n",
       "4   [ There 's tons of character development . ,  ...   \n",
       "5   [ consonant intervals , the smooth-sounding on...   \n",
       "6   [ Cities I Have Loved ,  hit ,  says researche...   \n",
       "7   [ It was believed that the tomb of St. Philip ...   \n",
       "8   [ natural aristocracy @ @ @ @ @ @ @ @ @ @ ,  b...   \n",
       "9   [ Whether they were playing a major-league tea...   \n",
       "10  [ stress hormone , ,  First we heard a rumble ...   \n",
       "\n",
       "                                     txtfiles_za_2011  \n",
       "0   [ Drinking treated effluent water is nothing n...  \n",
       "1   [ It was agreed that accommodation shall be pr...  \n",
       "2   [ crowd out ,  The currently high household de...  \n",
       "3   [ How can one trust an institution that promis...  \n",
       "4   [ Similar cardless transaction options from ot...  \n",
       "5   [ marginalisation ,  desperate ,  myth ,  We c...  \n",
       "6   [ dirt diggers ,  custodianship ,  four wheel-...  \n",
       "7   [ 20 percent time ,  prefetching ,  How can I ...  \n",
       "8   [ priority areas ,  health , education and rel...  \n",
       "9   [ The International Symposium on Exchange amon...  \n",
       "10  [ said Srivastava .  Pre-paid customers who pu...  "
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2012</th>\n",
       "      <th>txtfiles_bd_2012</th>\n",
       "      <th>txtfiles_ca_2012</th>\n",
       "      <th>txtfiles_gb_2012</th>\n",
       "      <th>txtfiles_gh_2012</th>\n",
       "      <th>txtfiles_hk_2012</th>\n",
       "      <th>txtfiles_ie_2012</th>\n",
       "      <th>txtfiles_in_2012</th>\n",
       "      <th>txtfiles_jm_2012</th>\n",
       "      <th>txtfiles_ke_2012</th>\n",
       "      <th>txtfiles_lk_2012</th>\n",
       "      <th>txtfiles_my_2012</th>\n",
       "      <th>txtfiles_ng_2012</th>\n",
       "      <th>txtfiles_nz_2012</th>\n",
       "      <th>txtfiles_ph_2012</th>\n",
       "      <th>txtfiles_pk_2012</th>\n",
       "      <th>txtfiles_sg_2012</th>\n",
       "      <th>txtfiles_tz_2012</th>\n",
       "      <th>txtfiles_us_2012</th>\n",
       "      <th>txtfiles_za_2012</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Its one of the closest ones I 've ever lost ...</td>\n",
       "      <td>[ The news of Manzur Ahmed 's sudden passing a...</td>\n",
       "      <td>[ shutter ,  I 'm thrilled with it , ,  It 's ...</td>\n",
       "      <td>[ swimming pool ,  Watch out ! ,  Poor kids , ...</td>\n",
       "      <td>[ reviewing the conformity to the law of any p...</td>\n",
       "      <td>[ I 'm probably the worst interviewee you can ...</td>\n",
       "      <td>[ I never have to pay for it -- maybe that 's ...</td>\n",
       "      <td>[ I agree with seventy per cent of Obama 's vi...</td>\n",
       "      <td>[ We have began to look at succession planning...</td>\n",
       "      <td>[ My friend , this woman was born with a ' red...</td>\n",
       "      <td>[ Nehru Memorial Scholarship Scheme ,  You nee...</td>\n",
       "      <td>[ CITIES are the engines of growth ,  long sui...</td>\n",
       "      <td>[ International Thief Thief ,  Lady . ,  Send ...</td>\n",
       "      <td>[ Hopium ,  value added ,  profit for the coun...</td>\n",
       "      <td>[ I 've already forgiven him , but I 'll never...</td>\n",
       "      <td>[ Cinema is the most lucrative business in Pak...</td>\n",
       "      <td>[ transform ,  touch ,  my way or the highway ...</td>\n",
       "      <td>[ At last , the government has agreed to pay 2...</td>\n",
       "      <td>[ Moles are pretty small -- not much bigger th...</td>\n",
       "      <td>[ We will always remember Tannie Ilse as the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ high-level political contest ,  reconsidered...</td>\n",
       "      <td>[ it was formed illegally ,  Once I take the o...</td>\n",
       "      <td>[ coal 's significance is due to the large ton...</td>\n",
       "      <td>[ White Walkers ,  actors act with their eyes ...</td>\n",
       "      <td>[ I see the law is now working and I do n't ca...</td>\n",
       "      <td>[ The situation is not as serious and dramatic...</td>\n",
       "      <td>[ Roach was a prime influence throughout my ca...</td>\n",
       "      <td>[ serious allegation ,  disappeared . ,  inves...</td>\n",
       "      <td>[ To me that gives it prominence and it could ...</td>\n",
       "      <td>[ We have forged local and international partn...</td>\n",
       "      <td>[ I visited Colombo airport today to observe t...</td>\n",
       "      <td>[ It was a wildly exaggerated story deliberate...</td>\n",
       "      <td>[  Upon a literal perception , this section pr...</td>\n",
       "      <td>[ daddy 's a dark riddle/mama 's a head full o...</td>\n",
       "      <td>[ Through this system , we can easily identify...</td>\n",
       "      <td>[ can change the mind-set of Pakistanis so the...</td>\n",
       "      <td>[ Seriously if I would 've known that I hit th...</td>\n",
       "      <td>[ Every cable operator other than the two have...</td>\n",
       "      <td>[ the last time we made a generational change ...</td>\n",
       "      <td>[ experiential and testimonial-style ,  Be Now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ Urghh I get it ! Pixels look cool ,  pots , ...</td>\n",
       "      <td>[ Tamim was in our team because he is one of t...</td>\n",
       "      <td>[ I ,  canary in the coal mine , ,  Children w...</td>\n",
       "      <td>[ Noel Gallagher 's High Flying Birds ,  God-l...</td>\n",
       "      <td>[ There has never been a movement where the le...</td>\n",
       "      <td>[ China model ,  state capitalism ,  market ca...</td>\n",
       "      <td>[ I would ask the court to take into considera...</td>\n",
       "      <td>[ Sakshaar Bharat Yatra ,  Rai Shumari Fowran ...</td>\n",
       "      <td>[ We must all work together in a sustained eff...</td>\n",
       "      <td>[ The Kikuyu , Luhya , Kalenjin , Luo and Kamb...</td>\n",
       "      <td>[ Police announce major changes ,  The Doric ,...</td>\n",
       "      <td>[ Yet , the number of pledgers is only about 0...</td>\n",
       "      <td>[ for the hydrological and water resources sec...</td>\n",
       "      <td>[ We get that question a lot . It 's hard to s...</td>\n",
       "      <td>[ It is unfortunate that plastic items -- led ...</td>\n",
       "      <td>[ Satanic Game ,  your ,  Basant of Sufis ,  R...</td>\n",
       "      <td>[ With the Kindergarten Education Act now in p...</td>\n",
       "      <td>[ Return to School policy ,  Since the closure...</td>\n",
       "      <td>[ day dreamers who craft folkie pop songs ,  O...</td>\n",
       "      <td>[ You need to show that the Black Business Cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ Rupert ,  akin to a small group of generals ...</td>\n",
       "      <td>[ Such programmes will inspire young learners ...</td>\n",
       "      <td>[ A ,  Probably the most significant factor th...</td>\n",
       "      <td>[ wheels were already falling off ,  The only ...</td>\n",
       "      <td>[ Firstly , the love shown to mankind by our h...</td>\n",
       "      <td>[ rumors ,  those who fabricate or spread rumo...</td>\n",
       "      <td>[ Steamy Windows . ,  Steamy windows in here ,...</td>\n",
       "      <td>[ CSI-TA , in its capacity as a company is in ...</td>\n",
       "      <td>[ informer fi dead ,  intolerable violation of...</td>\n",
       "      <td>[ in situ ,  We are giving the first steps in ...</td>\n",
       "      <td>[  Kumar Sangakkara ( Deccan Chargers ) : Sang...</td>\n",
       "      <td>[ ballsy ,  Congrats Ze ! You rocked the place...</td>\n",
       "      <td>[ Nov/Dec ( GCE ) NECO is designed for externa...</td>\n",
       "      <td>[ zero ,  zero , or very close to zero ,  tax ...</td>\n",
       "      <td>[ protective custody ,  Saffron Revolution ,  ...</td>\n",
       "      <td>[ No time is spent in identifying opportunitie...</td>\n",
       "      <td>[ non-recognized ,  Our research is aimed at b...</td>\n",
       "      <td>[ Indeed Muvek through RIU have been an eye op...</td>\n",
       "      <td>[ When I first went into exile , I looked at t...</td>\n",
       "      <td>[ He was married twice before his current marr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ One of the most curious spots I have ever se...</td>\n",
       "      <td>[ I cultivated boro paddy on 17 acres of land ...</td>\n",
       "      <td>[ close ,  casual ,  acquaintances ,  rule in ...</td>\n",
       "      <td>[ trapped ,  my plans were to stay here for 10...</td>\n",
       "      <td>[ I am assuring you of my determination to ens...</td>\n",
       "      <td>[ We 've lurched from what was originally a so...</td>\n",
       "      <td>[ I am a Jewish sow ,  Prometheus , ,  It woul...</td>\n",
       "      <td>[ to identify the basic principles that should...</td>\n",
       "      <td>[ They should not have been at school ... , , ...</td>\n",
       "      <td>[ The Media Council of Kenya and other relevan...</td>\n",
       "      <td>[ Ranaviru Suwanda ,  Ranaviru Suwanda ,  Rana...</td>\n",
       "      <td>[ Lobster man ,  Hidup , hidup , hidup Bersih ...</td>\n",
       "      <td>[ Kola nut is our own prayer book . We wake up...</td>\n",
       "      <td>[ we should buy our ticket from somewhere not ...</td>\n",
       "      <td>[ rap ,  ripple ,  The larger your shelf , the...</td>\n",
       "      <td>[ My Shikarpur was not like the one you see to...</td>\n",
       "      <td>[ his good intentions ,  The only realistic wa...</td>\n",
       "      <td>[ child watchers ,  the most interesting work ...</td>\n",
       "      <td>[ Humans of New York . ,  Every portrait is a ...</td>\n",
       "      <td>[ The installation of the mobile system has be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ generic ,  as is ,  Opt out of Registration ...</td>\n",
       "      <td>[ the form of child abuse in which an adult or...</td>\n",
       "      <td>[ not quite as the Beatles had intended . ,  g...</td>\n",
       "      <td>[ We need assurances that the necessary steps ...</td>\n",
       "      <td>[ Two More Vaccines for Healthier , Happier Ch...</td>\n",
       "      <td>[ public opinion guidance ,  interview , ,  re...</td>\n",
       "      <td>[ Lord of the Dance ,  Photography and ' Lord ...</td>\n",
       "      <td>[ Big Four ,  Big Four ,  Snakebite Mortality ...</td>\n",
       "      <td>[ tired , weary , and poor ,  liberty to apply...</td>\n",
       "      <td>[ Developing LED Lighting Technology and Pract...</td>\n",
       "      <td>[ Princess of Lanka ,  If country as a whole w...</td>\n",
       "      <td>[ swindled his gloriously connected Kazakh ste...</td>\n",
       "      <td>[ Goldberg came from the quest to satisfy all ...</td>\n",
       "      <td>[ It is increasingly clear that two incomes ha...</td>\n",
       "      <td>[ there 's a strong musical tradition here . T...</td>\n",
       "      <td>[ He was one of the prominent drug lords in th...</td>\n",
       "      <td>[ sweetspot ,  hot surface ,  Pitcairn ,  Beca...</td>\n",
       "      <td>[ Japan , China , US and other developed count...</td>\n",
       "      <td>[ It 's definitely not just the downtown guys ...</td>\n",
       "      <td>[ coolness ,  The Second Coming ,  Submit ,  P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ precise ,  soda straw ,  data crush ,  kill ...</td>\n",
       "      <td>[ There are a few formalities that are not com...</td>\n",
       "      <td>[ Two hundred years ago , the United States in...</td>\n",
       "      <td>[ he might hear complaints which might not oth...</td>\n",
       "      <td>[ galamsey ,  Okada ,  motor king ,  Water Sup...</td>\n",
       "      <td>[ It 's a major occasion , ,  Maybe he did it ...</td>\n",
       "      <td>[ Whether or not the city council of Galway , ...</td>\n",
       "      <td>[ while we make this transition in our name , ...</td>\n",
       "      <td>[ .  Noting that small states receive a much h...</td>\n",
       "      <td>[ Panama disease ,  Panama disease ,  Panama d...</td>\n",
       "      <td>[ one of the darkest blots on India 's develop...</td>\n",
       "      <td>[ Malaysia ,  helping ,  We abolished ISA beca...</td>\n",
       "      <td>[ Approved Estimates of Akwa Ibom State for 20...</td>\n",
       "      <td>[ safe to say ,  light anomalies ,  I think it...</td>\n",
       "      <td>[ to sell everything made out of corn . ,  We ...</td>\n",
       "      <td>[ Control drugs -- control crimes ,  safely , ...</td>\n",
       "      <td>[ The basic message to permanent residents is ...</td>\n",
       "      <td>[ how are we going to ensure there is robust e...</td>\n",
       "      <td>[ Ignite the Night . ,  Largest Free Concert ....</td>\n",
       "      <td>[ I love my baby , but I wanted what was best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ planned fighting patrol ,  battle ,  Crash ,...</td>\n",
       "      <td>[ stateless ,  The US government has and will ...</td>\n",
       "      <td>[ In high school my friends and I were all abo...</td>\n",
       "      <td>[ invention ,  Ora pro me , beata Martin ,  Al...</td>\n",
       "      <td>[ The marriage was @ @ @ @ @ @ @ @ @ @ Meanwhi...</td>\n",
       "      <td>[ not using one 's best efforts to win a match...</td>\n",
       "      <td>[ Unemployment impacts on people in many diffe...</td>\n",
       "      <td>[ It also specifies the dates on which differe...</td>\n",
       "      <td>[ The established church and chapels in this t...</td>\n",
       "      <td>[ he said .  The 12km road upgrade project is ...</td>\n",
       "      <td>[ Finance is one of the most crucial areas at ...</td>\n",
       "      <td>[ Those diagnosed early with mental health pro...</td>\n",
       "      <td>[ Justice Mariam Mukhtar , has worked twice as...</td>\n",
       "      <td>[ she 'll be right ,  FarmSafe ,  She 'll be r...</td>\n",
       "      <td>[ the use of any form of plastic bags on dry g...</td>\n",
       "      <td>[ most foreigners writing about Pakistan are i...</td>\n",
       "      <td>[ progress report ,  due to the surrounding ci...</td>\n",
       "      <td>[ I assure to our members especially at all di...</td>\n",
       "      <td>[ ridiculous claim ,  patently false ,  This i...</td>\n",
       "      <td>[ other devices ,  they will have to sell an a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ @ @ @ @ @ @ @ @ @ @ this star will be dedica...</td>\n",
       "      <td>[ safe cricket ,  One has to understand that t...</td>\n",
       "      <td>[ Did you hear that ? ,  Participatory expedit...</td>\n",
       "      <td>[ 50-year puzzle ,  sincere regret and deep sy...</td>\n",
       "      <td>[ All persons shall be equal before the law . ...</td>\n",
       "      <td>[ brainwash ,  moral and national education , ...</td>\n",
       "      <td>[ a rocky place , ,  It is a country where the...</td>\n",
       "      <td>[ This is the lifeline for people who are livi...</td>\n",
       "      <td>[ Natty Morgan ,  Natty Kris ,  Ah going fi mi...</td>\n",
       "      <td>[ intruder ,  according to his wishes ,  I wou...</td>\n",
       "      <td>[ Scenes of nature are my passion , ,  It requ...</td>\n",
       "      <td>[ We will rock you ,  YMCA ,  So , who bent th...</td>\n",
       "      <td>[ In this era of Blackberry phones , both the ...</td>\n",
       "      <td>[ past my use-by date ,  I have sought legal a...</td>\n",
       "      <td>[ The State shall adopt a comprehensive approa...</td>\n",
       "      <td>[ I get it cracking like a bad back , b**** ta...</td>\n",
       "      <td>[ I think the faculty want me out , ,  no conf...</td>\n",
       "      <td>[ Will floods affect Dar es Salaam City Again ...</td>\n",
       "      <td>[ Devastated . ,  Gossip Girl . ,  businesses ...</td>\n",
       "      <td>[ professionalising ,  disingenuous ,  My @ @ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    txtfiles_au_2012  \\\n",
       "0  [ Its one of the closest ones I 've ever lost ...   \n",
       "1  [ high-level political contest ,  reconsidered...   \n",
       "2  [ Urghh I get it ! Pixels look cool ,  pots , ...   \n",
       "3  [ Rupert ,  akin to a small group of generals ...   \n",
       "4  [ One of the most curious spots I have ever se...   \n",
       "5  [ generic ,  as is ,  Opt out of Registration ...   \n",
       "6  [ precise ,  soda straw ,  data crush ,  kill ...   \n",
       "7  [ planned fighting patrol ,  battle ,  Crash ,...   \n",
       "8  [ @ @ @ @ @ @ @ @ @ @ this star will be dedica...   \n",
       "\n",
       "                                    txtfiles_bd_2012  \\\n",
       "0  [ The news of Manzur Ahmed 's sudden passing a...   \n",
       "1  [ it was formed illegally ,  Once I take the o...   \n",
       "2  [ Tamim was in our team because he is one of t...   \n",
       "3  [ Such programmes will inspire young learners ...   \n",
       "4  [ I cultivated boro paddy on 17 acres of land ...   \n",
       "5  [ the form of child abuse in which an adult or...   \n",
       "6  [ There are a few formalities that are not com...   \n",
       "7  [ stateless ,  The US government has and will ...   \n",
       "8  [ safe cricket ,  One has to understand that t...   \n",
       "\n",
       "                                    txtfiles_ca_2012  \\\n",
       "0  [ shutter ,  I 'm thrilled with it , ,  It 's ...   \n",
       "1  [ coal 's significance is due to the large ton...   \n",
       "2  [ I ,  canary in the coal mine , ,  Children w...   \n",
       "3  [ A ,  Probably the most significant factor th...   \n",
       "4  [ close ,  casual ,  acquaintances ,  rule in ...   \n",
       "5  [ not quite as the Beatles had intended . ,  g...   \n",
       "6  [ Two hundred years ago , the United States in...   \n",
       "7  [ In high school my friends and I were all abo...   \n",
       "8  [ Did you hear that ? ,  Participatory expedit...   \n",
       "\n",
       "                                    txtfiles_gb_2012  \\\n",
       "0  [ swimming pool ,  Watch out ! ,  Poor kids , ...   \n",
       "1  [ White Walkers ,  actors act with their eyes ...   \n",
       "2  [ Noel Gallagher 's High Flying Birds ,  God-l...   \n",
       "3  [ wheels were already falling off ,  The only ...   \n",
       "4  [ trapped ,  my plans were to stay here for 10...   \n",
       "5  [ We need assurances that the necessary steps ...   \n",
       "6  [ he might hear complaints which might not oth...   \n",
       "7  [ invention ,  Ora pro me , beata Martin ,  Al...   \n",
       "8  [ 50-year puzzle ,  sincere regret and deep sy...   \n",
       "\n",
       "                                    txtfiles_gh_2012  \\\n",
       "0  [ reviewing the conformity to the law of any p...   \n",
       "1  [ I see the law is now working and I do n't ca...   \n",
       "2  [ There has never been a movement where the le...   \n",
       "3  [ Firstly , the love shown to mankind by our h...   \n",
       "4  [ I am assuring you of my determination to ens...   \n",
       "5  [ Two More Vaccines for Healthier , Happier Ch...   \n",
       "6  [ galamsey ,  Okada ,  motor king ,  Water Sup...   \n",
       "7  [ The marriage was @ @ @ @ @ @ @ @ @ @ Meanwhi...   \n",
       "8  [ All persons shall be equal before the law . ...   \n",
       "\n",
       "                                    txtfiles_hk_2012  \\\n",
       "0  [ I 'm probably the worst interviewee you can ...   \n",
       "1  [ The situation is not as serious and dramatic...   \n",
       "2  [ China model ,  state capitalism ,  market ca...   \n",
       "3  [ rumors ,  those who fabricate or spread rumo...   \n",
       "4  [ We 've lurched from what was originally a so...   \n",
       "5  [ public opinion guidance ,  interview , ,  re...   \n",
       "6  [ It 's a major occasion , ,  Maybe he did it ...   \n",
       "7  [ not using one 's best efforts to win a match...   \n",
       "8  [ brainwash ,  moral and national education , ...   \n",
       "\n",
       "                                    txtfiles_ie_2012  \\\n",
       "0  [ I never have to pay for it -- maybe that 's ...   \n",
       "1  [ Roach was a prime influence throughout my ca...   \n",
       "2  [ I would ask the court to take into considera...   \n",
       "3  [ Steamy Windows . ,  Steamy windows in here ,...   \n",
       "4  [ I am a Jewish sow ,  Prometheus , ,  It woul...   \n",
       "5  [ Lord of the Dance ,  Photography and ' Lord ...   \n",
       "6  [ Whether or not the city council of Galway , ...   \n",
       "7  [ Unemployment impacts on people in many diffe...   \n",
       "8  [ a rocky place , ,  It is a country where the...   \n",
       "\n",
       "                                    txtfiles_in_2012  \\\n",
       "0  [ I agree with seventy per cent of Obama 's vi...   \n",
       "1  [ serious allegation ,  disappeared . ,  inves...   \n",
       "2  [ Sakshaar Bharat Yatra ,  Rai Shumari Fowran ...   \n",
       "3  [ CSI-TA , in its capacity as a company is in ...   \n",
       "4  [ to identify the basic principles that should...   \n",
       "5  [ Big Four ,  Big Four ,  Snakebite Mortality ...   \n",
       "6  [ while we make this transition in our name , ...   \n",
       "7  [ It also specifies the dates on which differe...   \n",
       "8  [ This is the lifeline for people who are livi...   \n",
       "\n",
       "                                    txtfiles_jm_2012  \\\n",
       "0  [ We have began to look at succession planning...   \n",
       "1  [ To me that gives it prominence and it could ...   \n",
       "2  [ We must all work together in a sustained eff...   \n",
       "3  [ informer fi dead ,  intolerable violation of...   \n",
       "4  [ They should not have been at school ... , , ...   \n",
       "5  [ tired , weary , and poor ,  liberty to apply...   \n",
       "6  [ .  Noting that small states receive a much h...   \n",
       "7  [ The established church and chapels in this t...   \n",
       "8  [ Natty Morgan ,  Natty Kris ,  Ah going fi mi...   \n",
       "\n",
       "                                    txtfiles_ke_2012  \\\n",
       "0  [ My friend , this woman was born with a ' red...   \n",
       "1  [ We have forged local and international partn...   \n",
       "2  [ The Kikuyu , Luhya , Kalenjin , Luo and Kamb...   \n",
       "3  [ in situ ,  We are giving the first steps in ...   \n",
       "4  [ The Media Council of Kenya and other relevan...   \n",
       "5  [ Developing LED Lighting Technology and Pract...   \n",
       "6  [ Panama disease ,  Panama disease ,  Panama d...   \n",
       "7  [ he said .  The 12km road upgrade project is ...   \n",
       "8  [ intruder ,  according to his wishes ,  I wou...   \n",
       "\n",
       "                                    txtfiles_lk_2012  \\\n",
       "0  [ Nehru Memorial Scholarship Scheme ,  You nee...   \n",
       "1  [ I visited Colombo airport today to observe t...   \n",
       "2  [ Police announce major changes ,  The Doric ,...   \n",
       "3  [  Kumar Sangakkara ( Deccan Chargers ) : Sang...   \n",
       "4  [ Ranaviru Suwanda ,  Ranaviru Suwanda ,  Rana...   \n",
       "5  [ Princess of Lanka ,  If country as a whole w...   \n",
       "6  [ one of the darkest blots on India 's develop...   \n",
       "7  [ Finance is one of the most crucial areas at ...   \n",
       "8  [ Scenes of nature are my passion , ,  It requ...   \n",
       "\n",
       "                                    txtfiles_my_2012  \\\n",
       "0  [ CITIES are the engines of growth ,  long sui...   \n",
       "1  [ It was a wildly exaggerated story deliberate...   \n",
       "2  [ Yet , the number of pledgers is only about 0...   \n",
       "3  [ ballsy ,  Congrats Ze ! You rocked the place...   \n",
       "4  [ Lobster man ,  Hidup , hidup , hidup Bersih ...   \n",
       "5  [ swindled his gloriously connected Kazakh ste...   \n",
       "6  [ Malaysia ,  helping ,  We abolished ISA beca...   \n",
       "7  [ Those diagnosed early with mental health pro...   \n",
       "8  [ We will rock you ,  YMCA ,  So , who bent th...   \n",
       "\n",
       "                                    txtfiles_ng_2012  \\\n",
       "0  [ International Thief Thief ,  Lady . ,  Send ...   \n",
       "1  [  Upon a literal perception , this section pr...   \n",
       "2  [ for the hydrological and water resources sec...   \n",
       "3  [ Nov/Dec ( GCE ) NECO is designed for externa...   \n",
       "4  [ Kola nut is our own prayer book . We wake up...   \n",
       "5  [ Goldberg came from the quest to satisfy all ...   \n",
       "6  [ Approved Estimates of Akwa Ibom State for 20...   \n",
       "7  [ Justice Mariam Mukhtar , has worked twice as...   \n",
       "8  [ In this era of Blackberry phones , both the ...   \n",
       "\n",
       "                                    txtfiles_nz_2012  \\\n",
       "0  [ Hopium ,  value added ,  profit for the coun...   \n",
       "1  [ daddy 's a dark riddle/mama 's a head full o...   \n",
       "2  [ We get that question a lot . It 's hard to s...   \n",
       "3  [ zero ,  zero , or very close to zero ,  tax ...   \n",
       "4  [ we should buy our ticket from somewhere not ...   \n",
       "5  [ It is increasingly clear that two incomes ha...   \n",
       "6  [ safe to say ,  light anomalies ,  I think it...   \n",
       "7  [ she 'll be right ,  FarmSafe ,  She 'll be r...   \n",
       "8  [ past my use-by date ,  I have sought legal a...   \n",
       "\n",
       "                                    txtfiles_ph_2012  \\\n",
       "0  [ I 've already forgiven him , but I 'll never...   \n",
       "1  [ Through this system , we can easily identify...   \n",
       "2  [ It is unfortunate that plastic items -- led ...   \n",
       "3  [ protective custody ,  Saffron Revolution ,  ...   \n",
       "4  [ rap ,  ripple ,  The larger your shelf , the...   \n",
       "5  [ there 's a strong musical tradition here . T...   \n",
       "6  [ to sell everything made out of corn . ,  We ...   \n",
       "7  [ the use of any form of plastic bags on dry g...   \n",
       "8  [ The State shall adopt a comprehensive approa...   \n",
       "\n",
       "                                    txtfiles_pk_2012  \\\n",
       "0  [ Cinema is the most lucrative business in Pak...   \n",
       "1  [ can change the mind-set of Pakistanis so the...   \n",
       "2  [ Satanic Game ,  your ,  Basant of Sufis ,  R...   \n",
       "3  [ No time is spent in identifying opportunitie...   \n",
       "4  [ My Shikarpur was not like the one you see to...   \n",
       "5  [ He was one of the prominent drug lords in th...   \n",
       "6  [ Control drugs -- control crimes ,  safely , ...   \n",
       "7  [ most foreigners writing about Pakistan are i...   \n",
       "8  [ I get it cracking like a bad back , b**** ta...   \n",
       "\n",
       "                                    txtfiles_sg_2012  \\\n",
       "0  [ transform ,  touch ,  my way or the highway ...   \n",
       "1  [ Seriously if I would 've known that I hit th...   \n",
       "2  [ With the Kindergarten Education Act now in p...   \n",
       "3  [ non-recognized ,  Our research is aimed at b...   \n",
       "4  [ his good intentions ,  The only realistic wa...   \n",
       "5  [ sweetspot ,  hot surface ,  Pitcairn ,  Beca...   \n",
       "6  [ The basic message to permanent residents is ...   \n",
       "7  [ progress report ,  due to the surrounding ci...   \n",
       "8  [ I think the faculty want me out , ,  no conf...   \n",
       "\n",
       "                                    txtfiles_tz_2012  \\\n",
       "0  [ At last , the government has agreed to pay 2...   \n",
       "1  [ Every cable operator other than the two have...   \n",
       "2  [ Return to School policy ,  Since the closure...   \n",
       "3  [ Indeed Muvek through RIU have been an eye op...   \n",
       "4  [ child watchers ,  the most interesting work ...   \n",
       "5  [ Japan , China , US and other developed count...   \n",
       "6  [ how are we going to ensure there is robust e...   \n",
       "7  [ I assure to our members especially at all di...   \n",
       "8  [ Will floods affect Dar es Salaam City Again ...   \n",
       "\n",
       "                                    txtfiles_us_2012  \\\n",
       "0  [ Moles are pretty small -- not much bigger th...   \n",
       "1  [ the last time we made a generational change ...   \n",
       "2  [ day dreamers who craft folkie pop songs ,  O...   \n",
       "3  [ When I first went into exile , I looked at t...   \n",
       "4  [ Humans of New York . ,  Every portrait is a ...   \n",
       "5  [ It 's definitely not just the downtown guys ...   \n",
       "6  [ Ignite the Night . ,  Largest Free Concert ....   \n",
       "7  [ ridiculous claim ,  patently false ,  This i...   \n",
       "8  [ Devastated . ,  Gossip Girl . ,  businesses ...   \n",
       "\n",
       "                                    txtfiles_za_2012  \n",
       "0  [ We will always remember Tannie Ilse as the p...  \n",
       "1  [ experiential and testimonial-style ,  Be Now...  \n",
       "2  [ You need to show that the Black Business Cou...  \n",
       "3  [ He was married twice before his current marr...  \n",
       "4  [ The installation of the mobile system has be...  \n",
       "5  [ coolness ,  The Second Coming ,  Submit ,  P...  \n",
       "6  [ I love my baby , but I wanted what was best ...  \n",
       "7  [ other devices ,  they will have to sell an a...  \n",
       "8  [ professionalising ,  disingenuous ,  My @ @ ...  "
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2013</th>\n",
       "      <th>txtfiles_bd_2013</th>\n",
       "      <th>txtfiles_ca_2013</th>\n",
       "      <th>txtfiles_gb_2013</th>\n",
       "      <th>txtfiles_gh_2013</th>\n",
       "      <th>txtfiles_hk_2013</th>\n",
       "      <th>txtfiles_ie_2013</th>\n",
       "      <th>txtfiles_in_2013</th>\n",
       "      <th>txtfiles_jm_2013</th>\n",
       "      <th>txtfiles_ke_2013</th>\n",
       "      <th>txtfiles_lk_2013</th>\n",
       "      <th>txtfiles_my_2013</th>\n",
       "      <th>txtfiles_ng_2013</th>\n",
       "      <th>txtfiles_nz_2013</th>\n",
       "      <th>txtfiles_ph_2013</th>\n",
       "      <th>txtfiles_pk_2013</th>\n",
       "      <th>txtfiles_sg_2013</th>\n",
       "      <th>txtfiles_tz_2013</th>\n",
       "      <th>txtfiles_us_2013</th>\n",
       "      <th>txtfiles_za_2013</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ time left ,  all we can do ,  I was just thi...</td>\n",
       "      <td>[ If we consider the employment scenario in Ba...</td>\n",
       "      <td>[ I had actually wanted to quit after SARS , ,...</td>\n",
       "      <td>[ These findings illustrate how much family li...</td>\n",
       "      <td>[ population quota ,  military obedience ,  wi...</td>\n",
       "      <td>[ normal ,  re-shoring ,  business ,  blacklis...</td>\n",
       "      <td>[ Andrew , I can not describe how much I miss ...</td>\n",
       "      <td>[ ,   Katrina Kaif won the most profitable act...</td>\n",
       "      <td>[ best practice , ,  Men want women who will a...</td>\n",
       "      <td>[ ... dedicated to all those who have fallen i...</td>\n",
       "      <td>[ cakes ,  We are still not exhausted with our...</td>\n",
       "      <td>[ Global Food Losses and Food Waste ,  The amo...</td>\n",
       "      <td>[ NADECO Bishop ,  The law gives the DPR the r...</td>\n",
       "      <td>[ normal child ,  It really worries me @ @ @ @...</td>\n",
       "      <td>[ might have died from a heart attack because ...</td>\n",
       "      <td>[ That is 25,000 new child brides every day fo...</td>\n",
       "      <td>[ Black Beauty ,  The age of NWA 7034 is impor...</td>\n",
       "      <td>[ ... we have the skills to use computers , co...</td>\n",
       "      <td>[ Today is Christmas Eve . Today is my birthda...</td>\n",
       "      <td>[ Integrating these farms into value chains no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ Until recently the events of this day have r...</td>\n",
       "      <td>[ international ,  Operation Search Light ,  T...</td>\n",
       "      <td>[ I was quite surprised that only about three ...</td>\n",
       "      <td>[ law men ,  nisi per legale judicium parium s...</td>\n",
       "      <td>[ it is very difficult to acquire a passport b...</td>\n",
       "      <td>[ Lost in Thailand ,  Journey to the West : Co...</td>\n",
       "      <td>[ Lincoln ,  My Left Foot ,  There will be Blo...</td>\n",
       "      <td>[ It is a rebirth for me here , this home is m...</td>\n",
       "      <td>[ decreasing debt is a marathon , not a sprint...</td>\n",
       "      <td>[ The master plan for development of Hillcrest...</td>\n",
       "      <td>[ we recognize their right , the right of ever...</td>\n",
       "      <td>[ said Prince Robert Louis Francois Marie of L...</td>\n",
       "      <td>[ Justice Fatimo Akinbami who read the judgeme...</td>\n",
       "      <td>[ It 's tough . But it 's also spiritual and I...</td>\n",
       "      <td>[ Just a couple of weeks ago we said that LTE ...</td>\n",
       "      <td>[ When Dr Shaikh saw me , she told me she coul...</td>\n",
       "      <td>[ We are very proud that TAC has entrusted our...</td>\n",
       "      <td>[ So , this program will educate the public ab...</td>\n",
       "      <td>[ I did n't think it was really an accurate do...</td>\n",
       "      <td>[ for the purpose of this application , at thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ The shortage is likely to be felt by the mor...</td>\n",
       "      <td>[ The people of Kurigram have been dumbfounded...</td>\n",
       "      <td>[ urgent measures ,  The trouble is , the numb...</td>\n",
       "      <td>[ minimal ,  My ship is the Lollipop . It 's a...</td>\n",
       "      <td>[ mosaic plague ,  gnashing his teeth ' ' in a...</td>\n",
       "      <td>[ Regulation on Commercial Registration in the...</td>\n",
       "      <td>[ Does semen have antidepressant properties ? ...</td>\n",
       "      <td>[ The Attacks of 26/11 ,  insensitive ,  gory ...</td>\n",
       "      <td>[ Order in society , we find , is much better ...</td>\n",
       "      <td>[ Trans-local Peacebuilding among Pastoralists...</td>\n",
       "      <td>[ removed ,  Politically we are on the opposin...</td>\n",
       "      <td>[ I won because I attacked better than him . T...</td>\n",
       "      <td>[ compared with third quarter results , the ec...</td>\n",
       "      <td>[ To be honest , at first we kind of just soun...</td>\n",
       "      <td>[ world capital of political dynasties , ,  st...</td>\n",
       "      <td>[ differencing disks , ,  flings . ,  controll...</td>\n",
       "      <td>[ It is important to have proactive and respon...</td>\n",
       "      <td>[ This is a United Nations Board of Auditors (...</td>\n",
       "      <td>[ New South . ,  Southerners may live in the h...</td>\n",
       "      <td>[ Essentially , all these increases include an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ This car sets new benchmarks again and again...</td>\n",
       "      <td>[ He was standing near the court where I was p...</td>\n",
       "      <td>[ Harper 's handicap ,  Harper 's handicap ,  ...</td>\n",
       "      <td>[ ignorant of the countryside ,  brilliant ,  ...</td>\n",
       "      <td>[ career opportunities in the legal profession...</td>\n",
       "      <td>[ You can now get on Facebook and Twitter in C...</td>\n",
       "      <td>[ team names changed from year to year , team ...</td>\n",
       "      <td>[ Sunny ,  not a chest of Indigo reached Engla...</td>\n",
       "      <td>[ This is our ninth staging . We were in Lland...</td>\n",
       "      <td>[ scavengers ,  The problem of plastics is muc...</td>\n",
       "      <td>[ he said .  ,  he said .  Demanding an inquir...</td>\n",
       "      <td>[ It 's because Malaysian households now are v...</td>\n",
       "      <td>[ The Position of the Executive Council of Pla...</td>\n",
       "      <td>[ opening the door to abuse of power and impun...</td>\n",
       "      <td>[ hats ,  best and brightest ,  Bureaucracy , ...</td>\n",
       "      <td>[ surveillance ,  Even the judges and lawyers ...</td>\n",
       "      <td>[ Real chance ,  It 's easy to build ( new haw...</td>\n",
       "      <td>[ SUR Peru was the second channel launched by ...</td>\n",
       "      <td>[ Brother , ,  Or before winter 's done , ,  e...</td>\n",
       "      <td>[ wastes ,  another PARC ,  cat ,  yes ,  Ewe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ Unit 61398 ,  Industrial Control Systems ,  ...</td>\n",
       "      <td>[ The burial time was set for 10:00am , but it...</td>\n",
       "      <td>[ He requires constant supervision and because...</td>\n",
       "      <td>[ I still ca n't believe how making has taken ...</td>\n",
       "      <td>[ Act 462 Section 5 of the Constitution enshri...</td>\n",
       "      <td>[ The Treasury 's analysis of the currency opt...</td>\n",
       "      <td>[ look ,  wo n't they get in the way ? ,  sett...</td>\n",
       "      <td>[ Unfortunately ground water is like a bank , ...</td>\n",
       "      <td>[ There is no such loan ceiling set by either ...</td>\n",
       "      <td>[ Albanianization . ,  physical , political , ...</td>\n",
       "      <td>[ Buddhist Taliban ,  Justice , Peace and Huma...</td>\n",
       "      <td>[ get out the vote ,  friends ,  The flights i...</td>\n",
       "      <td>[ I can tell you with every sense of responsib...</td>\n",
       "      <td>[ It looked like Molenberg , ,  It was folded ...</td>\n",
       "      <td>[ In view of the failure of the prosecution to...</td>\n",
       "      <td>[ Chaudhry Munir has virtually edged Jafar Iqb...</td>\n",
       "      <td>[ food versus fuel ,  The production and use o...</td>\n",
       "      <td>[ If this poaching trend is left unchecked , o...</td>\n",
       "      <td>[ Big Data ,  Data Scientist . ,  ask ,  is , ...</td>\n",
       "      <td>[ I am delighted that I now have my good name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ leaning in ,  having it all ,  snap frozen ,...</td>\n",
       "      <td>[ Mohammad Ashraful has admitted to his guilt ...</td>\n",
       "      <td>[ American through and through ,  It will , in...</td>\n",
       "      <td>[ unavoidable complications ,  It 's as seriou...</td>\n",
       "      <td>[ perfume rice ,  everything made in Ghana is ...</td>\n",
       "      <td>[ Please be assured that China 's position has...</td>\n",
       "      <td>[ Ulysses ,  Phonograph record of a reading by...</td>\n",
       "      <td>[ Shiva ,  Shiva Trilogy ,  here you go ,  giv...</td>\n",
       "      <td>[ suitcase merchants ,  This is a signal honou...</td>\n",
       "      <td>[ We are not to fear the Europeans , ,  an @ @...</td>\n",
       "      <td>[ My cat is Siamese . It is beautiful , ,  My ...</td>\n",
       "      <td>[ There is a lack of skilled workers at all le...</td>\n",
       "      <td>[ It ( Nigeria ) is one of only four among the...</td>\n",
       "      <td>[ Jammies in June ,  Sadly , for some South Au...</td>\n",
       "      <td>[ ghost ,  TROs like this one effectively prev...</td>\n",
       "      <td>[ Toba Tek Singh ,  Toba Tek Singh ,  Toba Tek...</td>\n",
       "      <td>[ It will also help to reduce impulse purchase...</td>\n",
       "      <td>[ Men make their own history , but they do not...</td>\n",
       "      <td>[ reformers ,  fiscal crisis ,  portfolio ,  3...</td>\n",
       "      <td>[ she told The Mercury , a local newspaper .  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ nanny state ,  nanny state ,  rising grounds...</td>\n",
       "      <td>[ Unthinkable ,  water treatment , ,  shock tr...</td>\n",
       "      <td>[ no sense . ,  We can not manage a society li...</td>\n",
       "      <td>[ I guess you owe me one @ @ @ @ @ @ @ @ @ @ t...</td>\n",
       "      <td>[ What is freedom or better still freedom of s...</td>\n",
       "      <td>[ Russia and Indonesia declare that they wo n'...</td>\n",
       "      <td>[ headquarters ,  home ,  Many of the women ha...</td>\n",
       "      <td>[ consumer ,  for the purpose of this clause ,...</td>\n",
       "      <td>[ Memorandum of Objects and Reasons ,  It is m...</td>\n",
       "      <td>[ Big investors typically have the wherewithal...</td>\n",
       "      <td>[ Islands have a critical role to play in rela...</td>\n",
       "      <td>[ It 's good news that the price of key HIV dr...</td>\n",
       "      <td>[ Following the judgment of the Court of Appea...</td>\n",
       "      <td>[ She was surrounded by her husband , Alex , c...</td>\n",
       "      <td>[ We are invoking this right to send a strong ...</td>\n",
       "      <td>[ It 's impossible to achieve the MDG for educ...</td>\n",
       "      <td>[ representative sample ,  Near Threatened ,  ...</td>\n",
       "      <td>[ Education , Poverty and Disability in Develo...</td>\n",
       "      <td>[ A Harvest of Death , ,  The Hero of Gettysbu...</td>\n",
       "      <td>[ declaring his Kunene 's membership of the Ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ Long live the king ! ' ' as 85-year-old King...</td>\n",
       "      <td>[ Many had thought our relations with the Worl...</td>\n",
       "      <td>[ Two different officers will perceive the sit...</td>\n",
       "      <td>[ absolutely pathetic ,  fall guy ,  reduced r...</td>\n",
       "      <td>[ I would bleed and bleed to the extent that I...</td>\n",
       "      <td>[ We now have 12 pump stations with the potent...</td>\n",
       "      <td>[ There continues to be an expectation that pa...</td>\n",
       "      <td>[ In this last decade , the Indian team repres...</td>\n",
       "      <td>[ Some people ignore these signs , hoping they...</td>\n",
       "      <td>[ settled ,  Investigations have been done as ...</td>\n",
       "      <td>[ I visualize the need to follow a policy , ba...</td>\n",
       "      <td>[ red-stops ,  exclusions ,  strengthens ,  ve...</td>\n",
       "      <td>[ early Christianity was a movement built on s...</td>\n",
       "      <td>[ Pest-rid ,  Wildlife rehabilitator ,  The id...</td>\n",
       "      <td>[ The imposition @ @ @ @ @ @ @ @ @ @ current 7...</td>\n",
       "      <td>[ We have just finished the harvest and one wo...</td>\n",
       "      <td>[ Now there are definitely more tourists . Peo...</td>\n",
       "      <td>[ School heads are asked to welcome these stud...</td>\n",
       "      <td>[ Millennial ,  living at home ,  living with ...</td>\n",
       "      <td>[ Yes , I was taken aback by the negative reac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ Politicians will talk a lot about ' cost of ...</td>\n",
       "      <td>[ We have paid the fee , but none of us got ou...</td>\n",
       "      <td>[ an international transport artery that will ...</td>\n",
       "      <td>[ legal highs ,  addictions capital of Europe ...</td>\n",
       "      <td>[ Mobile Banking : The Impact of M-Pesa in Ken...</td>\n",
       "      <td>[ Rosneft is eager to ramp up its revenues fro...</td>\n",
       "      <td>[ She makes friends easily ... and she met a n...</td>\n",
       "      <td>[ Higher socio-economic strata is equally invo...</td>\n",
       "      <td>[ All that valuable and extensive run of land ...</td>\n",
       "      <td>[ While women remain a minority of combatants ...</td>\n",
       "      <td>[ It 's a proud moment and a blessing to walk ...</td>\n",
       "      <td>[ Known for his progressive views on politics ...</td>\n",
       "      <td>[ I was the first to publish report about the ...</td>\n",
       "      <td>[ boring ,  earnest ,  worthy ,  OK , but how ...</td>\n",
       "      <td>[ By taking firm action to curb the indiscrimi...</td>\n",
       "      <td>[ The PSM arrived at the present state due to ...</td>\n",
       "      <td>[ groundbreaking ,  A number of our previous s...</td>\n",
       "      <td>[ The idea of population momentum is vitally i...</td>\n",
       "      <td>[ beat a fast exit out of New York City , ,  b...</td>\n",
       "      <td>[ wrong ,  These are not profit-making institu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ I 'd try to find a mix of brand cachet versu...</td>\n",
       "      <td>[ I knew that the dogs move tail ; now I see t...</td>\n",
       "      <td>[ drink water regularly . ,  mythological orig...</td>\n",
       "      <td>[ easily the best ,  more than an ordinary bre...</td>\n",
       "      <td>[ External Influence on Ga Society and Culture...</td>\n",
       "      <td>[ Under the MOU , all of CNPC 's oil and gas b...</td>\n",
       "      <td>[ Even though I found the time since diagnosis...</td>\n",
       "      <td>[ said Neri .  President Dilma Rousseff had in...</td>\n",
       "      <td>[ We have noticed when we go into schools , te...</td>\n",
       "      <td>[ That camp has become a nursery for terrorist...</td>\n",
       "      <td>[ Family Protects You ,  No First Deployment o...</td>\n",
       "      <td>[ stopped ,  stop ,  ayes ,  @ @ @ @ @ @ @ @ @...</td>\n",
       "      <td>[ The union between Bafarawa and Wamakko was i...</td>\n",
       "      <td>[ poorly thought out ,  Housing New Zealand ad...</td>\n",
       "      <td>[ rap ,  ripple ,  The Boss , ,  Gigi ,  the d...</td>\n",
       "      <td>[ The 21st century is seeing an unprecedented ...</td>\n",
       "      <td>[ mechanism ,   Why on earth , in this shippin...</td>\n",
       "      <td>[ When my family took me to school there were ...</td>\n",
       "      <td>[ diamond field ,  on hold . ,  take a break ,...</td>\n",
       "      <td>[ busty 20-something ,  auto-level ,  ,  ,  Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ He could tell I was visibly worried , ,  Joh...</td>\n",
       "      <td>[ if the tannery industry is relocated by Dece...</td>\n",
       "      <td>[ The wampum we were to present him took our m...</td>\n",
       "      <td>[ Practically perfect in every way , ,  tart a...</td>\n",
       "      <td>[ Will a man rob God ? Yet you rob me . But yo...</td>\n",
       "      <td>[ reformatory holiday schedules . ,  back to b...</td>\n",
       "      <td>[ We 're the most charitable people in the wor...</td>\n",
       "      <td>[ Maybe it ( Nano ) gets launched in another c...</td>\n",
       "      <td>[ To qualify for the programme , an applicant ...</td>\n",
       "      <td>[ the suzerainty of His Majesty over the India...</td>\n",
       "      <td>[ those who try to seperate the country their ...</td>\n",
       "      <td>[ Legally speaking , there is no specific prov...</td>\n",
       "      <td>[ I will lead the blind by ways they have not ...</td>\n",
       "      <td>[ What makes our business unique is that we fu...</td>\n",
       "      <td>[ Yolanda , ,  Yolanda ,  There are storms tha...</td>\n",
       "      <td>[ damaged but recognisable ,  A delegation was...</td>\n",
       "      <td>[ Carbon remineralisation and sequestration in...</td>\n",
       "      <td>[ regional economic communities ( RECs ) .  Th...</td>\n",
       "      <td>[ 16 going on 17 ,  recombinant ,  Recombinant...</td>\n",
       "      <td>[ The Chilean population has 11.6 years of sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[ rapid rate ,  Health has gone up quite drama...</td>\n",
       "      <td>[ Pirates of the Caribbean : At World 's End ....</td>\n",
       "      <td>[ Put simply , we 're just not all given the s...</td>\n",
       "      <td>[ That was no problem . Of course , the other ...</td>\n",
       "      <td>[ , one of his students who is now a security ...</td>\n",
       "      <td>[ satisfactory ,  We target clients , domestic...</td>\n",
       "      <td>[ getting her head around ,  craic ,  . . . wh...</td>\n",
       "      <td>[ You know how people look at social sciences ...</td>\n",
       "      <td>[ a victimless crime ,  mandatory imprisonment...</td>\n",
       "      <td>[ drop in at the house of an age mate and even...</td>\n",
       "      <td>[ Super LARC ,  I will gather first-hand infor...</td>\n",
       "      <td>[ the veteran lawmaker told The Malaysian Insi...</td>\n",
       "      <td>[ Development University ,  development univer...</td>\n",
       "      <td>[ I hate talking about myself , I find it such...</td>\n",
       "      <td>[ YOLANDA ,  There is no more future for me in...</td>\n",
       "      <td>[ When we share videos and images on the inter...</td>\n",
       "      <td>[ Job 's instincts were against white products...</td>\n",
       "      <td>[ A Golden Opportunity ? How Tanzania is faili...</td>\n",
       "      <td>[ Even though it 's paid well , you are sinkin...</td>\n",
       "      <td>[ It is not possible for the trust to sustain ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     txtfiles_au_2013  \\\n",
       "0   [ time left ,  all we can do ,  I was just thi...   \n",
       "1   [ Until recently the events of this day have r...   \n",
       "2   [ The shortage is likely to be felt by the mor...   \n",
       "3   [ This car sets new benchmarks again and again...   \n",
       "4   [ Unit 61398 ,  Industrial Control Systems ,  ...   \n",
       "5   [ leaning in ,  having it all ,  snap frozen ,...   \n",
       "6   [ nanny state ,  nanny state ,  rising grounds...   \n",
       "7   [ Long live the king ! ' ' as 85-year-old King...   \n",
       "8   [ Politicians will talk a lot about ' cost of ...   \n",
       "9   [ I 'd try to find a mix of brand cachet versu...   \n",
       "10  [ He could tell I was visibly worried , ,  Joh...   \n",
       "11  [ rapid rate ,  Health has gone up quite drama...   \n",
       "\n",
       "                                     txtfiles_bd_2013  \\\n",
       "0   [ If we consider the employment scenario in Ba...   \n",
       "1   [ international ,  Operation Search Light ,  T...   \n",
       "2   [ The people of Kurigram have been dumbfounded...   \n",
       "3   [ He was standing near the court where I was p...   \n",
       "4   [ The burial time was set for 10:00am , but it...   \n",
       "5   [ Mohammad Ashraful has admitted to his guilt ...   \n",
       "6   [ Unthinkable ,  water treatment , ,  shock tr...   \n",
       "7   [ Many had thought our relations with the Worl...   \n",
       "8   [ We have paid the fee , but none of us got ou...   \n",
       "9   [ I knew that the dogs move tail ; now I see t...   \n",
       "10  [ if the tannery industry is relocated by Dece...   \n",
       "11  [ Pirates of the Caribbean : At World 's End ....   \n",
       "\n",
       "                                     txtfiles_ca_2013  \\\n",
       "0   [ I had actually wanted to quit after SARS , ,...   \n",
       "1   [ I was quite surprised that only about three ...   \n",
       "2   [ urgent measures ,  The trouble is , the numb...   \n",
       "3   [ Harper 's handicap ,  Harper 's handicap ,  ...   \n",
       "4   [ He requires constant supervision and because...   \n",
       "5   [ American through and through ,  It will , in...   \n",
       "6   [ no sense . ,  We can not manage a society li...   \n",
       "7   [ Two different officers will perceive the sit...   \n",
       "8   [ an international transport artery that will ...   \n",
       "9   [ drink water regularly . ,  mythological orig...   \n",
       "10  [ The wampum we were to present him took our m...   \n",
       "11  [ Put simply , we 're just not all given the s...   \n",
       "\n",
       "                                     txtfiles_gb_2013  \\\n",
       "0   [ These findings illustrate how much family li...   \n",
       "1   [ law men ,  nisi per legale judicium parium s...   \n",
       "2   [ minimal ,  My ship is the Lollipop . It 's a...   \n",
       "3   [ ignorant of the countryside ,  brilliant ,  ...   \n",
       "4   [ I still ca n't believe how making has taken ...   \n",
       "5   [ unavoidable complications ,  It 's as seriou...   \n",
       "6   [ I guess you owe me one @ @ @ @ @ @ @ @ @ @ t...   \n",
       "7   [ absolutely pathetic ,  fall guy ,  reduced r...   \n",
       "8   [ legal highs ,  addictions capital of Europe ...   \n",
       "9   [ easily the best ,  more than an ordinary bre...   \n",
       "10  [ Practically perfect in every way , ,  tart a...   \n",
       "11  [ That was no problem . Of course , the other ...   \n",
       "\n",
       "                                     txtfiles_gh_2013  \\\n",
       "0   [ population quota ,  military obedience ,  wi...   \n",
       "1   [ it is very difficult to acquire a passport b...   \n",
       "2   [ mosaic plague ,  gnashing his teeth ' ' in a...   \n",
       "3   [ career opportunities in the legal profession...   \n",
       "4   [ Act 462 Section 5 of the Constitution enshri...   \n",
       "5   [ perfume rice ,  everything made in Ghana is ...   \n",
       "6   [ What is freedom or better still freedom of s...   \n",
       "7   [ I would bleed and bleed to the extent that I...   \n",
       "8   [ Mobile Banking : The Impact of M-Pesa in Ken...   \n",
       "9   [ External Influence on Ga Society and Culture...   \n",
       "10  [ Will a man rob God ? Yet you rob me . But yo...   \n",
       "11  [ , one of his students who is now a security ...   \n",
       "\n",
       "                                     txtfiles_hk_2013  \\\n",
       "0   [ normal ,  re-shoring ,  business ,  blacklis...   \n",
       "1   [ Lost in Thailand ,  Journey to the West : Co...   \n",
       "2   [ Regulation on Commercial Registration in the...   \n",
       "3   [ You can now get on Facebook and Twitter in C...   \n",
       "4   [ The Treasury 's analysis of the currency opt...   \n",
       "5   [ Please be assured that China 's position has...   \n",
       "6   [ Russia and Indonesia declare that they wo n'...   \n",
       "7   [ We now have 12 pump stations with the potent...   \n",
       "8   [ Rosneft is eager to ramp up its revenues fro...   \n",
       "9   [ Under the MOU , all of CNPC 's oil and gas b...   \n",
       "10  [ reformatory holiday schedules . ,  back to b...   \n",
       "11  [ satisfactory ,  We target clients , domestic...   \n",
       "\n",
       "                                     txtfiles_ie_2013  \\\n",
       "0   [ Andrew , I can not describe how much I miss ...   \n",
       "1   [ Lincoln ,  My Left Foot ,  There will be Blo...   \n",
       "2   [ Does semen have antidepressant properties ? ...   \n",
       "3   [ team names changed from year to year , team ...   \n",
       "4   [ look ,  wo n't they get in the way ? ,  sett...   \n",
       "5   [ Ulysses ,  Phonograph record of a reading by...   \n",
       "6   [ headquarters ,  home ,  Many of the women ha...   \n",
       "7   [ There continues to be an expectation that pa...   \n",
       "8   [ She makes friends easily ... and she met a n...   \n",
       "9   [ Even though I found the time since diagnosis...   \n",
       "10  [ We 're the most charitable people in the wor...   \n",
       "11  [ getting her head around ,  craic ,  . . . wh...   \n",
       "\n",
       "                                     txtfiles_in_2013  \\\n",
       "0   [ ,   Katrina Kaif won the most profitable act...   \n",
       "1   [ It is a rebirth for me here , this home is m...   \n",
       "2   [ The Attacks of 26/11 ,  insensitive ,  gory ...   \n",
       "3   [ Sunny ,  not a chest of Indigo reached Engla...   \n",
       "4   [ Unfortunately ground water is like a bank , ...   \n",
       "5   [ Shiva ,  Shiva Trilogy ,  here you go ,  giv...   \n",
       "6   [ consumer ,  for the purpose of this clause ,...   \n",
       "7   [ In this last decade , the Indian team repres...   \n",
       "8   [ Higher socio-economic strata is equally invo...   \n",
       "9   [ said Neri .  President Dilma Rousseff had in...   \n",
       "10  [ Maybe it ( Nano ) gets launched in another c...   \n",
       "11  [ You know how people look at social sciences ...   \n",
       "\n",
       "                                     txtfiles_jm_2013  \\\n",
       "0   [ best practice , ,  Men want women who will a...   \n",
       "1   [ decreasing debt is a marathon , not a sprint...   \n",
       "2   [ Order in society , we find , is much better ...   \n",
       "3   [ This is our ninth staging . We were in Lland...   \n",
       "4   [ There is no such loan ceiling set by either ...   \n",
       "5   [ suitcase merchants ,  This is a signal honou...   \n",
       "6   [ Memorandum of Objects and Reasons ,  It is m...   \n",
       "7   [ Some people ignore these signs , hoping they...   \n",
       "8   [ All that valuable and extensive run of land ...   \n",
       "9   [ We have noticed when we go into schools , te...   \n",
       "10  [ To qualify for the programme , an applicant ...   \n",
       "11  [ a victimless crime ,  mandatory imprisonment...   \n",
       "\n",
       "                                     txtfiles_ke_2013  \\\n",
       "0   [ ... dedicated to all those who have fallen i...   \n",
       "1   [ The master plan for development of Hillcrest...   \n",
       "2   [ Trans-local Peacebuilding among Pastoralists...   \n",
       "3   [ scavengers ,  The problem of plastics is muc...   \n",
       "4   [ Albanianization . ,  physical , political , ...   \n",
       "5   [ We are not to fear the Europeans , ,  an @ @...   \n",
       "6   [ Big investors typically have the wherewithal...   \n",
       "7   [ settled ,  Investigations have been done as ...   \n",
       "8   [ While women remain a minority of combatants ...   \n",
       "9   [ That camp has become a nursery for terrorist...   \n",
       "10  [ the suzerainty of His Majesty over the India...   \n",
       "11  [ drop in at the house of an age mate and even...   \n",
       "\n",
       "                                     txtfiles_lk_2013  \\\n",
       "0   [ cakes ,  We are still not exhausted with our...   \n",
       "1   [ we recognize their right , the right of ever...   \n",
       "2   [ removed ,  Politically we are on the opposin...   \n",
       "3   [ he said .  ,  he said .  Demanding an inquir...   \n",
       "4   [ Buddhist Taliban ,  Justice , Peace and Huma...   \n",
       "5   [ My cat is Siamese . It is beautiful , ,  My ...   \n",
       "6   [ Islands have a critical role to play in rela...   \n",
       "7   [ I visualize the need to follow a policy , ba...   \n",
       "8   [ It 's a proud moment and a blessing to walk ...   \n",
       "9   [ Family Protects You ,  No First Deployment o...   \n",
       "10  [ those who try to seperate the country their ...   \n",
       "11  [ Super LARC ,  I will gather first-hand infor...   \n",
       "\n",
       "                                     txtfiles_my_2013  \\\n",
       "0   [ Global Food Losses and Food Waste ,  The amo...   \n",
       "1   [ said Prince Robert Louis Francois Marie of L...   \n",
       "2   [ I won because I attacked better than him . T...   \n",
       "3   [ It 's because Malaysian households now are v...   \n",
       "4   [ get out the vote ,  friends ,  The flights i...   \n",
       "5   [ There is a lack of skilled workers at all le...   \n",
       "6   [ It 's good news that the price of key HIV dr...   \n",
       "7   [ red-stops ,  exclusions ,  strengthens ,  ve...   \n",
       "8   [ Known for his progressive views on politics ...   \n",
       "9   [ stopped ,  stop ,  ayes ,  @ @ @ @ @ @ @ @ @...   \n",
       "10  [ Legally speaking , there is no specific prov...   \n",
       "11  [ the veteran lawmaker told The Malaysian Insi...   \n",
       "\n",
       "                                     txtfiles_ng_2013  \\\n",
       "0   [ NADECO Bishop ,  The law gives the DPR the r...   \n",
       "1   [ Justice Fatimo Akinbami who read the judgeme...   \n",
       "2   [ compared with third quarter results , the ec...   \n",
       "3   [ The Position of the Executive Council of Pla...   \n",
       "4   [ I can tell you with every sense of responsib...   \n",
       "5   [ It ( Nigeria ) is one of only four among the...   \n",
       "6   [ Following the judgment of the Court of Appea...   \n",
       "7   [ early Christianity was a movement built on s...   \n",
       "8   [ I was the first to publish report about the ...   \n",
       "9   [ The union between Bafarawa and Wamakko was i...   \n",
       "10  [ I will lead the blind by ways they have not ...   \n",
       "11  [ Development University ,  development univer...   \n",
       "\n",
       "                                     txtfiles_nz_2013  \\\n",
       "0   [ normal child ,  It really worries me @ @ @ @...   \n",
       "1   [ It 's tough . But it 's also spiritual and I...   \n",
       "2   [ To be honest , at first we kind of just soun...   \n",
       "3   [ opening the door to abuse of power and impun...   \n",
       "4   [ It looked like Molenberg , ,  It was folded ...   \n",
       "5   [ Jammies in June ,  Sadly , for some South Au...   \n",
       "6   [ She was surrounded by her husband , Alex , c...   \n",
       "7   [ Pest-rid ,  Wildlife rehabilitator ,  The id...   \n",
       "8   [ boring ,  earnest ,  worthy ,  OK , but how ...   \n",
       "9   [ poorly thought out ,  Housing New Zealand ad...   \n",
       "10  [ What makes our business unique is that we fu...   \n",
       "11  [ I hate talking about myself , I find it such...   \n",
       "\n",
       "                                     txtfiles_ph_2013  \\\n",
       "0   [ might have died from a heart attack because ...   \n",
       "1   [ Just a couple of weeks ago we said that LTE ...   \n",
       "2   [ world capital of political dynasties , ,  st...   \n",
       "3   [ hats ,  best and brightest ,  Bureaucracy , ...   \n",
       "4   [ In view of the failure of the prosecution to...   \n",
       "5   [ ghost ,  TROs like this one effectively prev...   \n",
       "6   [ We are invoking this right to send a strong ...   \n",
       "7   [ The imposition @ @ @ @ @ @ @ @ @ @ current 7...   \n",
       "8   [ By taking firm action to curb the indiscrimi...   \n",
       "9   [ rap ,  ripple ,  The Boss , ,  Gigi ,  the d...   \n",
       "10  [ Yolanda , ,  Yolanda ,  There are storms tha...   \n",
       "11  [ YOLANDA ,  There is no more future for me in...   \n",
       "\n",
       "                                     txtfiles_pk_2013  \\\n",
       "0   [ That is 25,000 new child brides every day fo...   \n",
       "1   [ When Dr Shaikh saw me , she told me she coul...   \n",
       "2   [ differencing disks , ,  flings . ,  controll...   \n",
       "3   [ surveillance ,  Even the judges and lawyers ...   \n",
       "4   [ Chaudhry Munir has virtually edged Jafar Iqb...   \n",
       "5   [ Toba Tek Singh ,  Toba Tek Singh ,  Toba Tek...   \n",
       "6   [ It 's impossible to achieve the MDG for educ...   \n",
       "7   [ We have just finished the harvest and one wo...   \n",
       "8   [ The PSM arrived at the present state due to ...   \n",
       "9   [ The 21st century is seeing an unprecedented ...   \n",
       "10  [ damaged but recognisable ,  A delegation was...   \n",
       "11  [ When we share videos and images on the inter...   \n",
       "\n",
       "                                     txtfiles_sg_2013  \\\n",
       "0   [ Black Beauty ,  The age of NWA 7034 is impor...   \n",
       "1   [ We are very proud that TAC has entrusted our...   \n",
       "2   [ It is important to have proactive and respon...   \n",
       "3   [ Real chance ,  It 's easy to build ( new haw...   \n",
       "4   [ food versus fuel ,  The production and use o...   \n",
       "5   [ It will also help to reduce impulse purchase...   \n",
       "6   [ representative sample ,  Near Threatened ,  ...   \n",
       "7   [ Now there are definitely more tourists . Peo...   \n",
       "8   [ groundbreaking ,  A number of our previous s...   \n",
       "9   [ mechanism ,   Why on earth , in this shippin...   \n",
       "10  [ Carbon remineralisation and sequestration in...   \n",
       "11  [ Job 's instincts were against white products...   \n",
       "\n",
       "                                     txtfiles_tz_2013  \\\n",
       "0   [ ... we have the skills to use computers , co...   \n",
       "1   [ So , this program will educate the public ab...   \n",
       "2   [ This is a United Nations Board of Auditors (...   \n",
       "3   [ SUR Peru was the second channel launched by ...   \n",
       "4   [ If this poaching trend is left unchecked , o...   \n",
       "5   [ Men make their own history , but they do not...   \n",
       "6   [ Education , Poverty and Disability in Develo...   \n",
       "7   [ School heads are asked to welcome these stud...   \n",
       "8   [ The idea of population momentum is vitally i...   \n",
       "9   [ When my family took me to school there were ...   \n",
       "10  [ regional economic communities ( RECs ) .  Th...   \n",
       "11  [ A Golden Opportunity ? How Tanzania is faili...   \n",
       "\n",
       "                                     txtfiles_us_2013  \\\n",
       "0   [ Today is Christmas Eve . Today is my birthda...   \n",
       "1   [ I did n't think it was really an accurate do...   \n",
       "2   [ New South . ,  Southerners may live in the h...   \n",
       "3   [ Brother , ,  Or before winter 's done , ,  e...   \n",
       "4   [ Big Data ,  Data Scientist . ,  ask ,  is , ...   \n",
       "5   [ reformers ,  fiscal crisis ,  portfolio ,  3...   \n",
       "6   [ A Harvest of Death , ,  The Hero of Gettysbu...   \n",
       "7   [ Millennial ,  living at home ,  living with ...   \n",
       "8   [ beat a fast exit out of New York City , ,  b...   \n",
       "9   [ diamond field ,  on hold . ,  take a break ,...   \n",
       "10  [ 16 going on 17 ,  recombinant ,  Recombinant...   \n",
       "11  [ Even though it 's paid well , you are sinkin...   \n",
       "\n",
       "                                     txtfiles_za_2013  \n",
       "0   [ Integrating these farms into value chains no...  \n",
       "1   [ for the purpose of this application , at thi...  \n",
       "2   [ Essentially , all these increases include an...  \n",
       "3   [ wastes ,  another PARC ,  cat ,  yes ,  Ewe ...  \n",
       "4   [ I am delighted that I now have my good name ...  \n",
       "5   [ she told The Mercury , a local newspaper .  ...  \n",
       "6   [ declaring his Kunene 's membership of the Ec...  \n",
       "7   [ Yes , I was taken aback by the negative reac...  \n",
       "8   [ wrong ,  These are not profit-making institu...  \n",
       "9   [ busty 20-something ,  auto-level ,  ,  ,  Ta...  \n",
       "10  [ The Chilean population has 11.6 years of sch...  \n",
       "11  [ It is not possible for the trust to sustain ...  "
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2014</th>\n",
       "      <th>txtfiles_bd_2014</th>\n",
       "      <th>txtfiles_ca_2014</th>\n",
       "      <th>txtfiles_gb_2014</th>\n",
       "      <th>txtfiles_gh_2014</th>\n",
       "      <th>txtfiles_hk_2014</th>\n",
       "      <th>txtfiles_ie_2014</th>\n",
       "      <th>txtfiles_in_2014</th>\n",
       "      <th>txtfiles_jm_2014</th>\n",
       "      <th>txtfiles_ke_2014</th>\n",
       "      <th>txtfiles_lk_2014</th>\n",
       "      <th>txtfiles_my_2014</th>\n",
       "      <th>txtfiles_ng_2014</th>\n",
       "      <th>txtfiles_nz_2014</th>\n",
       "      <th>txtfiles_ph_2014</th>\n",
       "      <th>txtfiles_pk_2014</th>\n",
       "      <th>txtfiles_sg_2014</th>\n",
       "      <th>txtfiles_tz_2014</th>\n",
       "      <th>txtfiles_us_2014</th>\n",
       "      <th>txtfiles_za_2014</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ the monkey mind ,  meditation ,  mental sile...</td>\n",
       "      <td>[ yes , our Supreme Court is independent and n...</td>\n",
       "      <td>[ If the world is serious about knocking down ...</td>\n",
       "      <td>[ Sloterdijk writes , ,   Sloterdijk sees Paxt...</td>\n",
       "      <td>[ The withdrawal of the allowances of the teac...</td>\n",
       "      <td>[ canned fresh air ,  Soon , I will go to Amer...</td>\n",
       "      <td>[ doing great ,  I came in for induction yeste...</td>\n",
       "      <td>[ The Swiss bosses of Volkart Bros. ( one of t...</td>\n",
       "      <td>[ a strategic framework on adolescent pregnanc...</td>\n",
       "      <td>[ During the first quarter of financial year 2...</td>\n",
       "      <td>[ What has happened to the service quality of ...</td>\n",
       "      <td>[ After our initial investigation of the scene...</td>\n",
       "      <td>[  While addressing the trainees , Kast remind...</td>\n",
       "      <td>[ It 's @ @ @ @ @ @ @ @ @ @ too precious about...</td>\n",
       "      <td>[ Comeback Fighter of the Year ,  On The Ropes...</td>\n",
       "      <td>[ One week has passed . We do n't know where t...</td>\n",
       "      <td>[ actual plausible event ,  webOS fan fantasy ...</td>\n",
       "      <td>[ .............. we can not continue with the ...</td>\n",
       "      <td>[ branch of the future ,  branch of the future...</td>\n",
       "      <td>[ We have 33 babies that have been born in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ As a Government which promised at the electi...</td>\n",
       "      <td>[ If you had to rely solely on your investment...</td>\n",
       "      <td>[ This is weird . Society sees a need for this...</td>\n",
       "      <td>[ When I returned to Gothenburg the following ...</td>\n",
       "      <td>[ joke ' ' by Prof Newman Kojo Kusi , a renown...</td>\n",
       "      <td>[ state secrets ,  The scope of secret items s...</td>\n",
       "      <td>[ Growing up here used to mean something , ,  ...</td>\n",
       "      <td>[ My marriage happened very quietly and quickl...</td>\n",
       "      <td>[ I knew it was never going to be easy to beat...</td>\n",
       "      <td>[ Child marriage is linked to poor health , cu...</td>\n",
       "      <td>[ PEACEFUL WAR : How the Chinese Dream and the...</td>\n",
       "      <td>[ she added .  The upward trend in the latest ...</td>\n",
       "      <td>[ The announcement of the refreshed identity i...</td>\n",
       "      <td>[ The biggest problem is so many people come i...</td>\n",
       "      <td>[ interpretative issues ,  Claims Administrato...</td>\n",
       "      <td>[ household ,  majority ,  majority ,  Muslims...</td>\n",
       "      <td>[ Yuan Xiao Jie ,  double happiness ,  For the...</td>\n",
       "      <td>[ However , number of internet users has been ...</td>\n",
       "      <td>[ I 'll always be indebted to what he did for ...</td>\n",
       "      <td>[ here we go again ,  Recent pointed intervent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ toxic sludge ,  toxic ,  Reef will be dredge...</td>\n",
       "      <td>[ We made the flag for our liberation , for a ...</td>\n",
       "      <td>[ Toronto is the engine of economic growth in ...</td>\n",
       "      <td>[ special agent ,  making straight all the tan...</td>\n",
       "      <td>[ One District Chief Executive makes a comment...</td>\n",
       "      <td>[ discipline investigators from Beijing , ' ' ...</td>\n",
       "      <td>[ bad ' ' bank at RBS .  At the time of the ba...</td>\n",
       "      <td>[ States ' power of remission is against Const...</td>\n",
       "      <td>[ slow but steady progress ,  The momentum of ...</td>\n",
       "      <td>[ we are not looking for those with millions o...</td>\n",
       "      <td>[ Chopin was such an introvert in real life . ...</td>\n",
       "      <td>[ The water is suitable for human consumption ...</td>\n",
       "      <td>[ We 've been hearing this planned increase in...</td>\n",
       "      <td>[ genuine , foul butt-crack smell with hints o...</td>\n",
       "      <td>[ contractualization , ,  which is ( the ) lab...</td>\n",
       "      <td>[ Well , we are a ' for-profit ' organisation ...</td>\n",
       "      <td>[ He was charming , handsome and had a powerfu...</td>\n",
       "      <td>[ Tanzania has several legislations that pose ...</td>\n",
       "      <td>[ incredible act of aggression ,  You just do ...</td>\n",
       "      <td>[ change the economic and industrial landscape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ extreme ,  an intense fear of failure and th...</td>\n",
       "      <td>[ We are working to make the examination compe...</td>\n",
       "      <td>[ cross-border conflict ,  This is state spons...</td>\n",
       "      <td>[ destroy ,  were about darkness and the world...</td>\n",
       "      <td>[ the chicken and the egg which came first ,  ...</td>\n",
       "      <td>[ We believe the offer price represents an att...</td>\n",
       "      <td>[ one of the select breed of highly successful...</td>\n",
       "      <td>[ the matter is serious ,  peruse the files co...</td>\n",
       "      <td>[ Banana Man ,  tek it to dem ,  a @ @ @ @ @ @...</td>\n",
       "      <td>[ We do not think that this is the time to dis...</td>\n",
       "      <td>[ Inazuma - 105 ,  Umigiri - 158 ,  Inazuma - ...</td>\n",
       "      <td>[ Rupert Murdoch ,  tree-talker ,  serenade an...</td>\n",
       "      <td>[ self-help ,  Western education is forbidden ...</td>\n",
       "      <td>[ He made it clear I was going to die . I was ...</td>\n",
       "      <td>[ fried chickens ,  crispy patas ,  inequitabl...</td>\n",
       "      <td>[ I had no idea what to expect . I did n't eve...</td>\n",
       "      <td>[ Goodnight , Malaysian three seven zero ,  Al...</td>\n",
       "      <td>[ We have only gone through part one of Chapte...</td>\n",
       "      <td>[ Chapter 26 ,  credit hour , ,  Father , how ...</td>\n",
       "      <td>[ the enemy ,  us and them ,  service delivery...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ align and fully embed health and medical res...</td>\n",
       "      <td>[ At least 12 people can sit in a microbus whi...</td>\n",
       "      <td>[ As a father , I 'm very proud to have suppor...</td>\n",
       "      <td>[ nation of drunks ,  significantly ,  the ava...</td>\n",
       "      <td>[ changes at the National Security Council Sec...</td>\n",
       "      <td>[ TSX Venture Exchange has been successful in ...</td>\n",
       "      <td>[ Ireland is the second largest exporter of IC...</td>\n",
       "      <td>[ This is my greatest tribute to love . Sussan...</td>\n",
       "      <td>[ By definition this is an epidemic since it r...</td>\n",
       "      <td>[ A county with a high population and low pove...</td>\n",
       "      <td>[ They were minor intensity blasts . Experts f...</td>\n",
       "      <td>[ it was never my dream to be in the governmen...</td>\n",
       "      <td>[ Key Issues before the Northern Delegates to ...</td>\n",
       "      <td>[ investing a lot of money in New Zealand ,  s...</td>\n",
       "      <td>[ If you look at our neighboring countries , a...</td>\n",
       "      <td>[ The whole problem with the world is that foo...</td>\n",
       "      <td>[ Egyptian judges are independent and there is...</td>\n",
       "      <td>[ We are working to ensure that the 2014/15 bu...</td>\n",
       "      <td>[ Yeah , sorry about that , ,  We had a financ...</td>\n",
       "      <td>[ One member was seriously wounded and is curr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ new approach for the provision of ICT infras...</td>\n",
       "      <td>[ Gold smuggling has gone up too much recently...</td>\n",
       "      <td>[ discomfort , anxiety , nervousness , or angu...</td>\n",
       "      <td>[ clean-up ,  more transparency in Fifa ,  tsu...</td>\n",
       "      <td>[ I think there 's a real issue . I do n't kno...</td>\n",
       "      <td>[ Through the Safe City Test Bed , we envision...</td>\n",
       "      <td>[ representational payment ,  noted ,  Dara Ca...</td>\n",
       "      <td>[ They 're decent people , regular travellers ...</td>\n",
       "      <td>[ shall be liable to be imprisoned for life wi...</td>\n",
       "      <td>[ the elephant in the room ,  The commission d...</td>\n",
       "      <td>[ IN ,  OUT ,  Doval is not likely to confine ...</td>\n",
       "      <td>[ To the Batcave ! ,  tropical paradise ,  ove...</td>\n",
       "      <td>[ It was a difficult decision , all the contes...</td>\n",
       "      <td>[ We are now in a perfect position to develop ...</td>\n",
       "      <td>[ rap ,  ripple ,  doctors for the people . , ...</td>\n",
       "      <td>[ Ideally , this ratio should be less than 30%...</td>\n",
       "      <td>[ There was nobody to push me , and the route ...</td>\n",
       "      <td>[ , whereby farmers are linked with a large fa...</td>\n",
       "      <td>[ It 's Official I 'm Back ! ! Cleared For All...</td>\n",
       "      <td>[ All communities will be treated with the dig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ The level of arrogance that this man believe...</td>\n",
       "      <td>[ Through this verdict , Bangladesh could fina...</td>\n",
       "      <td>[ who won free agency , ,  Phoenix ,  Phoenix ...</td>\n",
       "      <td>[ multiple times ,  he is one stubborn man and...</td>\n",
       "      <td>[ rice water ,  Reshaping the future of Ghana ...</td>\n",
       "      <td>[ semi-autonomous ,  Hong Kong 's core values ...</td>\n",
       "      <td>[ vast majority ,  Over the last number of mon...</td>\n",
       "      <td>[ The first thing I had taken objection to was...</td>\n",
       "      <td>[ This decision operationalises one of the rec...</td>\n",
       "      <td>[ The different colours help us @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ I 'm not going to comment on the operational...</td>\n",
       "      <td>[ he said , adding that he was advised to rece...</td>\n",
       "      <td>[ I have no malice , nor do I hate anyone . I ...</td>\n",
       "      <td>[ After work we would go to the manager 's hou...</td>\n",
       "      <td>[ unobligated allotments of agencies with low ...</td>\n",
       "      <td>[ to provide for protection against waging of ...</td>\n",
       "      <td>[ India is emerging as the hotbed for technolo...</td>\n",
       "      <td>[ The livestock sector is one of those involve...</td>\n",
       "      <td>[ Shades Of Cool ,  Lolita ,  a statement she ...</td>\n",
       "      <td>[ mass ALL the things ,  The first @ @ @ @ @ @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ Guardians of the Galaxy , ,  Six months no b...</td>\n",
       "      <td>[ But I 'll act according to law , ,  We 'll b...</td>\n",
       "      <td>[ Complacency and ego are killers of sharehold...</td>\n",
       "      <td>[ only a matter of time ,  10 billion Hiroshim...</td>\n",
       "      <td>[ Arable and industrial crop production has in...</td>\n",
       "      <td>[ hundreds of thousands of hours ,  We 've bee...</td>\n",
       "      <td>[ multilayered sense of belonging ,  shame , s...</td>\n",
       "      <td>[ Fabricated news articles . Demeaning my love...</td>\n",
       "      <td>[ He 's a top-class musician . Bobby Ellis is ...</td>\n",
       "      <td>[ Cost of maize production is high and it is s...</td>\n",
       "      <td>[ Viru Sisu Pradeepa - VI ,  Viru Sisu Pradeep...</td>\n",
       "      <td>[ harmful to health ,  junk food ,  That 's wh...</td>\n",
       "      <td>[ Ijoba ,  we have always maintained that the ...</td>\n",
       "      <td>[ Catherine 's ready ,  more of good luck than...</td>\n",
       "      <td>[ rap ,  ripple ,  Ring of Fire . ,  tsunamige...</td>\n",
       "      <td>[ India according to Lollywood ,   Recommend0 ...</td>\n",
       "      <td>[ We are very proud of Kexin and Annie in winn...</td>\n",
       "      <td>[ That 's why we decided to harmonise social s...</td>\n",
       "      <td>[ All Zionists are legitimate targets everywhe...</td>\n",
       "      <td>[ I whipped out my phone to take pictures ... ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ I could have ... but that 's not who I am , ...</td>\n",
       "      <td>[ . The scene cut to a tiny ramshackle wooden ...</td>\n",
       "      <td>[ After transitioning I ca n't get anyone to g...</td>\n",
       "      <td>[ selfie ,  strange ,  She was essentially say...</td>\n",
       "      <td>[ Moving forward , we want to have an arrangem...</td>\n",
       "      <td>[ The US challenge is really helping governmen...</td>\n",
       "      <td>[ Ireland 's greatest footballer ,  disappoint...</td>\n",
       "      <td>[ religiously ,  The interest not only reflect...</td>\n",
       "      <td>[ full impact ,  There are a total of over 9,0...</td>\n",
       "      <td>[ world 's berry capital ,  genocide ,  suppor...</td>\n",
       "      <td>[ also sent a small number of British officers...</td>\n",
       "      <td>[ . Ameera Al Hakawati , writing for this webs...</td>\n",
       "      <td>[ The card is not only a means of certifying y...</td>\n",
       "      <td>[ Nobody should go to work and worry about com...</td>\n",
       "      <td>[ Sabado ,  1995 ,  ISIS ,  Looney Tunes ,  Th...</td>\n",
       "      <td>[ very high ,  exceptionally high ,  the next ...</td>\n",
       "      <td>[ made in Singapore ,  The script will explain...</td>\n",
       "      <td>[ There is poor representation of people with ...</td>\n",
       "      <td>[ in the form of debt claims that did not have...</td>\n",
       "      <td>[ No one may be evicted from their homes , or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ Queensland leads Australia in attracting int...</td>\n",
       "      <td>[ We heard that Tk1,20,000 will be counted as ...</td>\n",
       "      <td>[ food insecurity ,  We all know people who ar...</td>\n",
       "      <td>[ brought rights home ,  HRA minus ,  I do n't...</td>\n",
       "      <td>[ For those who are outside the Lord , they wo...</td>\n",
       "      <td>[ Here , for you ! ,  A personal practice is n...</td>\n",
       "      <td>[ I worked in a sign manufacturing company unt...</td>\n",
       "      <td>[ raided ,  raided ,  raid ,  without fail ,  ...</td>\n",
       "      <td>[ I am truly honoured by this appointment and ...</td>\n",
       "      <td>[ Now that the loans is released to the nation...</td>\n",
       "      <td>[ Noble Wealth ,  -ucking bitch ,  Turned Back...</td>\n",
       "      <td>[ friends of God ,  The dead are living , ,  T...</td>\n",
       "      <td>[ Uriah ,  withstand the storm ,  Fly with the...</td>\n",
       "      <td>[ dress classy dance cheesy ,  and the video f...</td>\n",
       "      <td>[ rap ,  ripple ,  rap ,  ripple ,  The Imagin...</td>\n",
       "      <td>[ Government must have arrested Jamaat Ali Sha...</td>\n",
       "      <td>[ With the recently announced acquisition of V...</td>\n",
       "      <td>[ As among top leaders appointed by the Presid...</td>\n",
       "      <td>[ The Beef Goes On . ,  Here kitty-kitty . ,  ...</td>\n",
       "      <td>[ Nothing happens without the spirit of God , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ If people were just more aware of what calor...</td>\n",
       "      <td>[ real trip down memory lane ,  It was very cl...</td>\n",
       "      <td>[ why Nikola Tesla was the greatest geek who e...</td>\n",
       "      <td>[ orgone accumulator ,  to have a life of its ...</td>\n",
       "      <td>[ AndWe had sent Lot when he said to his peopl...</td>\n",
       "      <td>[ The ' moderate ' pan democratic parties are ...</td>\n",
       "      <td>[ Come on tae f**k , Rangers , ,  Put it in fr...</td>\n",
       "      <td>[ Open Doors ,  Katrina wants to share the ren...</td>\n",
       "      <td>[ The assessment shows that Kingston and St An...</td>\n",
       "      <td>[ different kind of army ,  The U.S. and other...</td>\n",
       "      <td>[ illegal slaughter of tens of thousands of an...</td>\n",
       "      <td>[ We are allowing the declaration sought by th...</td>\n",
       "      <td>[ Audit Bill 2014 , ,  The Finance Controls an...</td>\n",
       "      <td>[ acidic dirt ,  fine flavour ,  You get confl...</td>\n",
       "      <td>[ five years of loss and of a search for justi...</td>\n",
       "      <td>[ the transfer of knowledge @ @ @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ Most Disruptive ,  Being my first time in Ha...</td>\n",
       "      <td>[ playing around ,  get the feeling ,  The pla...</td>\n",
       "      <td>[ writer 's block ,  secretive ,  trained ,  w...</td>\n",
       "      <td>[ renowned for its elite clientele and exquisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[ We probably should have changed things , we ...</td>\n",
       "      <td>[ This competition has increased our self conf...</td>\n",
       "      <td>[ Do n't assume that because your beneficiary ...</td>\n",
       "      <td>[ She perfectly repressed her anger . ,  I act...</td>\n",
       "      <td>[ The score and rank of Ghana shows also that ...</td>\n",
       "      <td>[ obsession ,  according to photos posted to s...</td>\n",
       "      <td>[ Silent Night ,  home by Christmas ,  Day and...</td>\n",
       "      <td>[ She suffered severe ( grade IV ) injuries in...</td>\n",
       "      <td>[ I was shocked because it was never mentioned...</td>\n",
       "      <td>[ The countries that are resisting and subvert...</td>\n",
       "      <td>[ Round Up ,  have effectively shut Sri Lanka ...</td>\n",
       "      <td>[ Given the impact of such vitriolic rhetoric ...</td>\n",
       "      <td>[ My Watch ,  Although he might wish to do wel...</td>\n",
       "      <td>[ It was definitely an extreme , no question ,...</td>\n",
       "      <td>[ hydro-meteorological ( hydro-met ) hazards ,...</td>\n",
       "      <td>[ ridiculous ,  Why does no credible source fr...</td>\n",
       "      <td>[ as baggy and undisciplined as The Interview ...</td>\n",
       "      <td>[ Loss of agriculture produce is a challenge n...</td>\n",
       "      <td>[ It was February 1953 . There was no bridge ,...</td>\n",
       "      <td>[ commanding heights ,  above the parapet ,  a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     txtfiles_au_2014  \\\n",
       "0   [ the monkey mind ,  meditation ,  mental sile...   \n",
       "1   [ As a Government which promised at the electi...   \n",
       "2   [ toxic sludge ,  toxic ,  Reef will be dredge...   \n",
       "3   [ extreme ,  an intense fear of failure and th...   \n",
       "4   [ align and fully embed health and medical res...   \n",
       "5   [ new approach for the provision of ICT infras...   \n",
       "6   [ The level of arrogance that this man believe...   \n",
       "7   [ Guardians of the Galaxy , ,  Six months no b...   \n",
       "8   [ I could have ... but that 's not who I am , ...   \n",
       "9   [ Queensland leads Australia in attracting int...   \n",
       "10  [ If people were just more aware of what calor...   \n",
       "11  [ We probably should have changed things , we ...   \n",
       "\n",
       "                                     txtfiles_bd_2014  \\\n",
       "0   [ yes , our Supreme Court is independent and n...   \n",
       "1   [ If you had to rely solely on your investment...   \n",
       "2   [ We made the flag for our liberation , for a ...   \n",
       "3   [ We are working to make the examination compe...   \n",
       "4   [ At least 12 people can sit in a microbus whi...   \n",
       "5   [ Gold smuggling has gone up too much recently...   \n",
       "6   [ Through this verdict , Bangladesh could fina...   \n",
       "7   [ But I 'll act according to law , ,  We 'll b...   \n",
       "8   [ . The scene cut to a tiny ramshackle wooden ...   \n",
       "9   [ We heard that Tk1,20,000 will be counted as ...   \n",
       "10  [ real trip down memory lane ,  It was very cl...   \n",
       "11  [ This competition has increased our self conf...   \n",
       "\n",
       "                                     txtfiles_ca_2014  \\\n",
       "0   [ If the world is serious about knocking down ...   \n",
       "1   [ This is weird . Society sees a need for this...   \n",
       "2   [ Toronto is the engine of economic growth in ...   \n",
       "3   [ cross-border conflict ,  This is state spons...   \n",
       "4   [ As a father , I 'm very proud to have suppor...   \n",
       "5   [ discomfort , anxiety , nervousness , or angu...   \n",
       "6   [ who won free agency , ,  Phoenix ,  Phoenix ...   \n",
       "7   [ Complacency and ego are killers of sharehold...   \n",
       "8   [ After transitioning I ca n't get anyone to g...   \n",
       "9   [ food insecurity ,  We all know people who ar...   \n",
       "10  [ why Nikola Tesla was the greatest geek who e...   \n",
       "11  [ Do n't assume that because your beneficiary ...   \n",
       "\n",
       "                                     txtfiles_gb_2014  \\\n",
       "0   [ Sloterdijk writes , ,   Sloterdijk sees Paxt...   \n",
       "1   [ When I returned to Gothenburg the following ...   \n",
       "2   [ special agent ,  making straight all the tan...   \n",
       "3   [ destroy ,  were about darkness and the world...   \n",
       "4   [ nation of drunks ,  significantly ,  the ava...   \n",
       "5   [ clean-up ,  more transparency in Fifa ,  tsu...   \n",
       "6   [ multiple times ,  he is one stubborn man and...   \n",
       "7   [ only a matter of time ,  10 billion Hiroshim...   \n",
       "8   [ selfie ,  strange ,  She was essentially say...   \n",
       "9   [ brought rights home ,  HRA minus ,  I do n't...   \n",
       "10  [ orgone accumulator ,  to have a life of its ...   \n",
       "11  [ She perfectly repressed her anger . ,  I act...   \n",
       "\n",
       "                                     txtfiles_gh_2014  \\\n",
       "0   [ The withdrawal of the allowances of the teac...   \n",
       "1   [ joke ' ' by Prof Newman Kojo Kusi , a renown...   \n",
       "2   [ One District Chief Executive makes a comment...   \n",
       "3   [ the chicken and the egg which came first ,  ...   \n",
       "4   [ changes at the National Security Council Sec...   \n",
       "5   [ I think there 's a real issue . I do n't kno...   \n",
       "6   [ rice water ,  Reshaping the future of Ghana ...   \n",
       "7   [ Arable and industrial crop production has in...   \n",
       "8   [ Moving forward , we want to have an arrangem...   \n",
       "9   [ For those who are outside the Lord , they wo...   \n",
       "10  [ AndWe had sent Lot when he said to his peopl...   \n",
       "11  [ The score and rank of Ghana shows also that ...   \n",
       "\n",
       "                                     txtfiles_hk_2014  \\\n",
       "0   [ canned fresh air ,  Soon , I will go to Amer...   \n",
       "1   [ state secrets ,  The scope of secret items s...   \n",
       "2   [ discipline investigators from Beijing , ' ' ...   \n",
       "3   [ We believe the offer price represents an att...   \n",
       "4   [ TSX Venture Exchange has been successful in ...   \n",
       "5   [ Through the Safe City Test Bed , we envision...   \n",
       "6   [ semi-autonomous ,  Hong Kong 's core values ...   \n",
       "7   [ hundreds of thousands of hours ,  We 've bee...   \n",
       "8   [ The US challenge is really helping governmen...   \n",
       "9   [ Here , for you ! ,  A personal practice is n...   \n",
       "10  [ The ' moderate ' pan democratic parties are ...   \n",
       "11  [ obsession ,  according to photos posted to s...   \n",
       "\n",
       "                                     txtfiles_ie_2014  \\\n",
       "0   [ doing great ,  I came in for induction yeste...   \n",
       "1   [ Growing up here used to mean something , ,  ...   \n",
       "2   [ bad ' ' bank at RBS .  At the time of the ba...   \n",
       "3   [ one of the select breed of highly successful...   \n",
       "4   [ Ireland is the second largest exporter of IC...   \n",
       "5   [ representational payment ,  noted ,  Dara Ca...   \n",
       "6   [ vast majority ,  Over the last number of mon...   \n",
       "7   [ multilayered sense of belonging ,  shame , s...   \n",
       "8   [ Ireland 's greatest footballer ,  disappoint...   \n",
       "9   [ I worked in a sign manufacturing company unt...   \n",
       "10  [ Come on tae f**k , Rangers , ,  Put it in fr...   \n",
       "11  [ Silent Night ,  home by Christmas ,  Day and...   \n",
       "\n",
       "                                     txtfiles_in_2014  \\\n",
       "0   [ The Swiss bosses of Volkart Bros. ( one of t...   \n",
       "1   [ My marriage happened very quietly and quickl...   \n",
       "2   [ States ' power of remission is against Const...   \n",
       "3   [ the matter is serious ,  peruse the files co...   \n",
       "4   [ This is my greatest tribute to love . Sussan...   \n",
       "5   [ They 're decent people , regular travellers ...   \n",
       "6   [ The first thing I had taken objection to was...   \n",
       "7   [ Fabricated news articles . Demeaning my love...   \n",
       "8   [ religiously ,  The interest not only reflect...   \n",
       "9   [ raided ,  raided ,  raid ,  without fail ,  ...   \n",
       "10  [ Open Doors ,  Katrina wants to share the ren...   \n",
       "11  [ She suffered severe ( grade IV ) injuries in...   \n",
       "\n",
       "                                     txtfiles_jm_2014  \\\n",
       "0   [ a strategic framework on adolescent pregnanc...   \n",
       "1   [ I knew it was never going to be easy to beat...   \n",
       "2   [ slow but steady progress ,  The momentum of ...   \n",
       "3   [ Banana Man ,  tek it to dem ,  a @ @ @ @ @ @...   \n",
       "4   [ By definition this is an epidemic since it r...   \n",
       "5   [ shall be liable to be imprisoned for life wi...   \n",
       "6   [ This decision operationalises one of the rec...   \n",
       "7   [ He 's a top-class musician . Bobby Ellis is ...   \n",
       "8   [ full impact ,  There are a total of over 9,0...   \n",
       "9   [ I am truly honoured by this appointment and ...   \n",
       "10  [ The assessment shows that Kingston and St An...   \n",
       "11  [ I was shocked because it was never mentioned...   \n",
       "\n",
       "                                     txtfiles_ke_2014  \\\n",
       "0   [ During the first quarter of financial year 2...   \n",
       "1   [ Child marriage is linked to poor health , cu...   \n",
       "2   [ we are not looking for those with millions o...   \n",
       "3   [ We do not think that this is the time to dis...   \n",
       "4   [ A county with a high population and low pove...   \n",
       "5   [ the elephant in the room ,  The commission d...   \n",
       "6   [ The different colours help us @ @ @ @ @ @ @ ...   \n",
       "7   [ Cost of maize production is high and it is s...   \n",
       "8   [ world 's berry capital ,  genocide ,  suppor...   \n",
       "9   [ Now that the loans is released to the nation...   \n",
       "10  [ different kind of army ,  The U.S. and other...   \n",
       "11  [ The countries that are resisting and subvert...   \n",
       "\n",
       "                                     txtfiles_lk_2014  \\\n",
       "0   [ What has happened to the service quality of ...   \n",
       "1   [ PEACEFUL WAR : How the Chinese Dream and the...   \n",
       "2   [ Chopin was such an introvert in real life . ...   \n",
       "3   [ Inazuma - 105 ,  Umigiri - 158 ,  Inazuma - ...   \n",
       "4   [ They were minor intensity blasts . Experts f...   \n",
       "5   [ IN ,  OUT ,  Doval is not likely to confine ...   \n",
       "6   [ I 'm not going to comment on the operational...   \n",
       "7   [ Viru Sisu Pradeepa - VI ,  Viru Sisu Pradeep...   \n",
       "8   [ also sent a small number of British officers...   \n",
       "9   [ Noble Wealth ,  -ucking bitch ,  Turned Back...   \n",
       "10  [ illegal slaughter of tens of thousands of an...   \n",
       "11  [ Round Up ,  have effectively shut Sri Lanka ...   \n",
       "\n",
       "                                     txtfiles_my_2014  \\\n",
       "0   [ After our initial investigation of the scene...   \n",
       "1   [ she added .  The upward trend in the latest ...   \n",
       "2   [ The water is suitable for human consumption ...   \n",
       "3   [ Rupert Murdoch ,  tree-talker ,  serenade an...   \n",
       "4   [ it was never my dream to be in the governmen...   \n",
       "5   [ To the Batcave ! ,  tropical paradise ,  ove...   \n",
       "6   [ he said , adding that he was advised to rece...   \n",
       "7   [ harmful to health ,  junk food ,  That 's wh...   \n",
       "8   [ . Ameera Al Hakawati , writing for this webs...   \n",
       "9   [ friends of God ,  The dead are living , ,  T...   \n",
       "10  [ We are allowing the declaration sought by th...   \n",
       "11  [ Given the impact of such vitriolic rhetoric ...   \n",
       "\n",
       "                                     txtfiles_ng_2014  \\\n",
       "0   [  While addressing the trainees , Kast remind...   \n",
       "1   [ The announcement of the refreshed identity i...   \n",
       "2   [ We 've been hearing this planned increase in...   \n",
       "3   [ self-help ,  Western education is forbidden ...   \n",
       "4   [ Key Issues before the Northern Delegates to ...   \n",
       "5   [ It was a difficult decision , all the contes...   \n",
       "6   [ I have no malice , nor do I hate anyone . I ...   \n",
       "7   [ Ijoba ,  we have always maintained that the ...   \n",
       "8   [ The card is not only a means of certifying y...   \n",
       "9   [ Uriah ,  withstand the storm ,  Fly with the...   \n",
       "10  [ Audit Bill 2014 , ,  The Finance Controls an...   \n",
       "11  [ My Watch ,  Although he might wish to do wel...   \n",
       "\n",
       "                                     txtfiles_nz_2014  \\\n",
       "0   [ It 's @ @ @ @ @ @ @ @ @ @ too precious about...   \n",
       "1   [ The biggest problem is so many people come i...   \n",
       "2   [ genuine , foul butt-crack smell with hints o...   \n",
       "3   [ He made it clear I was going to die . I was ...   \n",
       "4   [ investing a lot of money in New Zealand ,  s...   \n",
       "5   [ We are now in a perfect position to develop ...   \n",
       "6   [ After work we would go to the manager 's hou...   \n",
       "7   [ Catherine 's ready ,  more of good luck than...   \n",
       "8   [ Nobody should go to work and worry about com...   \n",
       "9   [ dress classy dance cheesy ,  and the video f...   \n",
       "10  [ acidic dirt ,  fine flavour ,  You get confl...   \n",
       "11  [ It was definitely an extreme , no question ,...   \n",
       "\n",
       "                                     txtfiles_ph_2014  \\\n",
       "0   [ Comeback Fighter of the Year ,  On The Ropes...   \n",
       "1   [ interpretative issues ,  Claims Administrato...   \n",
       "2   [ contractualization , ,  which is ( the ) lab...   \n",
       "3   [ fried chickens ,  crispy patas ,  inequitabl...   \n",
       "4   [ If you look at our neighboring countries , a...   \n",
       "5   [ rap ,  ripple ,  doctors for the people . , ...   \n",
       "6   [ unobligated allotments of agencies with low ...   \n",
       "7   [ rap ,  ripple ,  Ring of Fire . ,  tsunamige...   \n",
       "8   [ Sabado ,  1995 ,  ISIS ,  Looney Tunes ,  Th...   \n",
       "9   [ rap ,  ripple ,  rap ,  ripple ,  The Imagin...   \n",
       "10  [ five years of loss and of a search for justi...   \n",
       "11  [ hydro-meteorological ( hydro-met ) hazards ,...   \n",
       "\n",
       "                                     txtfiles_pk_2014  \\\n",
       "0   [ One week has passed . We do n't know where t...   \n",
       "1   [ household ,  majority ,  majority ,  Muslims...   \n",
       "2   [ Well , we are a ' for-profit ' organisation ...   \n",
       "3   [ I had no idea what to expect . I did n't eve...   \n",
       "4   [ The whole problem with the world is that foo...   \n",
       "5   [ Ideally , this ratio should be less than 30%...   \n",
       "6   [ to provide for protection against waging of ...   \n",
       "7   [ India according to Lollywood ,   Recommend0 ...   \n",
       "8   [ very high ,  exceptionally high ,  the next ...   \n",
       "9   [ Government must have arrested Jamaat Ali Sha...   \n",
       "10  [ the transfer of knowledge @ @ @ @ @ @ @ @ @ ...   \n",
       "11  [ ridiculous ,  Why does no credible source fr...   \n",
       "\n",
       "                                     txtfiles_sg_2014  \\\n",
       "0   [ actual plausible event ,  webOS fan fantasy ...   \n",
       "1   [ Yuan Xiao Jie ,  double happiness ,  For the...   \n",
       "2   [ He was charming , handsome and had a powerfu...   \n",
       "3   [ Goodnight , Malaysian three seven zero ,  Al...   \n",
       "4   [ Egyptian judges are independent and there is...   \n",
       "5   [ There was nobody to push me , and the route ...   \n",
       "6   [ India is emerging as the hotbed for technolo...   \n",
       "7   [ We are very proud of Kexin and Annie in winn...   \n",
       "8   [ made in Singapore ,  The script will explain...   \n",
       "9   [ With the recently announced acquisition of V...   \n",
       "10  [ Most Disruptive ,  Being my first time in Ha...   \n",
       "11  [ as baggy and undisciplined as The Interview ...   \n",
       "\n",
       "                                     txtfiles_tz_2014  \\\n",
       "0   [ .............. we can not continue with the ...   \n",
       "1   [ However , number of internet users has been ...   \n",
       "2   [ Tanzania has several legislations that pose ...   \n",
       "3   [ We have only gone through part one of Chapte...   \n",
       "4   [ We are working to ensure that the 2014/15 bu...   \n",
       "5   [ , whereby farmers are linked with a large fa...   \n",
       "6   [ The livestock sector is one of those involve...   \n",
       "7   [ That 's why we decided to harmonise social s...   \n",
       "8   [ There is poor representation of people with ...   \n",
       "9   [ As among top leaders appointed by the Presid...   \n",
       "10  [ playing around ,  get the feeling ,  The pla...   \n",
       "11  [ Loss of agriculture produce is a challenge n...   \n",
       "\n",
       "                                     txtfiles_us_2014  \\\n",
       "0   [ branch of the future ,  branch of the future...   \n",
       "1   [ I 'll always be indebted to what he did for ...   \n",
       "2   [ incredible act of aggression ,  You just do ...   \n",
       "3   [ Chapter 26 ,  credit hour , ,  Father , how ...   \n",
       "4   [ Yeah , sorry about that , ,  We had a financ...   \n",
       "5   [ It 's Official I 'm Back ! ! Cleared For All...   \n",
       "6   [ Shades Of Cool ,  Lolita ,  a statement she ...   \n",
       "7   [ All Zionists are legitimate targets everywhe...   \n",
       "8   [ in the form of debt claims that did not have...   \n",
       "9   [ The Beef Goes On . ,  Here kitty-kitty . ,  ...   \n",
       "10  [ writer 's block ,  secretive ,  trained ,  w...   \n",
       "11  [ It was February 1953 . There was no bridge ,...   \n",
       "\n",
       "                                     txtfiles_za_2014  \n",
       "0   [ We have 33 babies that have been born in the...  \n",
       "1   [ here we go again ,  Recent pointed intervent...  \n",
       "2   [ change the economic and industrial landscape...  \n",
       "3   [ the enemy ,  us and them ,  service delivery...  \n",
       "4   [ One member was seriously wounded and is curr...  \n",
       "5   [ All communities will be treated with the dig...  \n",
       "6   [ mass ALL the things ,  The first @ @ @ @ @ @...  \n",
       "7   [ I whipped out my phone to take pictures ... ...  \n",
       "8   [ No one may be evicted from their homes , or ...  \n",
       "9   [ Nothing happens without the spirit of God , ...  \n",
       "10  [ renowned for its elite clientele and exquisi...  \n",
       "11  [ commanding heights ,  above the parapet ,  a...  "
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2015</th>\n",
       "      <th>txtfiles_bd_2015</th>\n",
       "      <th>txtfiles_ca_2015</th>\n",
       "      <th>txtfiles_gb_2015</th>\n",
       "      <th>txtfiles_gh_2015</th>\n",
       "      <th>txtfiles_hk_2015</th>\n",
       "      <th>txtfiles_ie_2015</th>\n",
       "      <th>txtfiles_in_2015</th>\n",
       "      <th>txtfiles_jm_2015</th>\n",
       "      <th>txtfiles_ke_2015</th>\n",
       "      <th>txtfiles_lk_2015</th>\n",
       "      <th>txtfiles_my_2015</th>\n",
       "      <th>txtfiles_ng_2015</th>\n",
       "      <th>txtfiles_nz_2015</th>\n",
       "      <th>txtfiles_ph_2015</th>\n",
       "      <th>txtfiles_pk_2015</th>\n",
       "      <th>txtfiles_sg_2015</th>\n",
       "      <th>txtfiles_tz_2015</th>\n",
       "      <th>txtfiles_us_2015</th>\n",
       "      <th>txtfiles_za_2015</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ We see our trees as just as important as roa...</td>\n",
       "      <td>[ This is not my own success . I am dedicating...</td>\n",
       "      <td>[ ca n't find the time ,  This is the exact wa...</td>\n",
       "      <td>[ Fed Put ,  Politburo Put ,  blame ,  behind-...</td>\n",
       "      <td>[ consent ,  THE TREATMENT OF CONSENT IN SEXUA...</td>\n",
       "      <td>[ Future of Work ,  Fudging ,  Skin cancer is ...</td>\n",
       "      <td>[ New Year 's Day ,  War . ,  500 Greatest Son...</td>\n",
       "      <td>[ India and Pakistan today exchanged , through...</td>\n",
       "      <td>[ The elevator is always a problem , as it har...</td>\n",
       "      <td>[ government body ,  Republika Srpska ,  Inter...</td>\n",
       "      <td>[ SATHMAHALA ,  government links ,  Janapathi ...</td>\n",
       "      <td>[ civil court ,  legal pluralism ,  review pow...</td>\n",
       "      <td>[ Power Africa ,  Power Africa ,  switch the l...</td>\n",
       "      <td>[ bordering on the edge of logic ,  did n't fa...</td>\n",
       "      <td>[ We discovered this after our aviation health...</td>\n",
       "      <td>[ Therefore , the Supreme Court should take ju...</td>\n",
       "      <td>[ Significant skills shortages will remain acr...</td>\n",
       "      <td>[ the Government 's economic assessment data a...</td>\n",
       "      <td>[ North Korea 's absolute leadership will crum...</td>\n",
       "      <td>[ The Timbuktu manuscripts point to a dense ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ professional traveller ,  expose people for ...</td>\n",
       "      <td>[ instructions ,  Will this @ @ @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ If we want to keep going on ( the old ) trac...</td>\n",
       "      <td>[ media ,  @ @ @ @ @ @ @ @ @ @ as I looked at ...</td>\n",
       "      <td>[ if we all speak the Bible and live by what t...</td>\n",
       "      <td>[ going out ,  It probably traded a little tow...</td>\n",
       "      <td>[ brought home bodies that night , both living...</td>\n",
       "      <td>[ early settlement ,  The two countries have s...</td>\n",
       "      <td>[ There are common modifiable risk factors tha...</td>\n",
       "      <td>[ Our main export product is broccoli . We hav...</td>\n",
       "      <td>[ As a citizen , Mr Rajapaksa can @ @ @ @ @ @ ...</td>\n",
       "      <td>[ Forgive me for sharing my views , but beside...</td>\n",
       "      <td>[ As you may be aware , there has been a lot o...</td>\n",
       "      <td>[ What people should know is that Inland Reven...</td>\n",
       "      <td>[ Yosi ,  mismanaged and underfunded ,  Kiko ,...</td>\n",
       "      <td>[ Cruise Technology ,  Thomas Paine  Jingoisti...</td>\n",
       "      <td>[ Prior to the scheduled announcement by the D...</td>\n",
       "      <td>[ The bigest challenge that Tanzania is now fa...</td>\n",
       "      <td>[ You need a military that has the right equip...</td>\n",
       "      <td>[ the other half live ,  Discreet luxury yes ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ I @ @ @ @ @ @ @ @ @ @ ,  did not proceed in ...</td>\n",
       "      <td>[ Sob-e ache Bed-e ,  Bangladesh Accord . ,  T...</td>\n",
       "      <td>[ transportation corridor ,  We ought not to b...</td>\n",
       "      <td>[ With something which is slimmer than a human...</td>\n",
       "      <td>[ Ahoofe ,  good news ,  Jim Iyke is busy in T...</td>\n",
       "      <td>[ Sun Art Retail ,  Group ,  In 2014 , China '...</td>\n",
       "      <td>[ The lads who sign up for this year 's festiv...</td>\n",
       "      <td>[ totally unethical ,  obscene ,  All the mone...</td>\n",
       "      <td>[ Overcoming poverty is not a gesture of chari...</td>\n",
       "      <td>[ The refugees have told us that their gardens...</td>\n",
       "      <td>[ The report shows that systematic human right...</td>\n",
       "      <td>[ Throughout his career , Tun Abdul Razak was ...</td>\n",
       "      <td>[ In addition , the fact is that there was no ...</td>\n",
       "      <td>[ and even change the balance of power ,  The ...</td>\n",
       "      <td>[ rap ,  ripple ,  rap ,  ripple ,  He let the...</td>\n",
       "      <td>[ This year 's recipients reflect women 's rol...</td>\n",
       "      <td>[ Fan said . ,   ,   The quasar dates from a t...</td>\n",
       "      <td>[ explained Deborah Yang , display supply chai...</td>\n",
       "      <td>[ The hills are alive ... ,  The Sound of Musi...</td>\n",
       "      <td>[ Their homecoming is a beginning of a new cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ Albert Einstein is supposed to have said the...</td>\n",
       "      <td>[ She was subjected to various kinds of physic...</td>\n",
       "      <td>[ Peak Uber ,  Uber of X ,  It 's a kind of la...</td>\n",
       "      <td>[ an opportunity to re-set the dial for the co...</td>\n",
       "      <td>[ failed ,  that breakthrough opportunity ,  N...</td>\n",
       "      <td>[ Images of Our City : Landmarks and Coastline...</td>\n",
       "      <td>[ shock ,  This is a big deal . I 've describe...</td>\n",
       "      <td>[ We have already sent two letter rogatories (...</td>\n",
       "      <td>[ We looked at it . We launched an investigati...</td>\n",
       "      <td>[ a platform to facilitate exchange of views a...</td>\n",
       "      <td>[ Tamil Makkal Peravai ,  any election held be...</td>\n",
       "      <td>[ do n't judge until you see it ,  Nons ,  sca...</td>\n",
       "      <td>[ I pledge myself and our in-coming administra...</td>\n",
       "      <td>[ living through a nightmare ,  we ca n't do t...</td>\n",
       "      <td>[ lapses in planning and lack of foresight , ,...</td>\n",
       "      <td>[ the major penalty of ' Dismissal from servic...</td>\n",
       "      <td>[ the tone of society ,  There is a revolution...</td>\n",
       "      <td>[ Our expectation is that the registration wil...</td>\n",
       "      <td>[ It could n't happen here ,  How utterly terr...</td>\n",
       "      <td>[ predatory elite ,  Jo , ke mohlolo ha ke rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ a landform of Australian red sandstone ,  ev...</td>\n",
       "      <td>[ Our per capita income has risen to $1,314 . ...</td>\n",
       "      <td>[ This is one of the programs we are most prou...</td>\n",
       "      <td>[ You 've skinned us , today you pay . ,  They...</td>\n",
       "      <td>[ It is a call to provide leadership , provide...</td>\n",
       "      <td>[ Gutai Manifesto ,  Destroy the Picture : Pai...</td>\n",
       "      <td>[ Burolandschaft or office landscaping was a m...</td>\n",
       "      <td>[ all hyperbole ,  Such claims are meant to gr...</td>\n",
       "      <td>[ less than a ganja spliff ,  For over 25 year...</td>\n",
       "      <td>[ The IRC has been on the frontlines of humani...</td>\n",
       "      <td>[ God save Sri Lanka ,  commoner ,  A vent is ...</td>\n",
       "      <td>[ More than 90% of the shutdowns in Johor betw...</td>\n",
       "      <td>[ better of the two evils ,  unmitigated disas...</td>\n",
       "      <td>[ The initial stage of our investigation has l...</td>\n",
       "      <td>[ There is no live telecast available there bu...</td>\n",
       "      <td>[ fight of the century ,  National Fist ,  Ave...</td>\n",
       "      <td>[ There are 32 graves , four bodies have now b...</td>\n",
       "      <td>[ It is important to involve the public in all...</td>\n",
       "      <td>[ Dead in the water . ,  save ,  Taj Mahals . ...</td>\n",
       "      <td>[ The Fight of the Century ,  his ,  Money May...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ more bearable ,  partly shield the reader on...</td>\n",
       "      <td>[ he said .  During the award ceremony , Prime...</td>\n",
       "      <td>[ resurgence in indigenous community resistanc...</td>\n",
       "      <td>[ bang , bang , you 're dead ,  I 'm not askin...</td>\n",
       "      <td>[ huge sums of money ,  For the past seven mon...</td>\n",
       "      <td>[ small impoverished cities ,  Internet police...</td>\n",
       "      <td>[ Potato Orphans . ,  Potato Orphans ,  Potato...</td>\n",
       "      <td>[ The Supreme Court had directed the governmen...</td>\n",
       "      <td>[ Between the bikes ... loud , horrid music , ...</td>\n",
       "      <td>[ Our view is that this new rule is fraught wi...</td>\n",
       "      <td>[ Mrs Rajapaksa also said in her statement @ @...</td>\n",
       "      <td>[ The public is advised to stop sharing images...</td>\n",
       "      <td>[ The necessary details and implementation str...</td>\n",
       "      <td>[ We believe our claim has been strengthened b...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ Freedom of Speech Rally Round II ,  Out of r...</td>\n",
       "      <td>[ there is no issue of a new abuse case being ...</td>\n",
       "      <td>[ Before we set out in the morning , we were p...</td>\n",
       "      <td>[ Game of Thrones ,  They 're coming to get yo...</td>\n",
       "      <td>[ Not enough ,  Traditional leaders of all ran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ Bert ,  I observed and photographed ' figure...</td>\n",
       "      <td>[ But @ @ @ @ @ @ @ @ @ @ more investments . S...</td>\n",
       "      <td>[ achievements as a pioneering and versatile a...</td>\n",
       "      <td>[ The Dominican Republic is denying tens of th...</td>\n",
       "      <td>[ Non-banks that have previously been offering...</td>\n",
       "      <td>[ national character ,  national character ,  ...</td>\n",
       "      <td>[ I do not believe it would have made any diff...</td>\n",
       "      <td>[ I admit that geographically speaking India h...</td>\n",
       "      <td>[ Hopefully , it 's finally going to be releas...</td>\n",
       "      <td>[ outstanding leadership and educational excel...</td>\n",
       "      <td>[ bloodless war , ,  Make in India ,  I dream ...</td>\n",
       "      <td>[ I raised concerns about freedom of expressio...</td>\n",
       "      <td>[ This culminated in the successful arrest of ...</td>\n",
       "      <td>[ It is very challenging in a way to make that...</td>\n",
       "      <td>[ Immediately following the fireworks , areas ...</td>\n",
       "      <td>[ eight days of military spending ,  It may ap...</td>\n",
       "      <td>[ All I wanted was one apology and a follow-th...</td>\n",
       "      <td>[ Women Rising : Becoming the next Generation ...</td>\n",
       "      <td>[ Dukes of Hazzard ,  removed ,  That flag on ...</td>\n",
       "      <td>[ Mall of Africa ,  The gross building area is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ You could have knocked me over with a feathe...</td>\n",
       "      <td>[ rich are getting richer . ,  strong creative...</td>\n",
       "      <td>[ The Prince of Darkness ,  go to benefit the ...</td>\n",
       "      <td>[ freedom of movement ,  freedom of movement ,...</td>\n",
       "      <td>[ We @ @ @ @ @ @ @ @ @ @ challenge in Liberty ...</td>\n",
       "      <td>[ It 's to show off that they have the technol...</td>\n",
       "      <td>[ unveil ,  the Others ,  Legacy ,  You could ...</td>\n",
       "      <td>[ What he ( Shinde ) was referring to was the ...</td>\n",
       "      <td>[ See me and come live with me are two differe...</td>\n",
       "      <td>[ We understand the importance of the communit...</td>\n",
       "      <td>[ Bharat Matha Ki Jai , ,  as a mark of respec...</td>\n",
       "      <td>[ having to do with integrity and public trust...</td>\n",
       "      <td>[ I am not aware that Saraki has approached th...</td>\n",
       "      <td>[ skydived ,  There 's been a lot of rumours g...</td>\n",
       "      <td>[ impersonal forces ' ' -- the complex interac...</td>\n",
       "      <td>[ We have repeatedly asked India to refrain fr...</td>\n",
       "      <td>[ white people ,  Under 22 years of Tun Dr Mah...</td>\n",
       "      <td>[ Ciena and Cyan make a great combination for ...</td>\n",
       "      <td>[ turned much of the state into a tinderbox , ...</td>\n",
       "      <td>[ the last place on earth ,  As a family , we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ This is well within the bounds of believabil...</td>\n",
       "      <td>[ We believe attacking teachers was a heinous ...</td>\n",
       "      <td>[ You do have to make sure that you clean it o...</td>\n",
       "      <td>[ The whole thing has got completely out of co...</td>\n",
       "      <td>[ @ @ @ @ @ @ @ @ @ @ . The rules are clear on...</td>\n",
       "      <td>[ DGS is famous for talented girls . Maryknoll...</td>\n",
       "      <td>[ The Department of Finance is projecting that...</td>\n",
       "      <td>[ There are a few discrepancies in her story a...</td>\n",
       "      <td>[ As it now stands persons are screened and re...</td>\n",
       "      <td>[ The due process for removal of a Deputy Insp...</td>\n",
       "      <td>[ This study was conducted by a group of medic...</td>\n",
       "      <td>[ However , the 14 years ' imprisonment was ex...</td>\n",
       "      <td>[ a declaration that as at 27th of August 2015...</td>\n",
       "      <td>[ in the extreme - death ,  lethal ,  for the ...</td>\n",
       "      <td>[ Birdman , ,  Everest , ,  Janis ,  Human , ,...</td>\n",
       "      <td>[ slipped through my hands ,  I was holding my...</td>\n",
       "      <td>[ Will we remain special , or become ordinary ...</td>\n",
       "      <td>[ To ensure equality before the law , the stat...</td>\n",
       "      <td>[ Describe the aroma of coffee -- why ca n't i...</td>\n",
       "      <td>[ ? Justice Project South Africa ( JPSA ) said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ About 80 per cent of the work I do is with R...</td>\n",
       "      <td>[ Following the most recent information from A...</td>\n",
       "      <td>[ blow up in their faces . ,  gives us a real ...</td>\n",
       "      <td>[ homosexual behavior ,  This is complicated b...</td>\n",
       "      <td>[ God is Love is a registered Fun Club and do ...</td>\n",
       "      <td>[ The reason you only have three Asean stock e...</td>\n",
       "      <td>[ Lord of the Dance : Dangerous Games , ,  Lor...</td>\n",
       "      <td>[ Some payments were as low as $5 , but when a...</td>\n",
       "      <td>[ We certainly did n't say let 's not have any...</td>\n",
       "      <td>[ The Speaker of the Senate and I have consent...</td>\n",
       "      <td>[ I have lived most of my life in Australia . ...</td>\n",
       "      <td>[ Despite having completed their housemanship ...</td>\n",
       "      <td>[ In an attempt to flee from an approaching na...</td>\n",
       "      <td>[ most valuable and readily marketable ,  He s...</td>\n",
       "      <td>[ Lando ,  What happened to us was sad as we w...</td>\n",
       "      <td>[ un-presidential ,  @ @ @ @ @ @ @ @ @ @ My Pu...</td>\n",
       "      <td>[ If it was going to happen it would have happ...</td>\n",
       "      <td>[ The school has been taking loans from the ba...</td>\n",
       "      <td>[ The changes to kindergarten make me sick , ,...</td>\n",
       "      <td>[ pretended ,  We are currently mobilising stu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    txtfiles_au_2015  \\\n",
       "0  [ We see our trees as just as important as roa...   \n",
       "1  [ professional traveller ,  expose people for ...   \n",
       "2  [ I @ @ @ @ @ @ @ @ @ @ ,  did not proceed in ...   \n",
       "3  [ Albert Einstein is supposed to have said the...   \n",
       "4  [ a landform of Australian red sandstone ,  ev...   \n",
       "5  [ more bearable ,  partly shield the reader on...   \n",
       "6  [ Bert ,  I observed and photographed ' figure...   \n",
       "7  [ You could have knocked me over with a feathe...   \n",
       "8  [ This is well within the bounds of believabil...   \n",
       "9  [ About 80 per cent of the work I do is with R...   \n",
       "\n",
       "                                    txtfiles_bd_2015  \\\n",
       "0  [ This is not my own success . I am dedicating...   \n",
       "1  [ instructions ,  Will this @ @ @ @ @ @ @ @ @ ...   \n",
       "2  [ Sob-e ache Bed-e ,  Bangladesh Accord . ,  T...   \n",
       "3  [ She was subjected to various kinds of physic...   \n",
       "4  [ Our per capita income has risen to $1,314 . ...   \n",
       "5  [ he said .  During the award ceremony , Prime...   \n",
       "6  [ But @ @ @ @ @ @ @ @ @ @ more investments . S...   \n",
       "7  [ rich are getting richer . ,  strong creative...   \n",
       "8  [ We believe attacking teachers was a heinous ...   \n",
       "9  [ Following the most recent information from A...   \n",
       "\n",
       "                                    txtfiles_ca_2015  \\\n",
       "0  [ ca n't find the time ,  This is the exact wa...   \n",
       "1  [ If we want to keep going on ( the old ) trac...   \n",
       "2  [ transportation corridor ,  We ought not to b...   \n",
       "3  [ Peak Uber ,  Uber of X ,  It 's a kind of la...   \n",
       "4  [ This is one of the programs we are most prou...   \n",
       "5  [ resurgence in indigenous community resistanc...   \n",
       "6  [ achievements as a pioneering and versatile a...   \n",
       "7  [ The Prince of Darkness ,  go to benefit the ...   \n",
       "8  [ You do have to make sure that you clean it o...   \n",
       "9  [ blow up in their faces . ,  gives us a real ...   \n",
       "\n",
       "                                    txtfiles_gb_2015  \\\n",
       "0  [ Fed Put ,  Politburo Put ,  blame ,  behind-...   \n",
       "1  [ media ,  @ @ @ @ @ @ @ @ @ @ as I looked at ...   \n",
       "2  [ With something which is slimmer than a human...   \n",
       "3  [ an opportunity to re-set the dial for the co...   \n",
       "4  [ You 've skinned us , today you pay . ,  They...   \n",
       "5  [ bang , bang , you 're dead ,  I 'm not askin...   \n",
       "6  [ The Dominican Republic is denying tens of th...   \n",
       "7  [ freedom of movement ,  freedom of movement ,...   \n",
       "8  [ The whole thing has got completely out of co...   \n",
       "9  [ homosexual behavior ,  This is complicated b...   \n",
       "\n",
       "                                    txtfiles_gh_2015  \\\n",
       "0  [ consent ,  THE TREATMENT OF CONSENT IN SEXUA...   \n",
       "1  [ if we all speak the Bible and live by what t...   \n",
       "2  [ Ahoofe ,  good news ,  Jim Iyke is busy in T...   \n",
       "3  [ failed ,  that breakthrough opportunity ,  N...   \n",
       "4  [ It is a call to provide leadership , provide...   \n",
       "5  [ huge sums of money ,  For the past seven mon...   \n",
       "6  [ Non-banks that have previously been offering...   \n",
       "7  [ We @ @ @ @ @ @ @ @ @ @ challenge in Liberty ...   \n",
       "8  [ @ @ @ @ @ @ @ @ @ @ . The rules are clear on...   \n",
       "9  [ God is Love is a registered Fun Club and do ...   \n",
       "\n",
       "                                    txtfiles_hk_2015  \\\n",
       "0  [ Future of Work ,  Fudging ,  Skin cancer is ...   \n",
       "1  [ going out ,  It probably traded a little tow...   \n",
       "2  [ Sun Art Retail ,  Group ,  In 2014 , China '...   \n",
       "3  [ Images of Our City : Landmarks and Coastline...   \n",
       "4  [ Gutai Manifesto ,  Destroy the Picture : Pai...   \n",
       "5  [ small impoverished cities ,  Internet police...   \n",
       "6  [ national character ,  national character ,  ...   \n",
       "7  [ It 's to show off that they have the technol...   \n",
       "8  [ DGS is famous for talented girls . Maryknoll...   \n",
       "9  [ The reason you only have three Asean stock e...   \n",
       "\n",
       "                                    txtfiles_ie_2015  \\\n",
       "0  [ New Year 's Day ,  War . ,  500 Greatest Son...   \n",
       "1  [ brought home bodies that night , both living...   \n",
       "2  [ The lads who sign up for this year 's festiv...   \n",
       "3  [ shock ,  This is a big deal . I 've describe...   \n",
       "4  [ Burolandschaft or office landscaping was a m...   \n",
       "5  [ Potato Orphans . ,  Potato Orphans ,  Potato...   \n",
       "6  [ I do not believe it would have made any diff...   \n",
       "7  [ unveil ,  the Others ,  Legacy ,  You could ...   \n",
       "8  [ The Department of Finance is projecting that...   \n",
       "9  [ Lord of the Dance : Dangerous Games , ,  Lor...   \n",
       "\n",
       "                                    txtfiles_in_2015  \\\n",
       "0  [ India and Pakistan today exchanged , through...   \n",
       "1  [ early settlement ,  The two countries have s...   \n",
       "2  [ totally unethical ,  obscene ,  All the mone...   \n",
       "3  [ We have already sent two letter rogatories (...   \n",
       "4  [ all hyperbole ,  Such claims are meant to gr...   \n",
       "5  [ The Supreme Court had directed the governmen...   \n",
       "6  [ I admit that geographically speaking India h...   \n",
       "7  [ What he ( Shinde ) was referring to was the ...   \n",
       "8  [ There are a few discrepancies in her story a...   \n",
       "9  [ Some payments were as low as $5 , but when a...   \n",
       "\n",
       "                                    txtfiles_jm_2015  \\\n",
       "0  [ The elevator is always a problem , as it har...   \n",
       "1  [ There are common modifiable risk factors tha...   \n",
       "2  [ Overcoming poverty is not a gesture of chari...   \n",
       "3  [ We looked at it . We launched an investigati...   \n",
       "4  [ less than a ganja spliff ,  For over 25 year...   \n",
       "5  [ Between the bikes ... loud , horrid music , ...   \n",
       "6  [ Hopefully , it 's finally going to be releas...   \n",
       "7  [ See me and come live with me are two differe...   \n",
       "8  [ As it now stands persons are screened and re...   \n",
       "9  [ We certainly did n't say let 's not have any...   \n",
       "\n",
       "                                    txtfiles_ke_2015  \\\n",
       "0  [ government body ,  Republika Srpska ,  Inter...   \n",
       "1  [ Our main export product is broccoli . We hav...   \n",
       "2  [ The refugees have told us that their gardens...   \n",
       "3  [ a platform to facilitate exchange of views a...   \n",
       "4  [ The IRC has been on the frontlines of humani...   \n",
       "5  [ Our view is that this new rule is fraught wi...   \n",
       "6  [ outstanding leadership and educational excel...   \n",
       "7  [ We understand the importance of the communit...   \n",
       "8  [ The due process for removal of a Deputy Insp...   \n",
       "9  [ The Speaker of the Senate and I have consent...   \n",
       "\n",
       "                                    txtfiles_lk_2015  \\\n",
       "0  [ SATHMAHALA ,  government links ,  Janapathi ...   \n",
       "1  [ As a citizen , Mr Rajapaksa can @ @ @ @ @ @ ...   \n",
       "2  [ The report shows that systematic human right...   \n",
       "3  [ Tamil Makkal Peravai ,  any election held be...   \n",
       "4  [ God save Sri Lanka ,  commoner ,  A vent is ...   \n",
       "5  [ Mrs Rajapaksa also said in her statement @ @...   \n",
       "6  [ bloodless war , ,  Make in India ,  I dream ...   \n",
       "7  [ Bharat Matha Ki Jai , ,  as a mark of respec...   \n",
       "8  [ This study was conducted by a group of medic...   \n",
       "9  [ I have lived most of my life in Australia . ...   \n",
       "\n",
       "                                    txtfiles_my_2015  \\\n",
       "0  [ civil court ,  legal pluralism ,  review pow...   \n",
       "1  [ Forgive me for sharing my views , but beside...   \n",
       "2  [ Throughout his career , Tun Abdul Razak was ...   \n",
       "3  [ do n't judge until you see it ,  Nons ,  sca...   \n",
       "4  [ More than 90% of the shutdowns in Johor betw...   \n",
       "5  [ The public is advised to stop sharing images...   \n",
       "6  [ I raised concerns about freedom of expressio...   \n",
       "7  [ having to do with integrity and public trust...   \n",
       "8  [ However , the 14 years ' imprisonment was ex...   \n",
       "9  [ Despite having completed their housemanship ...   \n",
       "\n",
       "                                    txtfiles_ng_2015  \\\n",
       "0  [ Power Africa ,  Power Africa ,  switch the l...   \n",
       "1  [ As you may be aware , there has been a lot o...   \n",
       "2  [ In addition , the fact is that there was no ...   \n",
       "3  [ I pledge myself and our in-coming administra...   \n",
       "4  [ better of the two evils ,  unmitigated disas...   \n",
       "5  [ The necessary details and implementation str...   \n",
       "6  [ This culminated in the successful arrest of ...   \n",
       "7  [ I am not aware that Saraki has approached th...   \n",
       "8  [ a declaration that as at 27th of August 2015...   \n",
       "9  [ In an attempt to flee from an approaching na...   \n",
       "\n",
       "                                    txtfiles_nz_2015  \\\n",
       "0  [ bordering on the edge of logic ,  did n't fa...   \n",
       "1  [ What people should know is that Inland Reven...   \n",
       "2  [ and even change the balance of power ,  The ...   \n",
       "3  [ living through a nightmare ,  we ca n't do t...   \n",
       "4  [ The initial stage of our investigation has l...   \n",
       "5  [ We believe our claim has been strengthened b...   \n",
       "6  [ It is very challenging in a way to make that...   \n",
       "7  [ skydived ,  There 's been a lot of rumours g...   \n",
       "8  [ in the extreme - death ,  lethal ,  for the ...   \n",
       "9  [ most valuable and readily marketable ,  He s...   \n",
       "\n",
       "                                    txtfiles_ph_2015  \\\n",
       "0  [ We discovered this after our aviation health...   \n",
       "1  [ Yosi ,  mismanaged and underfunded ,  Kiko ,...   \n",
       "2  [ rap ,  ripple ,  rap ,  ripple ,  He let the...   \n",
       "3  [ lapses in planning and lack of foresight , ,...   \n",
       "4  [ There is no live telecast available there bu...   \n",
       "5                                                 []   \n",
       "6  [ Immediately following the fireworks , areas ...   \n",
       "7  [ impersonal forces ' ' -- the complex interac...   \n",
       "8  [ Birdman , ,  Everest , ,  Janis ,  Human , ,...   \n",
       "9  [ Lando ,  What happened to us was sad as we w...   \n",
       "\n",
       "                                    txtfiles_pk_2015  \\\n",
       "0  [ Therefore , the Supreme Court should take ju...   \n",
       "1  [ Cruise Technology ,  Thomas Paine  Jingoisti...   \n",
       "2  [ This year 's recipients reflect women 's rol...   \n",
       "3  [ the major penalty of ' Dismissal from servic...   \n",
       "4  [ fight of the century ,  National Fist ,  Ave...   \n",
       "5  [ Freedom of Speech Rally Round II ,  Out of r...   \n",
       "6  [ eight days of military spending ,  It may ap...   \n",
       "7  [ We have repeatedly asked India to refrain fr...   \n",
       "8  [ slipped through my hands ,  I was holding my...   \n",
       "9  [ un-presidential ,  @ @ @ @ @ @ @ @ @ @ My Pu...   \n",
       "\n",
       "                                    txtfiles_sg_2015  \\\n",
       "0  [ Significant skills shortages will remain acr...   \n",
       "1  [ Prior to the scheduled announcement by the D...   \n",
       "2  [ Fan said . ,   ,   The quasar dates from a t...   \n",
       "3  [ the tone of society ,  There is a revolution...   \n",
       "4  [ There are 32 graves , four bodies have now b...   \n",
       "5  [ there is no issue of a new abuse case being ...   \n",
       "6  [ All I wanted was one apology and a follow-th...   \n",
       "7  [ white people ,  Under 22 years of Tun Dr Mah...   \n",
       "8  [ Will we remain special , or become ordinary ...   \n",
       "9  [ If it was going to happen it would have happ...   \n",
       "\n",
       "                                    txtfiles_tz_2015  \\\n",
       "0  [ the Government 's economic assessment data a...   \n",
       "1  [ The bigest challenge that Tanzania is now fa...   \n",
       "2  [ explained Deborah Yang , display supply chai...   \n",
       "3  [ Our expectation is that the registration wil...   \n",
       "4  [ It is important to involve the public in all...   \n",
       "5  [ Before we set out in the morning , we were p...   \n",
       "6  [ Women Rising : Becoming the next Generation ...   \n",
       "7  [ Ciena and Cyan make a great combination for ...   \n",
       "8  [ To ensure equality before the law , the stat...   \n",
       "9  [ The school has been taking loans from the ba...   \n",
       "\n",
       "                                    txtfiles_us_2015  \\\n",
       "0  [ North Korea 's absolute leadership will crum...   \n",
       "1  [ You need a military that has the right equip...   \n",
       "2  [ The hills are alive ... ,  The Sound of Musi...   \n",
       "3  [ It could n't happen here ,  How utterly terr...   \n",
       "4  [ Dead in the water . ,  save ,  Taj Mahals . ...   \n",
       "5  [ Game of Thrones ,  They 're coming to get yo...   \n",
       "6  [ Dukes of Hazzard ,  removed ,  That flag on ...   \n",
       "7  [ turned much of the state into a tinderbox , ...   \n",
       "8  [ Describe the aroma of coffee -- why ca n't i...   \n",
       "9  [ The changes to kindergarten make me sick , ,...   \n",
       "\n",
       "                                    txtfiles_za_2015  \n",
       "0  [ The Timbuktu manuscripts point to a dense ar...  \n",
       "1  [ the other half live ,  Discreet luxury yes ,...  \n",
       "2  [ Their homecoming is a beginning of a new cha...  \n",
       "3  [ predatory elite ,  Jo , ke mohlolo ha ke rat...  \n",
       "4  [ The Fight of the Century ,  his ,  Money May...  \n",
       "5  [ Not enough ,  Traditional leaders of all ran...  \n",
       "6  [ Mall of Africa ,  The gross building area is...  \n",
       "7  [ the last place on earth ,  As a family , we ...  \n",
       "8  [ ? Justice Project South Africa ( JPSA ) said...  \n",
       "9  [ pretended ,  We are currently mobilising stu...  "
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txtfiles_au_2016</th>\n",
       "      <th>txtfiles_bd_2016</th>\n",
       "      <th>txtfiles_ca_2016</th>\n",
       "      <th>txtfiles_gb_2016</th>\n",
       "      <th>txtfiles_gh_2016</th>\n",
       "      <th>txtfiles_hk_2016</th>\n",
       "      <th>txtfiles_ie_2016</th>\n",
       "      <th>txtfiles_in_2016</th>\n",
       "      <th>txtfiles_jm_2016</th>\n",
       "      <th>txtfiles_ke_2016</th>\n",
       "      <th>txtfiles_lk_2016</th>\n",
       "      <th>txtfiles_my_2016</th>\n",
       "      <th>txtfiles_ng_2016</th>\n",
       "      <th>txtfiles_nz_2016</th>\n",
       "      <th>txtfiles_ph_2016</th>\n",
       "      <th>txtfiles_pk_2016</th>\n",
       "      <th>txtfiles_sg_2016</th>\n",
       "      <th>txtfiles_tz_2016</th>\n",
       "      <th>txtfiles_us_2016</th>\n",
       "      <th>txtfiles_za_2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ as a result of low oil prices ( which accoun...</td>\n",
       "      <td>[ divine vengeance ,  terrorist cells ,  The k...</td>\n",
       "      <td>[ net metering ,  It 's not all Colten . You k...</td>\n",
       "      <td>[ He has an absolutely stainless reputation to...</td>\n",
       "      <td>[ I have noted with much concern the gap betwe...</td>\n",
       "      <td>[ Project 2020+ @ @ @ @ @ @ @ @ @ @ the improv...</td>\n",
       "      <td>[ Unfold provided a unified platform , showcas...</td>\n",
       "      <td>[ This marks the first time that CRISPR has su...</td>\n",
       "      <td>[ Seventy per cent ( of the @ @ @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ We have lived in harmony for years as Keroka...</td>\n",
       "      <td>[ Comrade Bardhan was the senior-most and tall...</td>\n",
       "      <td>[ very little need for a shortlist to be annou...</td>\n",
       "      <td>[ An action will certainly be adopted on the n...</td>\n",
       "      <td>[ I think it 's the best day of tennis we have...</td>\n",
       "      <td>[ I have been very vocal about it ( fighting M...</td>\n",
       "      <td>[ a lot more emotional ,  the idea of not bein...</td>\n",
       "      <td>[  Aside from the ongoing conflict in Syria , ...</td>\n",
       "      <td>[ because of substantial price increases impos...</td>\n",
       "      <td>[ family hub refrigerator . ,  smart home ,  S...</td>\n",
       "      <td>[ It appears that the fisherman had fallen off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ extremely out of character ,  impeccable ,  ...</td>\n",
       "      <td>[ He was handed over to high commission offici...</td>\n",
       "      <td>[ We are stewards of this magnificent land , a...</td>\n",
       "      <td>[ the establishment ,  enough is enough ,  Mak...</td>\n",
       "      <td>[ This is a clear manifestation that our MP do...</td>\n",
       "      <td>[ world city ,  Low @ @ @ @ @ @ @ @ @ @ Steel ...</td>\n",
       "      <td>[ enhances market coverage and complements the...</td>\n",
       "      <td>[ The minister plans to meet all 40-odd export...</td>\n",
       "      <td>[ What three tips would you give me for swimmi...</td>\n",
       "      <td>[ This is a failure on the part of the prosecu...</td>\n",
       "      <td>[ a country of prime focus with which to furth...</td>\n",
       "      <td>[ So far four cases involving allegations of c...</td>\n",
       "      <td>[ Our sources in Chibok have revealed the name...</td>\n",
       "      <td>[ unlimited ,  It can be difficult to negotiat...</td>\n",
       "      <td>[ The government should subsidize contribution...</td>\n",
       "      <td>[ That China data was disappointing , very wea...</td>\n",
       "      <td>[ We were given to understand that the funding...</td>\n",
       "      <td>[ They beat defending champions Yanga at their...</td>\n",
       "      <td>[ restrained ,  Victim B ,  secluded area ,  W...</td>\n",
       "      <td>[ safe harbour ,  have not been easy ,  safe h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ considered ... in light of developments in t...</td>\n",
       "      <td>[ Shahabuddin fell from his rickshaw on to the...</td>\n",
       "      <td>[ I came across a very nice post that featured...</td>\n",
       "      <td>[ lost generation ,  It 's not like they 've g...</td>\n",
       "      <td>[ to put a damper on Nana 's third and possibl...</td>\n",
       "      <td>[ 2016 Regulations ,  2002 Regulations ,  onli...</td>\n",
       "      <td>[ I love you more than anything , ,  I thought...</td>\n",
       "      <td>[ The property cards have been digitised , ver...</td>\n",
       "      <td>[ regarding the appointment of the Saudi billi...</td>\n",
       "      <td>[ KWS continues to work with the @ @ @ @ @ @ @...</td>\n",
       "      <td>[ The costumes are how we reveal ourselves to ...</td>\n",
       "      <td>[ It will play a vital role in maintaining Mal...</td>\n",
       "      <td>[ you are no longer recognized as a university...</td>\n",
       "      <td>[ It 's been a thrill to have been able to hel...</td>\n",
       "      <td>[ that would leave ( Olympic ) eligibility in ...</td>\n",
       "      <td>[ In theory , it ( Arab Light ) will go up 60-...</td>\n",
       "      <td>[ sex worker ,  Most of the abusers called me ...</td>\n",
       "      <td>[ Eduardo 's extensive pan-regional media expe...</td>\n",
       "      <td>[ I would say the diversity of Sonoma County i...</td>\n",
       "      <td>[ The ' Ambassador 's Footprint ' exhibition o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ hundreds of millions of dollars ,  the most ...</td>\n",
       "      <td>[ GCM Incurred More Losses ,  The angry suppor...</td>\n",
       "      <td>[ We were really thrilled with the response , ...</td>\n",
       "      <td>[ I was reading an article in a popular nation...</td>\n",
       "      <td>[ I think we should have faith in ourselves an...</td>\n",
       "      <td>[ zero quota ,  doubly non-permanent-resident ...</td>\n",
       "      <td>[ The Atlantic Bridge model was to take compan...</td>\n",
       "      <td>[ Mehbooba Mufti sahiba will be sworn in as Ch...</td>\n",
       "      <td>[ over a significant period of time ,  This is...</td>\n",
       "      <td>[ unprofessional misconduct ,  selfie ,  fart ...</td>\n",
       "      <td>[ today the country is in an era where the rec...</td>\n",
       "      <td>[ Whoopi &amp;amp; Maya ,  This was all inspired b...</td>\n",
       "      <td>[ As a result of the failure of the affected c...</td>\n",
       "      <td>[ Images have now become so extreme that what ...</td>\n",
       "      <td>[ blood is written all over the Aquino-Cojuang...</td>\n",
       "      <td>[ Top Gun 2 . ,  Yeah , of course ! ,  You do ...</td>\n",
       "      <td>[ Malaysia ( new A3 assessment ) is suffering ...</td>\n",
       "      <td>[ ADTM ,  These celebrations are not only abou...</td>\n",
       "      <td>[ The New York primary and the psychological i...</td>\n",
       "      <td>[ reluctantly ,  Hoole said .  ,  he said . , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ Interest in Brisbane hotels has understandab...</td>\n",
       "      <td>[ You workers have to be very careful about te...</td>\n",
       "      <td>[ It is what it is , ,  Maybe I lost the last ...</td>\n",
       "      <td>[ We need to work with local organisations to ...</td>\n",
       "      <td>[ tangible progress ,  regime obstruction of t...</td>\n",
       "      <td>[ Very good , ,  A little bit expensive . ,  W...</td>\n",
       "      <td>[ I knew it would be like this . That 's why t...</td>\n",
       "      <td>[ India and her problems ,  The child is taken...</td>\n",
       "      <td>[ little-known facts ,  This is a dream that w...</td>\n",
       "      <td>[ They ( fanatics ) demanded for the night , a...</td>\n",
       "      <td>[ Ratak Dinana Athak Lowak Dinana Hetak ,  Kap...</td>\n",
       "      <td>[ I wanted to make the final but I feel normal...</td>\n",
       "      <td>[ I knew it would be like this . That 's why t...</td>\n",
       "      <td>[ In New Zealand we have some amazingly talent...</td>\n",
       "      <td>[ rap ,  ripple ,  rap ,  ripple ,  So far , w...</td>\n",
       "      <td>[ the religion ,  Us ,  Them . ,  Other ,  Gar...</td>\n",
       "      <td>[ I knew it would be like this . That 's why t...</td>\n",
       "      <td>[ We have already forwarded the matter to the ...</td>\n",
       "      <td>[ That 's $1.7 million that parents saved in t...</td>\n",
       "      <td>[ King Ottokar 's Sceptre ,  King Ottokar 's S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ The Power of One ,  I think a lot of grain-f...</td>\n",
       "      <td>[ The prosecution is happy with the verdict , ...</td>\n",
       "      <td>[ We 're about seven months @ @ @ @ @ @ @ @ @ ...</td>\n",
       "      <td>[ Everyone is entitled to an off night , and t...</td>\n",
       "      <td>[ he said .  The Election 2016 project is bein...</td>\n",
       "      <td>[ heavenly market ,  invisible forces ,  life ...</td>\n",
       "      <td>[ ( What has happened in the past ) does not a...</td>\n",
       "      <td>[ If the taxes were proposed in the Budget 201...</td>\n",
       "      <td>[ uncertain voters ,  The best voter turnout i...</td>\n",
       "      <td>[ I 'm able to meet my financial obligations ....</td>\n",
       "      <td>[ I would like to retire from playing Test cri...</td>\n",
       "      <td>[ Cultural diversity should be a source of str...</td>\n",
       "      <td>[ It was a slow start for me and I got dropped...</td>\n",
       "      <td>[ true ,  It is grossly irresponsible and pate...</td>\n",
       "      <td>[ Gihadlok man ko niya . ( I was threatened by...</td>\n",
       "      <td>[ I congratulate you all on the completion of ...</td>\n",
       "      <td>[ The current level of international product p...</td>\n",
       "      <td>[ @ @ @ @ @ @ @ @ @ @ fresh , professional Chi...</td>\n",
       "      <td>[ It was better then than later , ,  We wanted...</td>\n",
       "      <td>[ anger ? frustration and an exaggerated sense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ Leanne Montgomerie , Kodah 's grandmother , ...</td>\n",
       "      <td>[ The presence of participants will be less if...</td>\n",
       "      <td>[ Pensida Anti-Redness Cream has been speciall...</td>\n",
       "      <td>[ off his head ,  large ,  He walked @ @ @ @ @...</td>\n",
       "      <td>[ I think that the leadership should @ @ @ @ @...</td>\n",
       "      <td>[ was found guilty of the charge of participat...</td>\n",
       "      <td>[ @ @ @ @ @ @ @ @ @ @ the balance of the year ...</td>\n",
       "      <td>[ Meg has been experiencing right shoulder pai...</td>\n",
       "      <td>[ Secretary General Odunton has several years ...</td>\n",
       "      <td>[ How many times have you seen the deputy in c...</td>\n",
       "      <td>[ A Love Like This ,  A Love Like This ,  It w...</td>\n",
       "      <td>[ on-site inspection , ,  Security Council Wor...</td>\n",
       "      <td>[ The technical crew is working tirelessly and...</td>\n",
       "      <td>[ If you read some newspapers in the Netherlan...</td>\n",
       "      <td>[ It 's not that bad finishing in the upper ha...</td>\n",
       "      <td>[ Ulema and Mashaikh Convention ,  Black Day ,...</td>\n",
       "      <td>[ on-site inspection , ,  soft target ,  soft ...</td>\n",
       "      <td>[ fundamental problem with democracy ,  With a...</td>\n",
       "      <td>[ We can not reopen Herbert Hoover , ,  I know...</td>\n",
       "      <td>[ 67 Blankets for Madiba ,  67 Blankets for Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ best practice and is governed by robust stru...</td>\n",
       "      <td>[ All other avenues of investment for general ...</td>\n",
       "      <td>[ Antioxidants Market -- Growth , Share , Oppo...</td>\n",
       "      <td>[ The place was bouncing , ,  We met in Legend...</td>\n",
       "      <td>[ We must all love peace for the development @...</td>\n",
       "      <td>[ paying the price ,  I 've never seen anyone ...</td>\n",
       "      <td>[ terrorist association ,  Thank you to all th...</td>\n",
       "      <td>[ There is a rise of 50 per cent in the number...</td>\n",
       "      <td>[ mourn ,  who feel they are young and invinci...</td>\n",
       "      <td>[ sticking up two fingers ,  cronyism ' ,  ove...</td>\n",
       "      <td>[ There was no plan regarding the Olympics whe...</td>\n",
       "      <td>[ At the moment , negotiations over the takeov...</td>\n",
       "      <td>[ We will now leave for Brazil on Monday ( tod...</td>\n",
       "      <td>[ I wrote it as a wero ( challenge ) to the li...</td>\n",
       "      <td>[ Fast and Furious ,  new cinematographic serv...</td>\n",
       "      <td>[ I worked on it ( with my coach ) , ,  I had ...</td>\n",
       "      <td>[ For more than 1,800 of the last 2,000 years ...</td>\n",
       "      <td>[ The Filmin Kids app is a step forward in our...</td>\n",
       "      <td>[ cheat sheet ,  granted joint motion of attor...</td>\n",
       "      <td>[ We have been fighting for this for many year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ That 's what home-court advantage is about ....</td>\n",
       "      <td>[ Momin Ali along with Delowar Hosaain , ex-pr...</td>\n",
       "      <td>[ September sees the hot sunshine in Cyprus , ...</td>\n",
       "      <td>[ moral and spiritual concern ,  Sadly , these...</td>\n",
       "      <td>[ acting prudently and responsibly , the gover...</td>\n",
       "      <td>[ Company ,  Group ,  fugitive ,  some power ....</td>\n",
       "      <td>[ If Angus wanted me to play then that 's up t...</td>\n",
       "      <td>[ Most incumbent networks have not built for d...</td>\n",
       "      <td>[ I 'm a weed lover , but I do n't just get up...</td>\n",
       "      <td>[ We have planned a series of demonstrations i...</td>\n",
       "      <td>[ The 09 athletes who will be leaving the coun...</td>\n",
       "      <td>[ At 10am ( 07:00 GMT ) the warplanes dropped ...</td>\n",
       "      <td>[ Redefining Leadership through Music and Entr...</td>\n",
       "      <td>[ We had a local meeting and the big question ...</td>\n",
       "      <td>[ It 's a very dangerous field , ,  The job st...</td>\n",
       "      <td>[ After cancelling the commitment , Bipasha st...</td>\n",
       "      <td>[ This is still a very ' young ' topic in @ @ ...</td>\n",
       "      <td>[ GOODBYE , Party of Nelson Mandela ,  a recor...</td>\n",
       "      <td>[ My goal , ,  is to ensure that all of our st...</td>\n",
       "      <td>[ The victim sustained a minor injury to his c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    txtfiles_au_2016  \\\n",
       "0  [ as a result of low oil prices ( which accoun...   \n",
       "1  [ extremely out of character ,  impeccable ,  ...   \n",
       "2  [ considered ... in light of developments in t...   \n",
       "3  [ hundreds of millions of dollars ,  the most ...   \n",
       "4  [ Interest in Brisbane hotels has understandab...   \n",
       "5  [ The Power of One ,  I think a lot of grain-f...   \n",
       "6  [ Leanne Montgomerie , Kodah 's grandmother , ...   \n",
       "7  [ best practice and is governed by robust stru...   \n",
       "8  [ That 's what home-court advantage is about ....   \n",
       "\n",
       "                                    txtfiles_bd_2016  \\\n",
       "0  [ divine vengeance ,  terrorist cells ,  The k...   \n",
       "1  [ He was handed over to high commission offici...   \n",
       "2  [ Shahabuddin fell from his rickshaw on to the...   \n",
       "3  [ GCM Incurred More Losses ,  The angry suppor...   \n",
       "4  [ You workers have to be very careful about te...   \n",
       "5  [ The prosecution is happy with the verdict , ...   \n",
       "6  [ The presence of participants will be less if...   \n",
       "7  [ All other avenues of investment for general ...   \n",
       "8  [ Momin Ali along with Delowar Hosaain , ex-pr...   \n",
       "\n",
       "                                    txtfiles_ca_2016  \\\n",
       "0  [ net metering ,  It 's not all Colten . You k...   \n",
       "1  [ We are stewards of this magnificent land , a...   \n",
       "2  [ I came across a very nice post that featured...   \n",
       "3  [ We were really thrilled with the response , ...   \n",
       "4  [ It is what it is , ,  Maybe I lost the last ...   \n",
       "5  [ We 're about seven months @ @ @ @ @ @ @ @ @ ...   \n",
       "6  [ Pensida Anti-Redness Cream has been speciall...   \n",
       "7  [ Antioxidants Market -- Growth , Share , Oppo...   \n",
       "8  [ September sees the hot sunshine in Cyprus , ...   \n",
       "\n",
       "                                    txtfiles_gb_2016  \\\n",
       "0  [ He has an absolutely stainless reputation to...   \n",
       "1  [ the establishment ,  enough is enough ,  Mak...   \n",
       "2  [ lost generation ,  It 's not like they 've g...   \n",
       "3  [ I was reading an article in a popular nation...   \n",
       "4  [ We need to work with local organisations to ...   \n",
       "5  [ Everyone is entitled to an off night , and t...   \n",
       "6  [ off his head ,  large ,  He walked @ @ @ @ @...   \n",
       "7  [ The place was bouncing , ,  We met in Legend...   \n",
       "8  [ moral and spiritual concern ,  Sadly , these...   \n",
       "\n",
       "                                    txtfiles_gh_2016  \\\n",
       "0  [ I have noted with much concern the gap betwe...   \n",
       "1  [ This is a clear manifestation that our MP do...   \n",
       "2  [ to put a damper on Nana 's third and possibl...   \n",
       "3  [ I think we should have faith in ourselves an...   \n",
       "4  [ tangible progress ,  regime obstruction of t...   \n",
       "5  [ he said .  The Election 2016 project is bein...   \n",
       "6  [ I think that the leadership should @ @ @ @ @...   \n",
       "7  [ We must all love peace for the development @...   \n",
       "8  [ acting prudently and responsibly , the gover...   \n",
       "\n",
       "                                    txtfiles_hk_2016  \\\n",
       "0  [ Project 2020+ @ @ @ @ @ @ @ @ @ @ the improv...   \n",
       "1  [ world city ,  Low @ @ @ @ @ @ @ @ @ @ Steel ...   \n",
       "2  [ 2016 Regulations ,  2002 Regulations ,  onli...   \n",
       "3  [ zero quota ,  doubly non-permanent-resident ...   \n",
       "4  [ Very good , ,  A little bit expensive . ,  W...   \n",
       "5  [ heavenly market ,  invisible forces ,  life ...   \n",
       "6  [ was found guilty of the charge of participat...   \n",
       "7  [ paying the price ,  I 've never seen anyone ...   \n",
       "8  [ Company ,  Group ,  fugitive ,  some power ....   \n",
       "\n",
       "                                    txtfiles_ie_2016  \\\n",
       "0  [ Unfold provided a unified platform , showcas...   \n",
       "1  [ enhances market coverage and complements the...   \n",
       "2  [ I love you more than anything , ,  I thought...   \n",
       "3  [ The Atlantic Bridge model was to take compan...   \n",
       "4  [ I knew it would be like this . That 's why t...   \n",
       "5  [ ( What has happened in the past ) does not a...   \n",
       "6  [ @ @ @ @ @ @ @ @ @ @ the balance of the year ...   \n",
       "7  [ terrorist association ,  Thank you to all th...   \n",
       "8  [ If Angus wanted me to play then that 's up t...   \n",
       "\n",
       "                                    txtfiles_in_2016  \\\n",
       "0  [ This marks the first time that CRISPR has su...   \n",
       "1  [ The minister plans to meet all 40-odd export...   \n",
       "2  [ The property cards have been digitised , ver...   \n",
       "3  [ Mehbooba Mufti sahiba will be sworn in as Ch...   \n",
       "4  [ India and her problems ,  The child is taken...   \n",
       "5  [ If the taxes were proposed in the Budget 201...   \n",
       "6  [ Meg has been experiencing right shoulder pai...   \n",
       "7  [ There is a rise of 50 per cent in the number...   \n",
       "8  [ Most incumbent networks have not built for d...   \n",
       "\n",
       "                                    txtfiles_jm_2016  \\\n",
       "0  [ Seventy per cent ( of the @ @ @ @ @ @ @ @ @ ...   \n",
       "1  [ What three tips would you give me for swimmi...   \n",
       "2  [ regarding the appointment of the Saudi billi...   \n",
       "3  [ over a significant period of time ,  This is...   \n",
       "4  [ little-known facts ,  This is a dream that w...   \n",
       "5  [ uncertain voters ,  The best voter turnout i...   \n",
       "6  [ Secretary General Odunton has several years ...   \n",
       "7  [ mourn ,  who feel they are young and invinci...   \n",
       "8  [ I 'm a weed lover , but I do n't just get up...   \n",
       "\n",
       "                                    txtfiles_ke_2016  \\\n",
       "0  [ We have lived in harmony for years as Keroka...   \n",
       "1  [ This is a failure on the part of the prosecu...   \n",
       "2  [ KWS continues to work with the @ @ @ @ @ @ @...   \n",
       "3  [ unprofessional misconduct ,  selfie ,  fart ...   \n",
       "4  [ They ( fanatics ) demanded for the night , a...   \n",
       "5  [ I 'm able to meet my financial obligations ....   \n",
       "6  [ How many times have you seen the deputy in c...   \n",
       "7  [ sticking up two fingers ,  cronyism ' ,  ove...   \n",
       "8  [ We have planned a series of demonstrations i...   \n",
       "\n",
       "                                    txtfiles_lk_2016  \\\n",
       "0  [ Comrade Bardhan was the senior-most and tall...   \n",
       "1  [ a country of prime focus with which to furth...   \n",
       "2  [ The costumes are how we reveal ourselves to ...   \n",
       "3  [ today the country is in an era where the rec...   \n",
       "4  [ Ratak Dinana Athak Lowak Dinana Hetak ,  Kap...   \n",
       "5  [ I would like to retire from playing Test cri...   \n",
       "6  [ A Love Like This ,  A Love Like This ,  It w...   \n",
       "7  [ There was no plan regarding the Olympics whe...   \n",
       "8  [ The 09 athletes who will be leaving the coun...   \n",
       "\n",
       "                                    txtfiles_my_2016  \\\n",
       "0  [ very little need for a shortlist to be annou...   \n",
       "1  [ So far four cases involving allegations of c...   \n",
       "2  [ It will play a vital role in maintaining Mal...   \n",
       "3  [ Whoopi &amp; Maya ,  This was all inspired b...   \n",
       "4  [ I wanted to make the final but I feel normal...   \n",
       "5  [ Cultural diversity should be a source of str...   \n",
       "6  [ on-site inspection , ,  Security Council Wor...   \n",
       "7  [ At the moment , negotiations over the takeov...   \n",
       "8  [ At 10am ( 07:00 GMT ) the warplanes dropped ...   \n",
       "\n",
       "                                    txtfiles_ng_2016  \\\n",
       "0  [ An action will certainly be adopted on the n...   \n",
       "1  [ Our sources in Chibok have revealed the name...   \n",
       "2  [ you are no longer recognized as a university...   \n",
       "3  [ As a result of the failure of the affected c...   \n",
       "4  [ I knew it would be like this . That 's why t...   \n",
       "5  [ It was a slow start for me and I got dropped...   \n",
       "6  [ The technical crew is working tirelessly and...   \n",
       "7  [ We will now leave for Brazil on Monday ( tod...   \n",
       "8  [ Redefining Leadership through Music and Entr...   \n",
       "\n",
       "                                    txtfiles_nz_2016  \\\n",
       "0  [ I think it 's the best day of tennis we have...   \n",
       "1  [ unlimited ,  It can be difficult to negotiat...   \n",
       "2  [ It 's been a thrill to have been able to hel...   \n",
       "3  [ Images have now become so extreme that what ...   \n",
       "4  [ In New Zealand we have some amazingly talent...   \n",
       "5  [ true ,  It is grossly irresponsible and pate...   \n",
       "6  [ If you read some newspapers in the Netherlan...   \n",
       "7  [ I wrote it as a wero ( challenge ) to the li...   \n",
       "8  [ We had a local meeting and the big question ...   \n",
       "\n",
       "                                    txtfiles_ph_2016  \\\n",
       "0  [ I have been very vocal about it ( fighting M...   \n",
       "1  [ The government should subsidize contribution...   \n",
       "2  [ that would leave ( Olympic ) eligibility in ...   \n",
       "3  [ blood is written all over the Aquino-Cojuang...   \n",
       "4  [ rap ,  ripple ,  rap ,  ripple ,  So far , w...   \n",
       "5  [ Gihadlok man ko niya . ( I was threatened by...   \n",
       "6  [ It 's not that bad finishing in the upper ha...   \n",
       "7  [ Fast and Furious ,  new cinematographic serv...   \n",
       "8  [ It 's a very dangerous field , ,  The job st...   \n",
       "\n",
       "                                    txtfiles_pk_2016  \\\n",
       "0  [ a lot more emotional ,  the idea of not bein...   \n",
       "1  [ That China data was disappointing , very wea...   \n",
       "2  [ In theory , it ( Arab Light ) will go up 60-...   \n",
       "3  [ Top Gun 2 . ,  Yeah , of course ! ,  You do ...   \n",
       "4  [ the religion ,  Us ,  Them . ,  Other ,  Gar...   \n",
       "5  [ I congratulate you all on the completion of ...   \n",
       "6  [ Ulema and Mashaikh Convention ,  Black Day ,...   \n",
       "7  [ I worked on it ( with my coach ) , ,  I had ...   \n",
       "8  [ After cancelling the commitment , Bipasha st...   \n",
       "\n",
       "                                    txtfiles_sg_2016  \\\n",
       "0  [  Aside from the ongoing conflict in Syria , ...   \n",
       "1  [ We were given to understand that the funding...   \n",
       "2  [ sex worker ,  Most of the abusers called me ...   \n",
       "3  [ Malaysia ( new A3 assessment ) is suffering ...   \n",
       "4  [ I knew it would be like this . That 's why t...   \n",
       "5  [ The current level of international product p...   \n",
       "6  [ on-site inspection , ,  soft target ,  soft ...   \n",
       "7  [ For more than 1,800 of the last 2,000 years ...   \n",
       "8  [ This is still a very ' young ' topic in @ @ ...   \n",
       "\n",
       "                                    txtfiles_tz_2016  \\\n",
       "0  [ because of substantial price increases impos...   \n",
       "1  [ They beat defending champions Yanga at their...   \n",
       "2  [ Eduardo 's extensive pan-regional media expe...   \n",
       "3  [ ADTM ,  These celebrations are not only abou...   \n",
       "4  [ We have already forwarded the matter to the ...   \n",
       "5  [ @ @ @ @ @ @ @ @ @ @ fresh , professional Chi...   \n",
       "6  [ fundamental problem with democracy ,  With a...   \n",
       "7  [ The Filmin Kids app is a step forward in our...   \n",
       "8  [ GOODBYE , Party of Nelson Mandela ,  a recor...   \n",
       "\n",
       "                                    txtfiles_us_2016  \\\n",
       "0  [ family hub refrigerator . ,  smart home ,  S...   \n",
       "1  [ restrained ,  Victim B ,  secluded area ,  W...   \n",
       "2  [ I would say the diversity of Sonoma County i...   \n",
       "3  [ The New York primary and the psychological i...   \n",
       "4  [ That 's $1.7 million that parents saved in t...   \n",
       "5  [ It was better then than later , ,  We wanted...   \n",
       "6  [ We can not reopen Herbert Hoover , ,  I know...   \n",
       "7  [ cheat sheet ,  granted joint motion of attor...   \n",
       "8  [ My goal , ,  is to ensure that all of our st...   \n",
       "\n",
       "                                    txtfiles_za_2016  \n",
       "0  [ It appears that the fisherman had fallen off...  \n",
       "1  [ safe harbour ,  have not been easy ,  safe h...  \n",
       "2  [ The ' Ambassador 's Footprint ' exhibition o...  \n",
       "3  [ reluctantly ,  Hoole said .  ,  he said . , ...  \n",
       "4  [ King Ottokar 's Sceptre ,  King Ottokar 's S...  \n",
       "5  [ anger ? frustration and an exaggerated sense...  \n",
       "6  [ 67 Blankets for Madiba ,  67 Blankets for Ma...  \n",
       "7  [ We have been fighting for this for many year...  \n",
       "8  [ The victim sustained a minor injury to his c...  "
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# wordfreq = []\n",
    "# counts_au_2010 = {}\n",
    "# for article in items:\n",
    "#     for quotes in article:\n",
    "#         words = quotes.lower().split()\n",
    "#         wordfreq.append(words)\n",
    "# counts_au_2010 = Counter(x for sublist in wordfreq for x in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_au_2010.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = []\n",
    "\n",
    "# for i in txtfiles_bd_2010:\n",
    "#     f = open(i, \"r\")\n",
    "#     article1 = f.read()\n",
    "    \n",
    "#     new1=article1.replace('','\"')\n",
    "\n",
    "#     new2=new1.replace('','\"')\n",
    "\n",
    "#     new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "#     quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "#     items.append(quo)\n",
    "\n",
    "# df_2010['txtfiles_bd_2010'] = pd.Series(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_ca_2010[\"'re\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(txtfiles_ca[11], \"r\")\n",
    "# article1 = f.read()\n",
    "# print(article1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# items2 = []\n",
    "\n",
    "# article1 = re.sub('\"','',article1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# new1=article1.replace('','\"')\n",
    "\n",
    "# new2=new1.replace('','\"')\n",
    "\n",
    "# new3 = new1.replace('<p>', \"\")\n",
    "\n",
    "# quo = re.findall('\"([^\"]*)\"', new3)\n",
    "\n",
    "# items2.append(quo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
